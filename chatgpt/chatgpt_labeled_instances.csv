,id,category,labels,link,text
0,679b7636c0da9620d9598948,commit,software quality -> optimization,https://github.com/phanxuanquang/Object-Oriented-Programming-UIT/commit/542340802fa92d05ad908ae86cff5dc9c77197e1,Use ChatGPT to optimize code
1,679bdc7ec0da9620d95a1b12,commit,learning -> how to use a library/framework/tool,https://github.com/Offroaders123/Dovetail/commit/5b52113e94f499b6ae2b284268b63ff79f35298e,"Workflow Repo Paths

Realized I didn't add the correct handling for where the repo is cloned from, I was supposed to add my own info in there, haha.

Not gonna lie, I've been using ChatGPT to help me out with learning these new tools, as they seem to be more confusing for me to understand than coding sometimes, geeez man. I see how they can speed things up, but they definitely take a while to fully dive into using in your own coding setup. Once you get on board though, it really is another level of cool. You just have to take that first leap! I guess I'm trying to do that with ChatGPT too, and it has shown itself to be very helpful to me already so far. I think my favorite thing about it is it's understanding in context for things you are asking it. It's less about keywords, and more about providing it with real context that it can break down and try to understand what you're trying to do. That's a really big thing to be able to break down. My thought about it is that it's still the same data that we're working with already (everything on the Internet), but it's a much more refined interface to be able to look at it through. The data's already out there, and now we have another tool to be able to look though, compare, and discern what we already know and have learned (as people, I guess I'd say). Of course the data on the Internet doesn't perfectly depict all of the information out in the world, but that's not ChatGPT's issue, that's because of what we've all decided to put up on the Internet in the first place. It also could be ChatGPT's fault for both parsing that Big-Data inaccurately, so I guess it's a bit 'lossless' on both sides, at least to how I'm thinking of it. If we as people can discern what we get out of our conversations with projects like ChatGPT, we can then *ourselves* also get the big picture from what it responds, since that's exactly what it's (GPT) doing also. I think this makes it ""reliable-ish"" in a way. I think navigating Wikipedia this way is also very smart. For how much data there is on a given resource you're looking at, I'd like to think that some majority of it is most likely credible. If you understand and accept that you don't know for sure though, than you can got to multiple different resources to try and see what each one represents. Then you yourself can decide how you want to take those conclusions.

Not sure what that rant was, or where that came from. That was awesome! XD"
2,679bf458c0da9620d95a3678,commit,development environment -> continuous integration -> integrating ChatGPT reviewing in CI,https://github.com/clickcaramel/PolyglotRocks/commit/8bb4757c110bc309768e94cd8d000923945fe19b,ci: trying to use ChatGPT for code review
3,679c02f5c0da9620d95a4be2,commit,development environment -> continuous integration -> integrating ChatGPT reviewing in CI,https://github.com/AirtestProject/Airtest/commit/b1f3b97f7f9bffb4ee4b9aab47aa6d147b107cf9,"try to add chatgpt code reviewer

(cherry picked from commit 876543b982647c3d079afba1d6f32e8b27f586f6)"
4,679c104cc0da9620d95a5c8d,commit,feature implementation/enhancement -> implementing a new feature,https://github.com/sayedihashimi/sayed-samples/commit/b67a11ebccdeadbaa055ba3b563b6f9a51a49f16,"chatgpt blackjack genetic algorithm
I asked chatgpt ""In C# write a genetic algorithm to solve blackjack"",
this is basically what was produced. I had to fix it up a bit though."
5,679c104cc0da9620d95a5df9,commit,development environment -> continuous integration -> integrating ChatGPT reviewing in CI,https://github.com/iotexproject/iotex-core/commit/237b5a22d0f57ef88213f2b4359dd7e12f317197,"add chatgpt code review (#3824)

* add chatgpt code review

* add chatgpt code review

* add chatgpt code review

---------

Co-authored-by: xianhuawei <xianhuawei@macair2018.local>"
6,679c3b76c0da9620d95a9c8a,commit,feature implementation/enhancement -> implementing a new feature,https://github.com/pathikrit/newswall/commit/423b3bdc950fb889b58f3dc8030696e6703fc68c,"chatgpt code suggestion
see: https://twitter.com/pathikrit_wrick/status/1640593676567977984"
7,679c4841c0da9620d95aa944,commit,development environment -> continuous integration -> integrating ChatGPT reviewing in CI,https://github.com/AirtestProject/Poco/commit/3e90e5ebe2ba82305d0c664553de8c3d2acb5200,"add chatgpt code reviewer

(cherry picked from commit 7f6f5b5b454c3ac533e87ba43adeb5fd50d5ff9b)"
8,679c4841c0da9620d95aab33,commit,software quality -> fixing,https://github.com/arnog/mathlive/commit/a72ed1cd18270abfdc1e78c89b93ec44976c73d4,"fix: fixed#1890, third attempt, this time with an assist from ChatGPT"
9,679c4841c0da9620d95aab3e,commit,software quality -> optimization,https://github.com/mostafakamal22/Sprints-MERN-E-Banking-System/commit/47317258c55c6c421555ac2ff81fc66b35221a8a,optimize hero section with help of chatgpt
10,679cd9f4c0da9620d95b7d0e,commit,"documentation,documentation -> generating api specification",https://github.com/esensible/silkflow/commit/7d7769990708a47ebf6d2fa1a399529603e0d8f4,ChatGPT generated docs
11,679cf305c0da9620d95b97d6,commit,"documentation -> generating readme,documentation -> internationalization support",https://github.com/lhprojects/blog/commit/0895d5bdcc17a2e421f46082e5d1713876ae29dc,"Create Chatgpt-splitmergefiles.md

Reflection on Using ChatGPT to Split and Merge Files"
12,679cf697c0da9620d95ba2d1,commit,software quality -> optimization,https://github.com/hjyssg/ShiguReader/commit/b554452fffe198ac310e511d038d19cc396356dd,[AMAZING] use ChatGPT to optimize my sql query
13,679d454ac0da9620d95c0afc,commit,documentation -> generating readme,https://github.com/osism/k8s-capi-images/commit/ffa158634121f447045f2f40e75e88b9d7aa5bb8,"README: use tables instead of lists (#185)

Generated with the help of ChatGPT

Signed-off-by: Christian Berendt <berendt@osism.tech>"
14,679d454ac0da9620d95c0b01,commit,documentation -> generating readme,https://github.com/osism/k8s-capi-images/commit/31cb0a787e87455d0c0f6eba0a6e8e2d7378a107,"README: use tables instead of lists (pt. 2) (#186)

Generated with the help of ChatGPT

Signed-off-by: Christian Berendt <berendt@osism.tech>"
15,679d5676c0da9620d95c2785,commit,software quality -> fixing -> functional bugs,https://github.com/MarginallyClever/Robot-Overlord-App/commit/717d37f2e6cc404378efeb429f9fff6a104096cf,suggested improvement from chatgpt
16,679d612dc0da9620d95c30b5,commit,"documentation -> generating readme,generating/manipulating data",https://github.com/nathanwang000/genAI_storyteller/commit/0ae2ef3848814615d3fc432049ebd889e70a7f92,added a story generated by chatgpt and illustrated by SD
17,679d632ec0da9620d95c3603,commit,feature implementation/enhancement -> implementing a new feature,https://github.com/LebsterFace/LebJS/commit/90aa087f4957457632c34444580d82db01895f3f,"ArrayPrototype: Implement `copyWithin`

This implementation was almost entirely generated by ChatGPT ðŸ¤¯"
18,679d6f2ac0da9620d95c44a2,commit,documentation -> generating code comments,https://github.com/oaplatform/oap/commit/30205f6375e718350d8c40a21a7b4eaece12c7b5,added some javadoc generated by ChatGPT
19,679d72c6c0da9620d95c4e9a,commit,documentation -> generating readme,https://github.com/gregorriegler/mob-programming-rpg/commit/5907691e320d123994165a453b2995af159d5822,"d add section in session-notes for summary generated by ChatGPT

Co-Authored-By: Joel Silberman <42779942+jcs-instructor@users.noreply.github.com>
Co-Authored-By: Michael R. Wolf <MichaelRWolf@att.net>
Co-Authored-By: Nitsan Avni <nitsanav@gmail.com>
Co-Authored-By: 4dsherwood <4dsherwood@users.noreply.github.com>
Co-Authored-By: David Campey <campey@gmail.com>"
20,679d7dedc0da9620d95c5974,commit,feature implementation/enhancement -> feature enhancement,https://github.com/FRCDiscord/Dozer/commit/82b8def79a72b6c990513fa7e469de4e878fddd3,"used chatgpt to create helper function to scrape data, and removed ephemeral :("
21,679d7f6fc0da9620d95c5d66,commit,feature implementation/enhancement -> implementing a new feature,https://github.com/GEMDAT-repos/GEMDAT/commit/a6aa3bf7965313d86a50e1e61b85a92ac57fa7f1,"files generated by chatgpt (#3)

* find_sites.py generated by chatgpt

* added chatgpt translations"
22,679d9127c0da9620d95c7cbc,commit,software quality -> testing -> generating tests,https://github.com/archeryue/go-torrent/commit/53e37826497436336a8345e3c6b525e07cd641d6,ADD: testcode for Bencode function generated by ChatGPT
23,679daaa7c0da9620d95c99fa,commit,"documentation -> generating readme,documentation -> improving writing",https://github.com/tyriis/home-ops/commit/f35de01dda08b2bf4f71f112a43f1f4ec654e942,"docs: update README, find better words with the help of chatGPT for my
project intro"
24,679dc5a3c0da9620d95cbebe,commit,feature implementation/enhancement -> implementing a new feature,https://github.com/kev-m/pyCoilGen/commit/eb9724e9d976cf770a5b4cc506d3c38616c1b51b,"Calculate group centers implemented (#5)

* As generated by ChatGPT

* Regenerated by accident.

* Moving structures around.

* WIP calculate_group_centers

* WIP: assemble combined point_sum arrays

* To handle case when test poly is a single (2,)

* WIP: calculate point_sum_

* WIP: reformatted

* MATLAB-like result structure instead of dictionary

* De-index structure properly.

* WIP: distance_to_inner_centre

* WIP: group_centres_2d done

* calculate_group_centers runs

* Removing unused variables

* Verifying calculate_group_centers, sort of.

* Cleanup

* Cleaning up debug information"
25,679dc764c0da9620d95cc4c4,commit,documentation -> improving writing,https://github.com/ben-manes/caffeine/commit/d0dab86b751099e01110503edb6ecf9f3230cf5f,improved the javadoc for readability using suggestions from chatgpt
26,679dd5b6c0da9620d95cd570,commit,"development environment -> continuous integration -> generating/updating docker container,development environment -> continuous integration -> implementing/fixing jobs/actions",https://github.com/versatiles-org/versatiles-rs/commit/8ec83eead1e1c45d8a2f089781ee4dcd5fe6b7aa,"Fix linux release and fix #36 (#37)

* don't ci on workflows
* fix target
* add musl
* rewrite cross.toml
* disable mac
* don't fail fast
* adding improvements from ChatGPT
* add libwebp-dev
* test using the correct architecture
* fix target directories
* testing images
* typo?
* test Dockerfile
* apt
* disable mac
* try building and running docker in github workflow
* add name
* maybe add push?
* setup qemu and buildx
* what does IT stand for?
* maybe ...
* extract from docker
* only binaries
* set bin
* Update aarch64-unknown-linux-musl.Dockerfile
* Update aarch64-unknown-linux-musl.Dockerfile
* Update aarch64-unknown-linux-musl.Dockerfile
* the big test
* fix
* add GH token
* Update test.yml
* Update release, don't duplicate
* merge test into release_binaries
* add cargo test
* more zen
* activate macos
* cleanup code
* remove cross.toml
* add some notes"
27,679de029c0da9620d95ce1cb,commit,feature implementation/enhancement -> implementing a new feature,https://github.com/1aienthusiast/audiocraft-infinity-webui/commit/4ba6c6cceae2b2610945b9a870ab4cdf5a34f3d8,"Add files via upload

I saw https://github.com/1aienthusiast/audiocraft-infinity-webui/issues/12 wanted batch mode, I also wanted this feature, so with the help of ChatGPT I was able to create a 'webuibatch.py'  file, it is basically the same thing as webui.py but it has a ""batch mode"" checkbox and then an ""infinite"" checkbox to just keep it running... so far it is working for me in infinite mode... also I made it so it doesn't play the music when in batch mode, since you are just letting it run making music"
28,679e2bd7c0da9620d95d53bf,commit,development environment -> continuous integration -> integrating ChatGPT reviewing in CI,https://github.com/linux-odyssey/linux-odyssey/commit/d0186727ced8b5707d07c7fa80f5ebd9c0e055ee,feat: add chatgpt code reivew
29,679e34abc0da9620d95d57a4,commit,software quality -> refactoring,https://github.com/vortext/esther/commit/9e9fd50fa2b5031f93bb2d18579d69e036b954b4,"Minor improvement to seq->memory

I blame ChatGPT for this sillyness."
30,679e3853c0da9620d95d60c5,commit,documentation -> improving writing,https://github.com/NetFabric/NetFabric.Numerics/commit/5c9cb3ff512e1f450fd9042b3066b510c4372f7b,Review with the help of ChatGPT
31,679e3931c0da9620d95d63b3,commit,feature implementation/enhancement -> implementing a new feature,https://github.com/jonstuebe/jonstuebe.com/commit/33781a3793186ec552c4835fb240fd087fa6932e,feat: added a parallax card (written with the help of chatGPT 4 ) (#12)
32,679e3931c0da9620d95d63b9,commit,software quality -> testing -> generating test data,https://github.com/wikimedia/mediawiki-tools-codesniffer/commit/c2f2b1282de0f2e9b453e882d5312cd856b24c97,"Add test for awfully inconsistent code

I've asked ChatGPT to generate a snippet of highly inconsistent PHP
code. Then I made it even worse. This should give us a test with some
extreme edge cases to see what our PHPCS standards do or do not catch.

For this test, the .fixed result is particularly interesting, as it
shows what PHPCS would consider correct after a pass of phpcbf. This can
be compared to what a human would deem to be correctly-formatted code.

Change-Id: Ifd4661bf5e722aa500140b6a4651481dc342116a"
33,679e4532c0da9620d95d725a,commit,development environment -> continuous integration -> generating/updating docker container,https://github.com/perfectly-preserved-pie/larentals/commit/070a74f3b158eaf570f9ea9b8f6243331f2dc9c1,This is why you don't use ChatGPT for Docker command syntax >:|
34,679e6280c0da9620d95d9e03,commit,documentation -> improving writing,https://github.com/chibiace/ComfyUI-Chibi-Nodes/commit/622e4774b941a2216f0a4aad06da22dd575fa6c4,"Update README.md

used chatgpt to maybe make it more coherent."
35,679e6cb9c0da9620d95da46e,commit,generating/manipulating data,https://github.com/jonathanpeppers/inclusive-code-reviews-ml/commit/1d7f29c2915c2913d0b338ccf3b9abb09aaec2cb,"Used ChatGPT 4 to classify more data (#149)

Context: https://github.com/dwyl/english-words

I uploaded a text file of every English word, and asked ChatGPT 4 to
classify it:

    This is a document with every English word. You are an expert software
    engineer that is particularly good at writing well written, thoughtful
    code reviews. Find the 10 ""most rude"" words you would not want to see in
    a code review

I then:

* Expanded to 50 words

* Asked it to create a CSV file I could copy-paste

* Asked it to turn the words into a sentence

* Asked it to generate another version with the words subtly misspelled

* Asked it to generate a version with compliments instead of rude words

This generated 100s of examples that work great for training our model."
36,679e6eb7c0da9620d95daaa3,commit,feature implementation/enhancement -> implementing a new feature,https://github.com/gregorriegler/mob-programming-rpg/commit/49311ab5357663630bf870cef91c2b67da98006a,"Merge pull request #9 from gregorriegler/session-note-cleanup

session-note cleanup (via ChatGPT generated code)"
37,679e7a7bc0da9620d95db64c,commit,feature implementation/enhancement -> feature enhancement -> user experience -> gui-related improvements,https://github.com/OpenPF2/Playground/commit/227944e85bd90804504620a79b1ce6f2993217d7,"[#9] [#53] Define Color Mappings for Damage Types

Credit to ChatGPT for helping with picking the colors and transcribing
(mostly correct) values to BGRA for DT import from CSV.

Source:
https://chat.openai.com/share/b4563864-14e9-4b5b-bb15-edd25737c47c"
38,679e7e2bc0da9620d95dc0b8,commit,feature implementation/enhancement -> migrating/reusing via programming language translation,https://github.com/linduxed/dotfiles/commit/01f5c5112ff208455499c59c6d45e976c2ab23d3,"Fish: translate `fgcpm` function from Zsh

With the help of ChatGPT I managed to translate the `fgcpm` function
(from `.zsh_programming_alias`) to a Fish function.

The main difference compared to the Zsh function is the addition of
a feature that I've been wanting for a long time: filtering the commits
to eliminate duplicate commit titles. I wanted to eliminate on duplicate
commit bodies, but this turned out to be a bit too difficult to achieve
(removing the newlines in the bodies, but not the newlines between the
commits), but just filtering on the titles is fine."
39,679e8d78c0da9620d95ddaea,commit,documentation -> improving writing,https://github.com/CPS-IT/project-builder/commit/885fb4890cfd2a5dfee7699f8a0b6b4ea1c3c661,[DOCS] Follow ChatGPT for language washing in documentation
40,679e97ddc0da9620d95de2c4,commit,documentation -> generating readme,https://github.com/Xetera/ghost-cursor/commit/a1a26eec700cbf5f655e8b1696ee01a8083a16c1,"ChatGPT generated methods documentation (#116)

* documentation methods improvements + hesitate option for click method"
41,679ea4a4c0da9620d95df164,commit,"documentation -> generating readme,documentation -> improving writing",https://github.com/brucepro/Memoir/commit/d9ab3a8f48a9e6fbd600f45bf9eabb00f353b495,"Update README.md

https://chat.openai.com/g/g-wpMtgVmzG-readme-generator

Just used chatgpt readme generator to edit the readme, 
not saying the one you made was bad or anything, 

just tryna contribute! :D"
42,679ea5a9c0da9620d95df325,commit,documentation -> generating readme,https://github.com/Jaseci-Labs/jaclang/commit/3bfa613f8fdbcbbbb5937729411695f02c27f698,doc: Trying to use chatgpt for documentation but its more time consuming than just writing the damn thing
43,679eaa0ac0da9620d95e0088,commit,software quality -> fixing,https://github.com/fdo-rs/fido-device-onboard-rs/commit/860772db5c48c69c3f3271e0b8ee48b00ce8ba4b,"chore: fix require error with commitlint

Changed the commitlint config file to use a dynamic import of
`commitlint/esnure` and updated the function to await the promise of
the import.

(This code was generated by ChatGPT and appears to fix the error with
using `require('@commitlint/ensure')`)

Closes: #633

Signed-off-by: Micah Abbott <miabbott@redhat.com>"
44,679eb5f0c0da9620d95e0ded,commit,documentation -> generating api specification,https://github.com/COMBINE-lab/grangers/commit/7dcfe4e80c4a9361a9d8da3124961980815257ef,add rustdoc for functions with the help from chatgpt
45,679eb6d2c0da9620d95e11b5,commit,feature implementation/enhancement -> feature enhancement,https://github.com/theothernt/AerialViews/commit/95e3c68e16b708fc45bb4d7d93ca4d679066a01d,"Check if city name can be provided in local lang

Used ChatGPT to take the lang list from the docs and format it into Kotlin"
46,679ed052c0da9620d95e2f21,commit,development environment -> continuous integration -> integrating ChatGPT reviewing in CI,https://github.com/ryanw-mobile/giphy-trending/commit/e5c82178255c9230e9482216fe17b3080c5324e1,[chore] Github Actions: experimental yaml changes and ChatGPT code review
47,679ededfc0da9620d95e46e5,commit,documentation -> improving writing,https://github.com/RedsXDD/neopywal.nvim/commit/5ff63e90bbe09c35da54184cdceb29312f6cb3ec,"Improved READEME.

If you're wondering, yes, i used chatgpt to help write the text xdddd."
48,679eeca1c0da9620d95e5884,commit,"feature implementation/enhancement -> feature enhancement -> user experience -> gui-related improvements,software quality -> fixing",https://github.com/pyanodon/pyalienlife/commit/5552d082162ed7fbcd2544170fef4a9d57d5d459,"Frozen (#243)

* Frozen (#242)

* Frozen (#240)

* make smart farms flora possitive

* version and changelog

* fix that collectors mk02-mk04 didnt get reset

fixes pyanodon/pybugreports#461

* i blame theros ass

* Smart farm fixes

* https://github.com/pyanodon/pybugreports/issues/458

* TURD balance & migrations

* Fixed that subcritical-water-01 was not localised when pyAE is missing

* Fixed that the alpha dingrits from dingrit turd path 1 could not be used as modules inside the reproductive complex.

* Energy drink qol improvements

* Fixed several bugs involved with biofluid vessel underground pipes

* Fixed several bugs involved with biofluid vessel underground pipes

* remove print statements

* Reduce earlygame manual wood requirement grind.

---------

Co-authored-by: kingarthur91 <arthurkoy_durbin@yahoo.com>

* Fixed returned cages (#235)

* Add files via upload

* Fixed returned cages

* Fixed returned cages

* Fixed returned cages

* Returned cages fixed

* Fixed cage return

* Global Caravan handling GUI (#222)

* list of clickables everything is broken though

* click opens the thing

* moved to a new file-> todo prevent item stealing
list by outpost
scrollpane

* outpost gui

* status icon

* less loc

* new state idle

* pasting untested stuff from chatgpt

* ho

* more stuff that will certainly work nocap

* connected gui works

* lets try this migrations thingy

* fixed the migration

* use the migration

* fixed the close button...

* cleanup code a bit

* fixed the esc key

* localization

* localization

* ship it

* new rule I like open better

* wuuups

* I broke esc again...

* Apply pyanodon code formatting standard

* Remove references to stdlib functions as part of an ongoing effort to remove stdlib.

* Streamline the amount of code needed to handle GUI closing events

* Make the GUI expand to the container instead of having a hardcoded size

* Improve the relative GUI type guessing algorithm

* Move hotkey based GUI to pycodex and add a camera

* Add more buttons and finalize styling

---------

Co-authored-by: notnotmelon <zacharypicco@gmail.com>

* Add a rename button to the original caravan GUI

* Add some randomly generated placeholder names for caravans.

---------

Co-authored-by: kingarthur91 <arthurkoy_durbin@yahoo.com>
Co-authored-by: Wisey <44213571+Wiseyuk@users.noreply.github.com>
Co-authored-by: TheGoesen <31041434+TheGoesen@users.noreply.github.com>

* Fix vonix cages issue introduced in the PR yesterday

* Fixed unknown key error in the new caravan GUI

* Day 1 patch for caravan GUI

* https://github.com/pyanodon/pybugreports/issues/469

* https://github.com/pyanodon/pybugreports/issues/471

* Add a searchbar to the caravan global gui

* Removed the incompatibility with omnimatter wood.

* Buffed korlex path 2

* Removed references to nauvis in the locale

* Removed references to nauvis in the locale

* Buffed fawogae module breeding

* Update luatest to include the frozen branch

* Moved the ralesia turd from py4 to chem science. Also changed the text description on path 3.

* Fix caravan copypast

* nevermind this causes issues

* increase version number

* Fixed that nexelit ore had broken graphics in low-resolution mode.

* Added compatibility between caravans and the remote configuration mod

* path_resolution_modifier for caravans

* Actually, lets only do this for aerials

* added tier markers to rennea and arum seeds

* 10X improvement for biofluid UPS

* Increase version number

---------

Co-authored-by: kingarthur91 <arthurkoy_durbin@yahoo.com>
Co-authored-by: Wisey <44213571+Wiseyuk@users.noreply.github.com>
Co-authored-by: TheGoesen <31041434+TheGoesen@users.noreply.github.com>"
49,679eee90c0da9620d95e5f2f,commit,feature implementation/enhancement -> implementing a new feature,https://github.com/BarthPaleologue/CosmosJourneyer/commit/26dbb7ca64bc0f10c416c68a9fd255196bc03e67,"added channel merging python script

credit chatgpt for initial version + me for the final version with arbitrary channel merging"
50,679eef2fc0da9620d95e5f88,commit,documentation -> generating readme,https://github.com/ryanw-mobile/OctoMeter/commit/f313d81ffaa3cb4cdc60d19b28b009840d5e2405,[docs] README.md: add ChatGPT generated dependency list
51,679efb24c0da9620d95e6c31,commit,documentation -> generating api specification,https://github.com/Jpickard1/BRAD/commit/89ba14e50ad4337e96c27bcbf376372754471b01,boiler plate docstrings were added w help of chatgpt to show how the documentation will be populated for the main brad.py file and the rag.py file. This documentation needs to be reviewed closer + put in a consistent format + added for all other files. Also we might want to rearrange the order of the functions in the .py files to make the documentation more readable. I should discuss this with cooper
52,679efd00c0da9620d95e71e8,commit,feature implementation/enhancement -> implementing a new feature,https://github.com/coolxv/cpp-stub/commit/7ee41b7085c1e494c5d39e4b0a6ac5434111a7ab,"ppc64,alpha,sparc,sw_64 generated by chatgpt"
53,679f09e6c0da9620d95e83ed,commit,generating/manipulating data,https://github.com/seniordev1012/llm-rag-agent/commit/cf53e48b5754cfd7964571c9a4465e92edfe8e66,"RU Transaltions (#1734)

* ru language added

* uncheck bad changes
Use chatGPT to create new dictionary for RU translation
turn off debug mode in produ for i18n
patch issue with null comparison in verifyTranslation.js

---------

Co-authored-by: UUSR <konstantik@gmail.com>"
54,679f0b18c0da9620d95e8785,commit,learning -> how to use a library/framework/tool -> generating code examples,https://github.com/srinathv/Coding-Examples/commit/23fa556dfbb72e1db4d856346321045e575b73cf,From chatGPT
55,679f0b18c0da9620d95e8817,commit,documentation -> generating code comments,https://github.com/Agenta-AI/agenta/commit/c96d25a347415027e56ee2d61612cd1eed5368ac,remove comment from chatgpt
56,679f0c2dc0da9620d95e8a5f,commit,feature implementation/enhancement -> feature enhancement,https://github.com/brhahlen/dc/commit/cc36f150b3d240c190ab59372711f433305f9b3c,Updated with suggestions from ChatGPT
57,679f0cd6c0da9620d95e8beb,commit,software quality -> fixing -> functional bugs,https://github.com/rust-bitcoin/rust-bitcoin/commit/74519c3e55146ce8458a8a45592e5d8ed3b009b8,"Merge rust-bitcoin/rust-bitcoin#2925: Fix fuzz script

31a2324bfddb862fb2bf5c9a54078039bcedc326 Fix fuzz script (Tobin C. Harding)

Pull request description:

  Currently the `fuzz.sh` script fails with ""unbound variable"" if called without any arguments, this has gone undetected since we added `set -euox pipefail` because in CI we always call it with an argument.

  Use chatGPT to fix the bug.

  Fix: #2924

ACKs for top commit:
  apoelstra:
    ACK 31a2324bfddb862fb2bf5c9a54078039bcedc326

Tree-SHA512: 28e018bafd02c4034863c84e3b491c83962379d41bddeae184c4e35b889cb3e5aed9605b01749a97c60e3090525db52786574e487ff20cbf61fbdc8eaa8ab5a0"
58,679f1a39c0da9620d95e9c56,commit,documentation -> improving writing,https://github.com/SSWConsulting/SSW.Rules.Content/commit/cbd09ea53bcb6409ec07605570073f61e74f2249,"XSâœ” â—¾ Replaced broken link with content from ChatGPT (#8866)

* Replaced broken link with content from ChatGPT

* Auto-fix Markdown files

* reduced image size for better readability

* Auto-fix Markdown files

---------

Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>"
59,679f2357c0da9620d95ea14d,commit,feature implementation/enhancement -> feature enhancement,https://github.com/excaliburjs/Excalibur/commit/2745795a95be0c72655fa050adb55d245f557f86,"fix: rewrite ClosestLine and PolygonPolygonClosestLine logic for improved accuracy on sloped polygons (#3124)


Improves accuracy of PolygonPolygonClosestLine and ClosestLine functions. Big disclaimer, I heavily relied on ChatGPT for the math logic here... extra eyes greatly appreciated. I myself plan to do a deeper review on the math here before merging.

Before:

https://github.com/user-attachments/assets/a0d9d5d5-76e9-4e05-8459-15b6a48b0dcb

After:


https://github.com/user-attachments/assets/85d4410b-2bf2-4f51-90e2-4ee828a9693b


I also think there's a faster way to do the `PolygonPolygonClosestLine` logic. Right now it iterates over each side of the polygon, draws a line, and returns the shortest result. Potentially it could use SAT to determine the closest sides, but I had unexpected results

```ts
PolygonPolygonClosestLine(polygonA: PolygonCollider, polygonB: PolygonCollider) {
    const aSat = SeparatingAxis.findPolygonPolygonSeparation(polygonA, polygonB);
    const bSat = SeparatingAxis.findPolygonPolygonSeparation(polygonB, polygonA);

    return ClosestLine2(aSat.side, bSat.side);
}
```

https://github.com/user-attachments/assets/be6460cb-7786-4864-93aa-d8053478aa53"
60,679f2575c0da9620d95ea61a,commit,feature implementation/enhancement -> implementing a new feature,https://github.com/tjrantal/Direct-Linear-Transformation/commit/72325bc30af4770d00e1181dbdae1d6987154188,"not change anything, started thinking about pseudo inverse solution. Implementation from chatGPT"
61,679f2649c0da9620d95ea980,commit,development environment -> setting/implementing a new configuration file,https://github.com/peterhcharlton/ppg-beats/commit/80c54105bd6f7a2fde78a991574a41e5b968925f,"Create .readthedocs.yaml

Attempted from https://blog.readthedocs.com/migrate-configuration-v2/ and modifications from chatgpt"
62,679f2649c0da9620d95eabf1,commit,feature implementation/enhancement -> scripting,https://github.com/joshukraine/dotfiles/commit/615a0f2effa0e78eacc1174a7f962a75f468a92d,"Add download script

- Learned this from ChatGPT
- Useful for downloading files from GitHub"
63,679f363bc0da9620d95ec2ef,commit,feature implementation/enhancement -> feature enhancement -> upgrade to latest version of a library,https://github.com/nvlgzr/fngrng/commit/a5f44bb9a4464ab93ef95b8f271bce9ca73acb56,"Reworked Hoverable to account for breaking change from Svelte 3 to Svelte 4

Specifically, https://svelte.dev/docs/v4-migration-guide#default-slot-bindings

This proved to be a good use of ChatGPT for accomplishing the task with minimal effort on my part. Normally, I would prefer to learn the details myself rather than relying on AI, but it had been a long time since I last worked with Svelte slots, and they're about to be made completely obsolete by Svelte 5, which is honestly the only reason Iâ€™m even bothering with the update to Svelte 4."
64,679f439ec0da9620d95ed535,commit,software quality -> code review,https://github.com/mikiepure/pydlt/commit/4fd161516c67fad4f89dca7f5247a7a7f2c726ac,Fix according to ChatGPT code review
65,679f443ac0da9620d95ed974,commit,documentation -> improving writing,https://github.com/great-majority/KoikatuCharaLoader/commit/e4b516f9e800f3041dfd19025ee8baff138d60ed,Refactor instructions based on feedback from ChatGPT
66,679f6be1c0da9620d95f0bde,commit,generating/manipulating data,https://github.com/clp-research/clembench/commit/4d3c6c83cebe46e047b764cb33344d71b305594a,"Including Codenames in clembench (#113)

* Add a first mock implementation of the original codenames (without context)

* Fixing typos in framework

* Added type annotations, fixed prompt substitution of sets, fixed some validation errors, fixed gameplay aborting on failed guesser validation

* Add logging of validation errors

* Added a reprompting mechanism to the dialogue game master

* Add scoring to gamemaster

* Refined scoring function and extracted CodenamesBoard class

* Added some preliminary functions and todos to instancegenerator

* Small changes to cluegiver prompt

* Extracted scoring methods into a GameScorer class and implemented its usage in taboo (still needs to be used for the other four standard games)

* Implemented required metrics using the new GameScorer class

* Cleaned scoring and metrics

* Changed prompts, added intermittent prompts and allowed some slack for model answers

* Refactored usage of strings for lookups to use constants and enums

* Implemented simple mock opponent player that reveals a number of their cards each turn

* HF backend refactor (#37)

- Model settings are now stored in external JSON model registry
- Two new prototyping methods to check messages for issues and context limit
- General Exception to handle exceeding context
- Gated model (Llama2) handling integrated into single HF backend
- New and updated documentation

* - use huggingfaces' cache lookup mechanism

* Added missing turn_log case for scoring

* New bench score calculation

* Benchscore is NaN when game was aborted

* Enable reprompting in codenames, adding a flag for logging messages

* Updated prompts, number of targets gets inferred now, validation error messages are reported back on reprompt

* Main Score is now harmonic mean of success (avg target f1) and efficiency (number of turns)

* Fix small scoring errors, add flags to ignore correctable errors

* Introduced flags that ignore rambling and false targets/guesses for development purposes

* Changed prompt template to require Clue and Targets on two lines

* logging how many rambles and false targets/guesses were ignored on game end

* Prepared instancegenerator for multiple experiments

* Small changes to prompts and game

* Fixing typos

* InstanceGenerator now takes command-line arguments and replaces old instances if only generating instances for a specific experiment

* Copied categories.json and code for simple instances from jupyter notebook

* Generate instances for difficult assignments across teams

* Trying to generate all experiment instances now skips to next when wordlist is not available

* Added default.json made from categories.json and respective game instances

* Added a wordlist_cleaner, already cleaned wordlist and generated new instances

* Fix small errors

* Add some more validation checks, morphological similarity check not used yet, but is there

* Started working on a score and error evaluation script

* Added validation error classes to better log and evaluate errors

* Started working on error evaluation script

* Worked more on codenames specific scoring tables

* More flags, more logs, changed scores

* Log model response when log_messages is true

* Adapted score logging and main result score computation

* Cleaned todos in code

* Small bugfix after merging

* Small changes to prompts

* Reduced number of experiments for testing purposes

* More fixes after merge

* Changed number of original word examples down to default (3)

* Cleaning and improving flag usage, logging, constants, and respective errors

* Split up classes into separate files

* Logging target assignments now for cluegiver precision measurement

* Cleaned comments, included morphology check

* Small prompt changes

* Changed prompts with help of ChatGPT

* Some small fixes and changes to error messages

* Add wordlists and cleaned wordlists for word frequency experiments

* Keep old results but ignore folder in .gitignore

* Add pipeline script to run codenames for multiple models

* Fixed some errors in scoring and gameflow

* New instances, now 10 per experiment

* Remove alphabetical check in clue, does not happen that often and should not be punished

* Added more turn scores

* Some todos and small changes

* Fixed errors when running new scoring

* Fixed some more errors with new scoring

* Changed evaluation script, aborted games should now get NaN values (not tested yet)

* Mock player only picks one word each turn instead of two

* Fixed error in setting metrics to NaN when aborted

* Making sure that experiment scores remain in order

* Small fixes

* Bugfixes and renamings

* Added last cleaned wordlists, generated new generous and strict instances

* Bugfix where model would guess the same word twice in strict mode caused a crash

* Revised frequency wordlists

* Refactored instancegenerator to have an option to generate strict instances automatically

* Fixed frequency wordlists being in the wrong order

* Added an ideal n=1 mock player

* Changed efficiency calculation

* Add more validation errors and fixed non-randomized order in guesser prompts

* Fixed errors in model evaluation

* Always allow words to be stripped and handling some more edge cases in response parsing and validation

* Refactored validation errors, added some more

* Completed pipeline script with scoring, evaluation, and deleting result files before running

* Sample categories based on category size, generated new set of instances

* Removed todo comments

* Added feedback for guesser and cluegiver about the guessers guesses

* Small bugfixes

* Flag and request counts are now summed instead of averaged

* Bugfixing in score evaluation

* Fixed reproducibility bug in instances

* Automating latex table generation and prettifying tables overall

* Small improvements

* Moved evaluation scripts into codenames game folder

* Reduced experiments.json but kept old state in all_experiments.json

* Small cleanups

* Add README

* Extended README

* Renamed experiment config files

* Replaced match-case with if-elif statements for python3.9 compatibility

* Renaming instances files, generating new instances for clembench

* Fixed syntax error in scorer

---------

Co-authored-by: Jonathan Jordan <knawurzelkopp@hotmail.de>
Co-authored-by: Philipp Sadler <philipp.sadler@gmail.com>"
67,679f6cf3c0da9620d95f0e85,commit,software quality -> code review,https://github.com/project-gemmi/gemmi/commit/8ad0fcf17e4a6771b6ea9f23c76b14f66890e863,"edit docs

cell.rst was reviewed with the help of ChatGPT 4o mini"
68,679f7ba5c0da9620d95f229e,commit,feature implementation/enhancement -> implementing a new feature,https://github.com/workadventure/workadventure/commit/cf4a8c7af2971e4d95756b9731cd9286e3b0077f,"Using a custom made dump function

Removing circular-json

Replacing it with a custom made dumper (thanks ChatGPT for the help)"
69,679f95aac0da9620d95f43a4,commit,software quality -> testing -> generating tests,https://github.com/Pikachuxxxx/Razix/commit/13b9e3be816a2664bf2c747ad8fa60886f311429,Added RZHandle Tests : chatgpt generated test cases in 2 min
70,679f978fc0da9620d95f4899,commit,documentation -> generating readme,https://github.com/VanceVagell/kv4p-ht/commit/c5d9b58fab6e4c3335d7b2975a3b537e8fb47fce,"had chatGPT create a readme.md for interfacing (even though the Android device does all the work) (#118)

* had chatGPT create an readme.md

This is an ICD that was generated by ChatGPT

* Update readme.md

Fixed detail on squelch, from being binary off and on (0-1) to be a range of 0-8 based on feedback from author"
71,679f98b7c0da9620d95f4bd4,commit,documentation -> generating readme,https://github.com/filecoin-project/filecoin-docs/commit/d4bca8d656c572f0ebdeed7c308381e46b386572,"Add information about Glif Nodes subgraphs (#2336)

* Create fevm-indexers.md

from template from ChatGPT and my manual patches

* Update fevm-indexers.md

More of my fixes

* Update fevm-indexers.md

* Update smart-contracts/advanced/fevm-indexers.md

Co-authored-by: Steve Loeppky <stvn@loeppky.com>

* Update smart-contracts/advanced/fevm-indexers.md

Co-authored-by: Steve Loeppky <stvn@loeppky.com>

* Update smart-contracts/advanced/fevm-indexers.md

Co-authored-by: Steve Loeppky <stvn@loeppky.com>

* Update fevm-indexers.md

* Update smart-contracts/advanced/fevm-indexers.md

Co-authored-by: Steve Loeppky <stvn@loeppky.com>

* Update fevm-indexers.md

made some minor changes on words and formating.

* Remove ""Although it is not very complicated""

* Patch .gitbook.yaml

* Patch summary

---------

Co-authored-by: Steve Loeppky <stvn@loeppky.com>
Co-authored-by: longfeiW <34025001+longfeiWan9@users.noreply.github.com>"
72,679f999ec0da9620d95f50d4,commit,software quality -> optimization,https://github.com/transientskp/pyse/commit/37466a738822ca011ab7d56df2f8775e3d10096e,"Faster way of calculating 'slices_to_indices'.

This way of calculating 'slices_to_indices' is faster by a factor 2.7 for a large image with 167,000 sources. Accelerated with help from ChatGPT 4.0. The changes seem trivial, but apparently have a considerable effect on the speed."
73,679fa56ec0da9620d95f5b3b,commit,generating/manipulating data,https://github.com/Vretu-Dev/UsefulHints/commit/d77c2f6bd0e74d141b1fb798d678188d7c588205,More Translations from ChatGPT
74,679a7a2ecbe8284d7a71c7df,issue,learning -> how to use a library/framework/tool,https://github.com/SNNafi/media_store_plus/issues/1,"[Help Wanted] Should I use it given these circumstances
Hi @SNNafi , first of all thank you for your work on this package!

If you can, could you please help me sort out what I should do given the following circumstances? I have read all the official docs, searched on StackOverflow and asked ChatGPT in all kind of ways but it's still unclear for me what should be done to **migrate existing video files created by my app to the `Movies` directory**.

My [open source video diary](https://github.com/KyleKun/one_second_diary) update got rejected by Google because they won't allow the manage all files permission, which I was using to avoid such issues on Android 13. The published version of my app created a directory in the root path (Internal Storage > MyAppFolder) and saved videos there. The sdk target in manifest is `targetSdkVersion 29`.

From what I have seen in the [migration docs](https://developer.android.com/training/data-storage/use-cases#migrate-legacy-storage), I should keep the `targetSdkVersion 29`, keep the `android:requestLegacyExternalStorage=""true""` and then move the files and delete this original folder. Then after all users updated and migrated the files, I should target a newer sdk and keep saving files there (I think the `Movies` folder makes more sense).

This file moving process should be done with your package in order to register them in the MediaStore API? Will this work for any Android version or should I do different things for Android 10 and lower? I would like to preserve these files even if the user unninstalls the app, so I want to be able to still have access to them after a reinstall. And after the migration is complete, should I use your package to write the files?

I will be extremely grateful if you could clarify these points, my app is ready to update with plenty new features and this is the last thing holding me back. Thanks so much!


"
75,679a7a2ecbe8284d7a71c854,issue,software quality -> fixing -> functional bugs,https://github.com/kohya-ss/sd-webui-additional-networks/issues/43,"fix to the: AttributeError: 'JSON' object has no attribute 'value'
Was getting the error after automatic1111's update so i used chatgpt to fix it ...

not sure how Github works with fixes to errors so i gonna put it here:

````
import os
import glob
import zipfile
import json
import stat
import sys
import inspect
from collections import OrderedDict

import torch

import modules.scripts as scripts
from modules import shared, script_callbacks
import gradio as gr

from modules.processing import Processed, process_images
from modules import sd_models
import modules.ui

from scripts import lora_compvis


MAX_MODEL_COUNT = 5
LORA_MODEL_EXTS = ["".pt"", "".ckpt"", "".safetensors""]
lora_models = {}      # ""My_Lora(abcd1234)"" -> C:/path/to/model.safetensors
lora_model_names = {} # ""my_lora"" -> ""My_Lora(abcd1234)""
lora_models_dir = os.path.join(scripts.basedir(), ""models/lora"")
os.makedirs(lora_models_dir, exist_ok=True)


def traverse_all_files(curr_path, model_list):
  f_list = [(os.path.join(curr_path, entry.name), entry.stat()) for entry in os.scandir(curr_path)]
  for f_info in f_list:
    fname, fstat = f_info
    if os.path.splitext(fname)[1] in LORA_MODEL_EXTS:
      model_list.append(f_info)
    elif stat.S_ISDIR(fstat.st_mode):
      model_list = traverse_all_files(fname, model_list)
  return model_list


def get_all_models(sort_by, filter_by, path):
  res = OrderedDict()
  fileinfos = traverse_all_files(path, [])
  filter_by = filter_by.strip("" "")
  if len(filter_by) != 0:
    fileinfos = [x for x in fileinfos if filter_by.lower() in os.path.basename(x[0]).lower()]
  if sort_by == ""name"":
    fileinfos = sorted(fileinfos, key=lambda x: os.path.basename(x[0]))
  elif sort_by == ""date"":
    fileinfos = sorted(fileinfos, key=lambda x: -x[1].st_mtime)
  elif sort_by == ""path name"":
    fileinfos = sorted(fileinfos)

  for finfo in fileinfos:
    filename = finfo[0]
    name = os.path.splitext(os.path.basename(filename))[0]
    # Prevent a hypothetical ""None.pt"" from being listed.
    if name != ""None"":
      res[name + f""({sd_models.model_hash(filename)})""] = filename

  return res


def find_closest_lora_model_name(search: str):
    if not search:
        return None
    if search in lora_models:
        return search
    search = search.lower()
    if search in lora_model_names:
        return lora_model_names.get(search)
    applicable = [name for name in lora_model_names.keys() if search in name.lower()]
    if not applicable:
        return None
    applicable = sorted(applicable, key=lambda name: len(name))
    return lora_model_names[applicable[0]]


def update_lora_models():
  global lora_models, lora_model_names
  res = OrderedDict()
  paths = [lora_models_dir]
  extra_lora_path = shared.opts.data.get(""additional_networks_extra_lora_path"", None)
  if extra_lora_path and os.path.exists(extra_lora_path):
    paths.append(extra_lora_path)
  for path in paths:
    sort_by = shared.opts.data.get(""additional_networks_sort_models_by"", ""name"")
    filter_by = shared.opts.data.get(""additional_networks_model_name_filter"", """")
    found = get_all_models(sort_by, filter_by, path)
    res = {**found, **res}
  lora_models = OrderedDict(**{""None"": None}, **res)
  lora_model_names = {}
  for name_and_hash, filename in lora_models.items():
      if filename == None:
          continue
      name = os.path.splitext(os.path.basename(filename))[0].lower()
      lora_model_names[name] = name_and_hash


update_lora_models()


class Script(scripts.Script):
  def __init__(self) -> None:
    super().__init__()
    self.latest_params = [(None, None, None)] * MAX_MODEL_COUNT
    self.latest_networks = []
    self.latest_model_hash = """"

  def title(self):
    return ""Additional networks for generating""

  def show(self, is_img2img):
    return scripts.AlwaysVisible

  def ui(self, is_img2img):
    # NOTE: Changing the contents of `ctrls` means the XY Grid support may need
    # to be updated, see end of file
    ctrls = []
    model_dropdowns = []
    self.infotext_fields = []
    with gr.Group():
      with gr.Accordion('Additional Networks', open=False):
        enabled = gr.Checkbox(label='Enable', value=False)
        ctrls.append(enabled)
        self.infotext_fields.append((enabled, ""AddNet Enabled""))

        for i in range(MAX_MODEL_COUNT):
          with gr.Row():
            module = gr.Dropdown([""LoRA""], label=f""Network module {i+1}"", value=""LoRA"")
            model = gr.Dropdown(list(lora_models.keys()),
                                label=f""Model {i+1}"",
                                value=""None"")

            weight = gr.Slider(label=f""Weight {i+1}"", value=1.0, minimum=-1.0, maximum=2.0, step=.05)
          ctrls.extend((module, model, weight))
          model_dropdowns.append(model)

          self.infotext_fields.extend([
              (module, f""AddNet Module {i+1}""),
              (model, f""AddNet Model {i+1}""),
              (weight, f""AddNet Weight {i+1}""),
          ])

        def refresh_all_models(*dropdowns):
          update_lora_models()
          updates = []
          for dd in dropdowns:
            if dd in lora_models:
              selected = dd
            else:
              selected = ""None""
            update = gr.Dropdown.update(value=selected, choices=list(lora_models.keys()))
            updates.append(update)
          return updates

        refresh_models = gr.Button(value='Refresh models')
        refresh_models.click(refresh_all_models, inputs=model_dropdowns, outputs=model_dropdowns)
        ctrls.append(refresh_models)

    return ctrls

  def set_infotext_fields(self, p, params):
    for i, t in enumerate(params):
      module, model, weight = t
      if model is None or model == ""None"" or len(model) == 0 or weight == 0:
        continue
      p.extra_generation_params.update({
          ""AddNet Enabled"": True,
          f""AddNet Module {i+1}"": module,
          f""AddNet Model {i+1}"": model,
          f""AddNet Weight {i+1}"": weight,
      })

  def process(self, p, *args):
    unet = p.sd_model.model.diffusion_model
    text_encoder = p.sd_model.cond_stage_model

    def restore_networks():
      if len(self.latest_networks) > 0:
        print(""restoring last networks"")
        for network, _ in self.latest_networks[::-1]:
          network.restore(text_encoder, unet)
        self.latest_networks.clear()

    if not args[0]:
      restore_networks()
      return

    params = []
    for i, ctrl in enumerate(args[1:]):
      if i % 3 == 0:
        param = [ctrl]
      else:
        param.append(ctrl)
        if i % 3 == 2:
          params.append(param)

    models_changed = (len(self.latest_networks) == 0)                   # no latest network (cleared by check-off)
    models_changed = models_changed or self.latest_model_hash != p.sd_model.sd_model_hash
    if not models_changed:
      for (l_module, l_model, l_weight), (module, model, weight) in zip(self.latest_params, params):
        if l_module != module or l_model != model or l_weight != weight:
          models_changed = True
          break

    if models_changed:
      restore_networks()
      self.latest_params = params
      self.latest_model_hash = p.sd_model.sd_model_hash

      for module, model, weight in self.latest_params:
        if model is None or model == ""None"" or len(model) == 0:
          continue
        if weight == 0:
          print(f""ignore because weight is 0: {model}"")
          continue

        model_path = lora_models.get(model, None)
        if model_path is None:
          raise RuntimeError(f""model not found: {model}"")

        if model_path.startswith(""\"""") and model_path.endswith(""\""""):             # trim '""' at start/end
          model_path = model_path[1:-1]
        if not os.path.exists(model_path):
          print(f""file not found: {model_path}"")
          continue

        print(f""{module} weight: {weight}, model: {model}"")
        if module == ""LoRA"":
          if os.path.splitext(model_path)[1] == '.safetensors':
            from safetensors.torch import load_file
            du_state_dict = load_file(model_path)
          else:
            du_state_dict = torch.load(model_path, map_location='cpu')

          network, info = lora_compvis.create_network_and_apply_compvis(du_state_dict, weight, text_encoder, unet)
          network.to(p.sd_model.device, dtype=p.sd_model.dtype)         # in medvram, device is different for u-net and sd_model, so use sd_model's

          print(f""LoRA model {model} loaded: {info}"")
          self.latest_networks.append((network, model))
      if len(self.latest_networks) > 0:
        print(""setting (or sd model) changed. new networks created."")

    self.set_infotext_fields(p, self.latest_params)


def read_lora_metadata(model_path, module):
  if model_path.startswith(""\"""") and model_path.endswith(""\""""):             # trim '""' at start/end
    model_path = model_path[1:-1]
  if not os.path.exists(model_path):
    return None

  metadata = None
  if module == ""LoRA"":
    if os.path.splitext(model_path)[1] == '.safetensors':
      from safetensors.torch import safe_open
      with safe_open(model_path, framework=""pt"") as f:
        metadata = f.metadata()

  return metadata


def on_ui_tabs():
  with gr.Blocks(analytics_enabled=False) as additional_networks_interface:
    with gr.Row().style(equal_height=False):
      with gr.Column(variant='panel'):
        with gr.Row():
          module = gr.Dropdown([""LoRA""], label=f""Network module (used throughout this tab)"", value=""LoRA"", interactive=True)
          model = gr.Dropdown(list(lora_models.keys()), label=f""Model"", value=""None"", interactive=True)
          modules.ui.create_refresh_button(model, update_lora_models, lambda: {""choices"": list(lora_models.keys())}, ""refresh_lora_models"")

        with gr.Row():
            with gr.Column():
              gr.HTML(value=""Get comma-separated list of models (for XY Grid)"")
              model_dir = gr.Textbox("""", label=f""Model directory"", placeholder=""Optional, uses selected model's directory if blank"")
              model_sort_by = gr.Radio(label=""Sort models by"", choices=[""name"", ""date"", ""path name""], value=""name"", type=""value"")
              get_list_button = gr.Button(""Get List"")
            with gr.Column():
              model_list = gr.Textbox(value="""", label=""Model list"", placeholder=""Model list will be output here"")

      with gr.Column():
        metadata_view = gr.JSON(data=""test"", label=""Network metadata"")

    def update_metadata(module, model):
      if model == ""None"":
        return {}

      model_path = lora_models.get(model, None)
      if model_path is None:
        metadata_view.data = f""file not found: {model}""

      metadata = read_lora_metadata(model_path, module)

      if metadata is None:
        return ""No metadata found.""
      else:
        return metadata

    model.change(update_metadata, inputs=[module, model], outputs=[metadata_view])

    def output_model_list(module, model, model_dir, sort_by):
        if model_dir == """":
            # Get list of models with same folder as this one
            model_path = lora_models.get(model, None)
            if model_path is None:
                model_list.value = f""directory not found: {model_dir}""
            model_dir = os.path.dirname(model_path)

        if not os.path.isdir(model_dir):
            return f""directory not found: {model_dir}""

        found = get_all_models(sort_by, """", model_dir)
        return "", "".join(found.keys())

    get_list_button.click(output_model_list, inputs=[module, model, model_dir, model_sort_by], outputs=[model_list])

  return [(additional_networks_interface, ""Additional Networks"", ""additional_networks"")]


def update_script_args(p, value, arg_idx):
    for s in scripts.scripts_txt2img.alwayson_scripts:
        if isinstance(s, Script):
            args = list(p.script_args)
            # print(f""Changed arg {arg_idx} from {args[s.args_from + arg_idx - 1]} to {value}"")
            args[s.args_from + arg_idx] = value
            p.script_args = tuple(args)
            break


def confirm_models(p, xs):
    for x in xs:
        if x in ["""", ""None""]:
            continue
        if not find_closest_lora_model_name(x):
            raise RuntimeError(f""Unknown LoRA model: {x}"")


def apply_module(p, x, xs, i):
    update_script_args(p, True, 0)      # set Enabled to True
    update_script_args(p, x, 1 + 3 * i) # enabled, ({module}, model, weight), ...


def apply_model(p, x, xs, i):
    name = find_closest_lora_model_name(x)
    update_script_args(p, True, 0)
    update_script_args(p, name, 2 + 3 * i) # enabled, (module, {model}, weight), ...


def apply_weight(p, x, xs, i):
    update_script_args(p, True, 0)
    update_script_args(p, x, 3 + 3 * i) # enabled, (module, model, {weight), ...


LORA_METADATA_NAMES = {
    ""ss_learning_rate"": ""Learning rate"",
    ""ss_text_encoder_lr"": ""Text encoder LR"",
    ""ss_unet_lr"": ""UNet LR"",
    ""ss_num_train_images"": ""# of training images"",
    ""ss_num_reg_images"": ""# of reg images"",
    ""ss_num_batches_per_epoch"": ""Batches per epoch"",
    ""ss_num_epochs"": ""Total epochs"",
    ""ss_batch_size_per_device"": ""Batch size/device"",
    ""ss_total_batch_size"": ""Total batch size"",
    ""ss_gradient_accumulation_steps"": ""Gradient accum. steps"",
    ""ss_max_train_steps"": ""Max train steps"",
    ""ss_lr_warmup_steps"": ""LR warmup steps"",
    ""ss_lr_scheduler"": ""LR scheduler"",
    ""ss_network_module"": ""Network module"",
    ""ss_network_dim"": ""Network dim"",
    ""ss_mixed_precision"": ""Mixed precision"", 
    ""ss_full_fp16"": ""Full FP16"",
    ""ss_v2"": ""V2"",
    ""ss_resolution"": ""Resolution"",
    ""ss_clip_skip"": ""Clip skip"",
    ""ss_max_token_length"": ""Max token length"",
    ""ss_color_aug"": ""Color aug"",
    ""ss_flip_aug"": ""Flip aug"",
    ""ss_random_crop"": ""Random crop"",
    ""ss_shuffle_caption"": ""Shuffle caption"",
    ""ss_cache_latents"": ""Cache latents"",
    ""ss_enable_bucket"": ""Enable bucket"",
    ""ss_min_bucket_reso"": ""Min bucket reso."",
    ""ss_max_bucket_reso"": ""Max bucket reso."",
    ""ss_seed"": ""Seed"", 
    ""ss_sd_model_name"": ""SD model name"",
    ""ss_vae_name"": ""VAE name""
}


def format_lora_model(p, opt, x):
    model = find_closest_lora_model_name(x)
    if model is None or model.lower() in ["""", ""none""]:
        return ""None""

    value = xy_grid.format_value(p, opt, model)

    model_path = lora_models.get(model)
    metadata = read_lora_metadata(model_path, ""LoRA"")
    if not metadata:
        return value

    metadata_names = shared.opts.data.get(""additional_networks_xy_grid_model_metadata"", """").split("","")
    if not metadata_names:
        return value

    for name in metadata_names:
        name = name.strip()
        if name in metadata:
            formatted_name = LORA_METADATA_NAMES.get(name, name)
            value += f""\n{formatted_name}: {metadata[name]}, ""

    return value.strip("" "").strip("","")


for scriptDataTuple in scripts.scripts_data:
    if os.path.basename(scriptDataTuple.path) == ""xy_grid.py"":
        xy_grid = scriptDataTuple.module
        for i in range(MAX_MODEL_COUNT):
           model = xy_grid.AxisOption(f""AddNet Model {i+1}"", str, lambda p, x, xs, i=i: apply_model(p, x, xs, i), format_lora_model, confirm_models)
           weight = xy_grid.AxisOption(f""AddNet Weight {i+1}"", float, lambda p, x, xs, i=i: apply_weight(p, x, xs, i), xy_grid.format_value_add_label, None)
           xy_grid.axis_options.extend([model, weight])


def on_ui_settings():
    section = ('additional_networks', ""Additional Networks"")
    shared.opts.add_option(""additional_networks_extra_lora_path"", shared.OptionInfo("""", ""Extra path to scan for LoRA models (e.g. training output directory)"", section=section))
    shared.opts.add_option(""additional_networks_sort_models_by"", shared.OptionInfo(""name"", ""Sort LoRA models by"", gr.Radio, {""choices"": [""name"", ""date"", ""path name""]}, section=section))
    shared.opts.add_option(""additional_networks_model_name_filter"", shared.OptionInfo("""", ""LoRA model name filter"", section=section))
    shared.opts.add_option(""additional_networks_xy_grid_model_metadata"", shared.OptionInfo("""", ""Metadata to show in XY-Grid label for Model axes, comma-separated (example: \""ss_learning_rate, ss_num_epochs\"")"", section=section))


def on_infotext_pasted(infotext, params):
    for i in range(MAX_MODEL_COUNT):
        if f""AddNet Module {i+1}"" not in params:
            params[f""AddNet Module {i+1}""] = ""LoRA""
        if f""AddNet Model {i+1}"" not in params:
            params[f""AddNet Model {i+1}""] = ""None""
        if f""AddNet Weight {i+1}"" not in params:
            params[f""AddNet Weight {i+1}""] = ""0""


script_callbacks.on_ui_tabs(on_ui_tabs)
script_callbacks.on_ui_settings(on_ui_settings)
script_callbacks.on_infotext_pasted(on_infotext_pasted)
```
"
76,679a7a2ecbe8284d7a71c86f,issue,learning -> how to use a library/framework/tool -> generating code examples,https://github.com/PenguLoader/PenguLoader/issues/10,"Can you help me detecting when connecting to lobby ?
Hey,

First of all thank you for this tool it's very useful.

I want to detect when I'm connecting to the chat in champ-select. I don't have much knowledge, but I found a few things on the internet and they work for everything but what I want.

I am trying to use MutationObserver API to detect when an element of class `chat-message` is created, so upon arriving in the champ-select I run this in the console : 
```js
function listenForChatMessages() {
  var targetNode = document.getElementById('chat-messages-frame').contentDocument.body;
  var config = { attributes: true, childList: true, subtree: true };
  var callback = function(mutationsList) {
    for(var mutation of mutationsList) {
        console.log(mutation)
      if (mutation.type === 'childList') {
        var newNodes = mutation.addedNodes;
        for (var i = 0; i < newNodes.length; i++) {
          var node = newNodes[i];
          if (node.classList && node.classList.contains('chat-message')) {
            //Do something here
            console.log(""New chat message element detected!"");
          }
        }
      }
    }
  };
  var observer = new MutationObserver(callback);
  observer.observe(targetNode, config);
}
```

And then I send a message to see if it caught it, and it didn't.
I tried using the MutationObserver in league-loader for capturing other things and it did work, here specifically it won't.

I hoped you could help me, not necessarily using MutationObserver, but anything that can detect when I'm connected to the chat in champ-select will do.

The code example is just one of about ten other ways I tried to do it but that didn't work, I'm running out of ideas. I even tried using ChatGPT to generate code, and it did, and again it worked for everything but for the chat messages :(

Thank you !"
77,679a7af8cbe8284d7a71c8da,issue,software quality -> refactoring,https://github.com/R3BRootGroup/R3BRoot/issues/763,"Replacing char* with std::string
For most array types, std::vector, std::array and boost::multi_array provide drop in replacements. You just change the definition and all the rest keeps working as before. 

For NUL terminated character arrays (which are virtually all character arrays), which are used to store strings, things are more difficult. The correct replacement here would be std::string, which has different semantics. 

Just for fun, I have asked chatGPT ""rewrite the following using std::string instead of char*: ([code of R3BGeoCave::read](https://github.com/R3BRootGroup/R3BRoot/blob/master/passive/R3BGeoCave.cxx#L38))""

Here is the diff of its version to the original:
```diff c++
-Bool_t R3BGeoCave::read(fstream& fin, FairGeoMedia* media)
+Bool_t R3BGeoCave::read(std::fstream& fin, FairGeoMedia* media)
 {
   // Reads the geometry from file
   if (!media)
@@ -6,22 +6,22 @@
       return kFALSE;
     }
   const Int_t maxbuf = 256;
-  char buf[maxbuf];
+  std::string buf;
   FairGeoNode* volu = 0;
   FairGeoMedium* medium;
   Bool_t rc = kTRUE;
   do
     {
-      fin.getline(buf, maxbuf);
-      if (buf[0] != '\0' && buf[0] != '/' && !fin.eof())
+      std::getline(fin, buf);
+      if (!buf.empty() && buf[0] != '/' && !fin.eof())
         {
-         if (strcmp(buf, name) == 0)
+         if (buf == name)
             {
              volu = new FairGeoNode;
              volu->SetName(buf);
              volu->setVolumeType(kFairGeoTopNode);
              volu->setActive();
-             fin.getline(buf, maxbuf);
+             std::getline(fin, buf);
              TString shape(buf);
              FairGeoBasicShape* sh = pShapes->selectShape(shape);
              if (sh)
@@ -32,7 +32,7 @@
                 {
                  rc = kFALSE;
                 }
-             fin.getline(buf, maxbuf);
+             std::getline(fin, buf);
              medium = media->getMedium(buf);
              if (!medium)
                 {
@@ -69,4 +69,3 @@
     }
   return rc;
 }
```
Like most GPT output, it looks plausible at a first glance. Of course, the devil is in the details. TString(buf) works, while FairGeoMedia::getMedium() has no variant taking a std::string, so one would have to use c_str() there, I guess. 

I think we should only try to fix C strings after we have fixed the other uses of C arrays. 

Edit: it also does a somewhat reasonable job on ""in the following code, please replace all 1d C arrays with std::array and all multi-dimensional arrays with boost::multi_array."" It does convert a plain pointer to an ``std::array<int, 1>``, which is wrong."
78,679a7af8cbe8284d7a71cb10,issue,development environment -> continuous integration -> integrating ChatGPT reviewing in CI,https://github.com/cirolini/genai-code-review/issues/3,"Using chatgpt-github-actions with copilot
Is it possible to use Copilot instead of ChatGPT to generate the review comments?"
79,679a7af8cbe8284d7a71cb45,issue,software quality -> fixing -> supporting debugging,https://github.com/raspiblitz/raspiblitz/issues/3640,"Mempool and Thunderhub can not be installed
My issue is that every installation try of a Mempool and Tunderhub has failed on **V1.8 patched** 
The 32GB SD card is new, fast, and durable. I'm running Blitz over wifi on Raspberry 4b 8Gb, 2TB SSD

As I said. installing Mempool or Thunderhub always failed.
![Screenshot (199)](https://user-images.githubusercontent.com/34000272/215330382-09416be3-1cde-4a0c-901c-4d36972f9382.png)
********************************************************
![Screenshot (201)](https://user-images.githubusercontent.com/34000272/215330493-9e7100cc-3548-473f-a51d-a51670f24ebe.png)

Here are the logs...

`sudo cat /var/cache/raspiblitz/debug.log | nc termbin.com 9999`
raspiblitz debug logs: https://termbin.com/3oefe 
Mempool installation log: https://termbin.com/zkux
Thunderhub installation log: https://termbin.com/ricl7 (Line 3282 gives an error about connection but I am sure the connection is just fine and I'm not behind any proxy)

Then I upgraded the Raspiblitz to **v1.9.0rc1** as Openoms suggested
Raspiblitz debug logs: https://termbin.com/tlqu

I could install Thunderhub (The status on webUI says installation successful and even got the links to the service) but couldn't run it (I reinstalled it two times and could not get it to run. Log was always the same ""This site canâ€™t be reached"")
thunderbird log: https://termbin.com/atio

![Screenshot (206)](https://user-images.githubusercontent.com/34000272/215343648-9814ed60-7335-4b47-822f-b37e92a12a96.png)

When trying to install Mempool, it said it is already installed but it wasn't (No logs, just a picture)

![Screenshot (202)](https://user-images.githubusercontent.com/34000272/215330628-acbf5ee2-a201-42c0-be4d-a2593db8ffc0.png)


Openoms also suggested updating node version with `config.scripts/bonus.nodejs.sh on`
Old node version: v16.14.2
node version after update: 18.12.0
I also did `config.scripts/bonus.nodejs.sh off` and ""on"" again

There were no changes doing that. I still can't install mempool and thundehub .

For example what Thunderhug log is complaining about after all that...

*** LAST 20 Thunderhub LOGS ***
sudo journalctl -u thunderhub -b --no-pager -n20
-- Journal begins at Sun 2023-01-29 04:34:17 GMT, ends at Sun 2023-01-29 16:03:2        6 GMT. --
Jan 29 16:02:45 raspberrypi systemd[1]: thunderhub.service: Scheduled restart jo        b, restart counter is at 4.
Jan 29 16:02:45 raspberrypi systemd[1]: Stopped ThunderHub daemon.
Jan 29 16:02:45 raspberrypi systemd[1]: thunderhub.service: Consumed 1.603s CPU         time.
Jan 29 16:02:45 raspberrypi systemd[1]: Started ThunderHub daemon.
Jan 29 16:02:46 raspberrypi npm[679854]: > thunderhub@0.13.6 start
Jan 29 16:02:46 raspberrypi npm[679854]: > cross-env NODE_ENV=production nest start
Jan 29 16:02:46 raspberrypi npm[679978]: sh: 1: cross-env: not found
Jan 29 16:02:46 raspberrypi systemd[1]: thunderhub.service: Main process exited,code=exited, status=127/n/a
Jan 29 16:02:46 raspberrypi systemd[1]: thunderhub.service: Failed with result 'exit-code'.
Jan 29 16:02:46 raspberrypi systemd[1]: thunderhub.service: Consumed 1.690s CPUtime.
Jan 29 16:03:17 raspberrypi systemd[1]: thunderhub.service: Scheduled restart job, restart counter is at 5.
Jan 29 16:03:17 raspberrypi systemd[1]: Stopped ThunderHub daemon.
Jan 29 16:03:17 raspberrypi systemd[1]: thunderhub.service: Consumed 1.690s CPU time.
Jan 29 16:03:17 raspberrypi systemd[1]: Started ThunderHub daemon.
Jan 29 16:03:18 raspberrypi npm[684376]: > thunderhub@0.13.6 start
Jan 29 16:03:18 raspberrypi npm[684376]: > cross-env NODE_ENV=production nest start
Jan 29 16:03:19 raspberrypi npm[684728]: sh: 1: cross-env: not found
Jan 29 16:03:19 raspberrypi systemd[1]: thunderhub.service: Main process exited,code=exited, status=127/n/a
Jan 29 16:03:19 raspberrypi systemd[1]: thunderhub.service: Failed with result 'exit-code'.
Jan 29 16:03:19 raspberrypi systemd[1]: thunderhub.service: Consumed 1.896s CPU time.



Next, I will start fresh and update node to v18.12.9 before installing other apps.

_To be continued..._

**Starting fresh on v1.8...**

The first app that I tried to install is Mempool and got the same error as before.  (""Try to update to npm@9.4.0"") So I did that and came across a blockade about some permissions and here's the log of a mempool install: https://termbin.com/zw37
![Screenshot (222)](https://user-images.githubusercontent.com/34000272/215351425-9bab1219-c608-420a-9fb0-3db2e79113b4.png)

Updating npm with `npm install -g npm@9.4.0`  give this results:  https://termbin.com/u9v6n

![Screenshot (224)](https://user-images.githubusercontent.com/34000272/215351588-903f4197-0cb3-4732-8c2b-7e8c7656a213.png)

I've tried to update npm with `sudo npm install -g npm@9.4.0` and the new version is installed but that still didn't solve the problem with these two apps unfortunately. 
The Network error has gone after an npm update and there's another one https://termbin.com/vqq8

![Screenshot (225)](https://user-images.githubusercontent.com/34000272/215355590-6a147caa-e24a-4181-b93d-2d3c65a59c24.png)

**********************************

**MEMPOOL has been installed**

I asked chatGPT what this error could be, and the AI gave me an important clue...

![Screenshot (226)](https://user-images.githubusercontent.com/34000272/215565568-cc987a9b-9105-4b78-a8af-0a8ca655005a.png)

 Deleting  `sudo rm -rf /home/mempool/mempool/frontend/node_modules`  folder lets through a clean installation of  Mempool. 

The mempool operation is completed :)

Next is Thunderhub...



"
80,679a86e9cbe8284d7a71d512,issue,feature implementation/enhancement -> migrating/reusing via programming language translation,https://github.com/maxi-blocks/maxi-blocks/issues/4386,"Frontend styles loader
### Description

One of the weakest points of Maxi it's the fact that we need to load editor side and save the content to have styles. Some situations where this can affect us is:
1. When migrate a site
2. When we directly copy code to `code editor` without rendering the blocks
3. When we install Maxi Theme (or any other theme in future)
4. Probably there are many other situations...

It's a known issue we've been carrying waiting for fixing it in future, and I think there are at least 3 reasons why now is the moment:
1. We are about to release beta, which means we are closer to be opened to the public.
2. We have FSE support and Maxi Theme.
3. We can use chatGPT (â¤ï¸ )

The main idea is: when loading a page, if the styles doesn't exist on Maxi DB, create them and save on the DB, so next time we'll require them from there again.

### Other plugins or websites examples

_No response_

### How to implement it

I've been doing a prototype to check the viability of the feature and how hard it can be. Our best chance is to use chatGPT (I didn't like the way copilot did the transform) to transform our current JS functions to PHP. The prototype can be found [here](https://github.com/maxi-blocks/maxi-blocks/tree/frontend-styles-loader). This prototype it's a base to start working on, but has been modified to force the load of styles from the frontend loader and not the DB. To test it you can use this [code editor](https://github.com/maxi-blocks/maxi-blocks/files/10765707/frontendloader.txt) that will render a Group Maxi affected by some margin styles generated on frontend and not coming from DB. 
The prototype currently implements:
1. New method `get_styles_from_blocks` on `MaxiBlocks_Styles` that iterates over the blocks to get the styles.
2. New class `MaxiBlocks_Group_Block` as possible example of a block class to get the styles, similar to `styles.js` of each block folder.
3. New `utils` coming from JS side like `get_last_breakpoint_attribute` or `get_group_attributes`. It also contains all the JS side functions to create the styles strings, with same workflow and names.
4. Translated `helper` for margin-padding.

To create it, I've been using chatGPT to translate these functions from JS to PHP, and the result is awesomely fast. They normally need some small details, but it's going to ðŸš€  the implementation of it. So, tasks we need to do to fix this:
1. Create a class for each block as `MaxiBlocks_Group_Block`. It needs an improvement with the way it gets the `$data` values, as it should get directly from the block `data.js` file; and obviously, it needs to be fulfilled with the rest of content to create correct styles ðŸ‘ 
2. Translate all styles helpers.
3. A better way to access to `$defaults` on `get_group_attributes` to keep them always update.
4. Generate the necessary logics to check if the styles aren't saved on DB, to create them and then save to be used in future.
5. Clean code and ensure better and cleaner 'requires'. It can be a good idea to think on other organization than the one of the prototype: maybe in classes, not sure. The current organization of the prototype looks dirty to me ðŸ‘ 

Probably we'll face some issues with contexts (like `rowProps` used on Column Maxi) and things like this; once arrived in that point we can check together how to deal with that situations ðŸ‘ 


### Other information

_No response_"
81,679a87c2cbe8284d7a71d735,issue,learning -> how to use a library/framework/tool,https://github.com/anggrayudi/SimpleStorage/issues/113,"Is there some way to listen for changes under a tree URI or at least over a single document URI?
My use-case is pretty simple: 

1. List files under a selected folder.
2. Listen for any changes over this folder (deletion, renaming, creation, etc.). So, for instance, even if the user deletes a child file `A` using a third-party app (e.g the OS file manager) my app will be able to catch the event and delete the file reference, to avoid ghost files in my recycler view.

To list the files under a selected folder, which is a simple tree URI that my app gains access through `openDocumentTree`, I'm already using SAF, I can use the tree URI to delete, rename, move child documents, etc. so permission is not a problem here.

But I am not able to find any resource pointing to some way to do the ""listening"" part. I've already:

- Read the Google storage documentation, but they cover every edge-case but not this one https://developer.android.com/guide/topics/data.
- I tried to see an example of it on the [android/storage-samples](https://github.com/android/storage-samples) repository but even there they are just reloading the entire directory when the user renames a folder, and it is not handling when the user does the alter by a third-party app, [android/storage-samples/blob/22784d8cbf1d990958ae554ec61afea1a9da93c1](https://github.com/android/storage-samples/blob/22784d8cbf1d990958ae554ec61afea1a9da93c1/ActionOpenDocumentTree/app/src/main/java/com/example/android/ktfiles/DirectoryFragment.kt#L133).
- https://github.com/K1rakishou/Fuck-Storage-Access-Framework library, no mention to any kind of listenable APIs.
- https://github.com/anggrayudi/SimpleStorage (this library), no mention to any kind of listenable APIs.
- https://github.com/google/modernstorage, same...
- I even asked ChatGPT but he brings me a solution that doesn't works, the `ContentResolver.registerContentObserver`, which is exactly the approach of some unanswered SO questions (see below).

I found and tried some SO questions about it (a bit 'older' ~ 3 years), which also doesn't have answers yet:

- https://stackoverflow.com/questions/60796380/storage-access-framework-register-content-observer.
- https://stackoverflow.com/questions/75180057/register-content-observer-on-external-storage.

My question is, do we have access to something that allow us to listen for changes over a folder that we've permission over?

I'm just about to give-up this shit and add `MANAGE_EXTERNAL_STORAGE` permission and use the [FileObserver API](https://gist.github.com/gitanuj/888ef7592be1d3f617f6), and if it does not work either I think I'll just start brute-forcing the user storage searching for changes foff <img src=""https://user-images.githubusercontent.com/51419598/220283630-5184d2e6-b838-4821-a831-cc864ab3c0f5.gif"">.
"
82,679a87c2cbe8284d7a71d8ac,issue,learning -> how to use a library/framework/tool,https://github.com/KIOS-Research/QChatGPT/issues/8,"Error: No module named openai in Windows
Hi, when installing the plugin it asks me to install a series of packages, I accept the installation but it returns the error ""ModuleNotFoundError: No module named 'openai'"". I asked ChatGPT about the error and they told me to install the openai pip from cmd (pip install openai) but the problem persists. How can I solve it?

The more detailed the answer, the better. Thanks in advance.

![image](https://user-images.githubusercontent.com/121056026/220899972-ff3aa3a8-0908-4e38-9bf0-dbbdeb3e9149.png)
"
83,679a95d9cbe8284d7a71ed69,issue,learning -> how to use a library/framework/tool,https://github.com/cnr-isti-vclab/PyMeshLab/issues/286,"[SOLVED]unable to run Transfer Vertex Properties to Texture filter
Hello,

With the help of chatGPT I got till this script, it goes till it runs the filter but the filter fails, all the params are fed as per the documentation but I guet the error I paste below the code:

`import os
import pymeshlab as ml

# Set the paths for the input point cloud and output mesh folders
point_cloud_folder = ""E:/LIDAR_JAPAN/MMS8_Prototype_3/MMS/03_RoadMesh/04_Texturing/test/color""
mesh_folder = ""E:/LIDAR_JAPAN/MMS8_Prototype_3/MMS/03_RoadMesh/04_Texturing/test/mesh""
output_folder = ""E:/LIDAR_JAPAN/MMS8_Prototype_3/MMS/03_RoadMesh/04_Texturing/test/output""

# Print the variable names and their values
print(""point_cloud_folder:"", point_cloud_folder)
print(""mesh_folder:"", mesh_folder)
print(""output_folder:"", output_folder)

# Loop through all files in the point cloud folder
for filename in os.listdir(point_cloud_folder):
    
    # Check if the file is a PLY file
    if filename.endswith("".ply""):
        
        # Construct the input and output file paths
        input_file = os.path.join(point_cloud_folder, filename)
        output_file = os.path.join(mesh_folder, os.path.splitext(filename)[0] + "".obj"")
        # output_texture = os.path.join(output_folder, os.path.splitext(filename)[0] + "".png"")# filename is the name only  must not be full path
        
        # Print the variable names and their values
        print(""input_file:"", input_file)
        print(""output_file:"", output_file)
        
        # Create a MeshSet object
        ms = ml.MeshSet()
        
        # Load the mesh from the OBJ file
        ms.load_new_mesh(output_file)
        
        # Transfer color attributes from the point cloud to the mesh texture
        ms.apply_filter('transfer_attributes_to_texture_per_vertex', sourcemesh=input_file, targetmesh=output_file,
                 attributeenum='Vertex Color', upperbound='Percentage=2%', textname=os.path.splitext(filename)[0] + "".png"", 
                 textw=16384, texth=16384, overwrite=False, pullpush=True)
        
        # Save the modified mesh to the output folder
        ms.save_current_mesh(output_file)
`


This is the error : 

**(base) E:\LIDAR_JAPAN\MMS8_Prototype_3\MMS\03_RoadMesh\04_Texturing\test>python auto_texturing_text.py
point_cloud_folder: E:/LIDAR_JAPAN/MMS8_Prototype_3/MMS/03_RoadMesh/04_Texturing/test/color
mesh_folder: E:/LIDAR_JAPAN/MMS8_Prototype_3/MMS/03_RoadMesh/04_Texturing/test/mesh
output_folder: E:/LIDAR_JAPAN/MMS8_Prototype_3/MMS/03_RoadMesh/04_Texturing/test/output
input_file: E:/LIDAR_JAPAN/MMS8_Prototype_3/MMS/03_RoadMesh/04_Texturing/test/color\08OE2718.ply
output_file: E:/LIDAR_JAPAN/MMS8_Prototype_3/MMS/03_RoadMesh/04_Texturing/test/mesh\08OE2718.obj
Traceback (most recent call last):
  File ""E:\LIDAR_JAPAN\MMS8_Prototype_3\MMS\03_RoadMesh\04_Texturing\test\auto_texturing_text.py"", line 36, in <module>
    ms.apply_filter('transfer_attributes_to_texture_per_vertex', sourcemesh=input_file, targetmesh=output_file,
RuntimeError: Unable to cast Python instance to C++ type (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)**

I'm totally stuck :\

edit: files used in the test: 
https://drive.google.com/file/d/1Xa_Cs0Kx4N2svqKtDL9aaWvTQdcE0bXE/view?usp=share_link
edit2
I noticed that in the GUI there is a param ""abs"" that is missing the pyMeshLab, wonder if that could be the cause of the issue."
84,679a9763cbe8284d7a71f12d,issue,learning -> how to use a library/framework/tool,https://github.com/wazuh/wazuh-docker/issues/789,"Definite guide to changing the default password single node deployment, container. 
Good day

Is there a 100% working guide on how to change the admin password from SecretPassword to something else? I've went through every single guide, even asked ChatGPT 4 with a doc dump into the prompt. Nothing works.

My wazuh deployment is now in a worse of state than before. "
85,679a9763cbe8284d7a71f273,issue,feature implementation/enhancement -> prototyping -> implementing proof of concept,https://github.com/MichielvanBeers/Flow.Launcher.Plugin.ChatGPT/issues/4,"Logging chat records to .txt file?
Given this plugin's current functionality, I am curious if it would be possible to expand the option of opening ChatGPT's responses in a .txt file to also save these responses as an iterative log of both user prompts and GPT's responses (saved locally)?

For example, I've just asked ChatGPT the following: 
> ai how can I write code that will generate a .txt file with a saved record of our conversation together? ||

ChatGPT's response: 
> To generate a .txt file with a saved record of our conversation, you can use the following Python code:
> 
> ```
> # Open a new file in write mode
> with open('conversation.txt', 'w') as file:
>    # Write the conversation to the file
>    file.write('Here is a record of our conversation:\n')
>    for message in messages: # messages is a list of our conversation
>       file.write(message + '\n')
> print('Conversation saved in conversation.txt')
> ```
> 
> In this code, we first open a new file called ""conversation.txt"" in write mode. Using the `with` statement ensures that the file is closed properly after it's done being written to. 
> 
> Inside the `with` block, we write a header to the file indicating that it's a record of our conversation. Then, we iterate over each message in the `messages` list (which would contain all the messages in our conversation) and write each one to a new line in the file.
> 
> Finally, we print a message indicating that the conversation has been saved in the file.
> 
> Make sure to replace the `messages` list with the actual list of messages that you want to save.

I don't have the understanding to implement this as a request, but wanted to leave this as a topic for consideration. Storing a local record of both prompts and the responses would be especially helpful when GPT is overloaded, or chat history is not saved displaying correctly on OpenAI's website. 

Thanks for all of your work on this so far! Cheers!"
86,679aa1fecbe8284d7a71f775,issue,generating/manipulating data,https://github.com/obophenotype/cell-ontology/issues/1908,"Build general, upper-level slim
The aim here is to tag a set of general cell types that would make sense to most biologists as grouping terms. Some judgement will be required in generating this - there is no perfect recipe, although we might gradually evolve some guidelines.

The terms chose should all be in the native cell (classification) branch, although the the immediate children of this term are mostly unsuitable (and any cells under here with no children after reasoning need review).  Example of folding down to a level that is more suitable - neuron and muscle cell look appropriate but their parents do not.

<img width=""324"" alt=""image"" src=""https://user-images.githubusercontent.com/112839/227902634-704f3930-81ef-49c6-9da9-a9c29fbfd60c.png"">

 Some terms may cross metazoans (neuron, muscle cell, epithelial cell), whereas other will be vertebrate specific (leukocyte).    It may be useful to refer to text books on major vertebrate/mammalian cell types or use ChatGPT for inspiration

- [ ] Add upper_cell_type_slim* AP with short definition - subProperty of http://www.geneontology.org/formats/oboInOwl#SubsetProperty (*I'm open to better names!)
- [ ]  Tag a set of high level cell types (see discussion above)
- [ ]  Work with @ubyndr to test coverage under native cell - aiming for 80% coverage (excluding ancestors of slim terms)


"
87,679aa26ccbe8284d7a71fa70,issue,learning -> how to use a library/framework/tool -> generating code examples,https://github.com/jaywcjlove/rehype-attr/issues/46,"Parsing as className instead of class for React
> First of all I'm sorry to open an issue in your GitHub repository! I really appreciate the tool you created and thank you for the work.

## Main question

I was wondering if there is a way to parse markdown with this tool and give the attribute `className` instead of the regular `class` when using a comment like:

```markdown
Some paragraph of my markdown file...
<!--rehype:className=regularText-->
```

The problem with this is that if I render things as `.use(rehypeAttr, { properties: 'attr' })`, this gives a translation that adds `class` to the resulting `<p>` instead of `className`. I'm asking this in the hope that something already exists, but if not I'd be happy to collaborate on some code for the implementation.

## Another question

I was using ChatGPT to help me fix this, and it suggested a syntax similar to this:

```js
.use(rehypeAttr, {
      p: { className: 'text-gray-700 my-10' },
      h1: { className: 'text-3xl font-bold' },
})
```

Is this valid? Because when I try to use it nothing happens."
88,679aa3a0cbe8284d7a71fcee,issue,learning -> how to use a library/framework/tool -> database querying,https://github.com/asgeirrr/pgantomizer/issues/8,"Can't truncate tables with foreign keys
I have two tables that have a foreign key, and I want to truncate both of them. When I do it, though, I get the following error. 

```
psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
DETAIL:  Table ""tutor_bot_messages"" references ""tutor_bot_conversations"".
HINT:  Truncate table ""tutor_bot_messages"" at the same time, or use TRUNCATE ... CASCADE.
```

I asked ChatGPT for an answer:

 * how do i truncate tables with foreign keys using pgantomizer?
 * To truncate tables with foreign keys using pgantomizer, you can use the cascade_truncate function provided by the tool. The cascade_truncate function will automatically disable or drop the foreign key constraints, truncate the table, and then re-enable or recreate the foreign key constraints.

It would be great if that were actually true"
89,679ab0d9cbe8284d7a720c9b,issue,learning -> how to use a library/framework/tool -> database querying,https://github.com/smashedtoatoms/asdf-postgres/issues/70,"how to access postgres environemt in the asdf?
I am having an issue where I can't create a database due to the following error:
`createdb: error: connection to server on socket ""/tmp/.s.PGSQL.5432"" failed: FATAL:  role ""luisgomes"" does not exist`

In order to solve that, I ran `sudo -i -u postgres` and then `createuser -s -P luisgomes`. 
It gave me an `bash: createuser: command not found...` .

I asked chatGPT what should I do and it explained that createuser is available only on the postgresql created by **asdf**.
It also told me to run `asdf shell postgres 15.2` but nothing happened. (15.2 is the version I have installed)

What should I do now?"
90,679ab17fcbe8284d7a721085,issue,software quality -> fixing -> supporting debugging,https://github.com/PipedreamHQ/pipedream/issues/6097,"[APP] Milvus, ZIlliz Cloud
**Name of app/service**
Milvus (milvus.io)
Milvus Cloud (cloud.zilliz.com)

**Link to developer documentation**
https://milvus.io/docs
https://milvus.io/api-reference/pymilvus/v2.2.7/About.md

**Is lack of support preventing you from building workflows, or do you have a workaround?**
Trying to connect to my Milvus Cloud endpoint in pipedream using Python but running into too many problems doing so. I've even used chatgpt to see what may be wrong with code and nothing so it would be easier if integration was made to eliminate this. I love the integration Pipedream has for Pinecone and would love to see one for Milvus/Milvus Cloud.

**Are there specific actions, or triggers, you'd like to see for this app? Please let us know here or use the Action and Trigger issue templates to open requests for each!**
- Import data,
- Query data,
- Create collections
and any others the pipedream team feels would be appropriate from the docs.

"
91,679ab2bccbe8284d7a7216d4,issue,"process -> release planning,software quality -> code review",https://github.com/GFW-knocker/gfw_resist_tls_proxy/issues/49,"some suggestions from chatgpt
Some possible suggestions to make this project better are:

1.Use a context manager or a try-finally block to ensure that the sockets are closed properly, even if an exception occurs. This will prevent resource leaks and errors. For example:
with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
    # do something with s

2.Use logging instead of print statements to record the events and errors in the socket communication. This will make it easier to debug and monitor the program. For example:
import logging
logging.basicConfig(level=logging.INFO)
logging.info('os is linux')

3.Use argparse or a configuration file to store and parse the constants, such as listen_PORT, Cloudflare_IP, num_fragment, etc. This will make it easier to change and customize the parameters without modifying the code. For example:
import argparse
parser = argparse.ArgumentParser()
parser.add_argument('--port', type=int, default=2500, help='listening port')
args = parser.parse_args()
listen_PORT = args.port

4.Use comments and docstrings to document the purpose and functionality of the code. This will make it easier to understand and maintain the code. For example:
def listen(self):
    """"""Listen for incoming connections and spawn a thread for each one.""""""
    self.sock.listen(128)  # up to 128 concurrent unaccepted socket queued , the more is refused untill accepting those.
    while True:
        client_sock , client_addr = self.sock.accept()                    
        client_sock.settimeout(my_socket_timeout)
        # create a new thread to handle the client connection
        threading.Thread(target=self.handle_client, args=(client_sock, client_addr)).start()"
92,679ab33ecbe8284d7a7218e7,issue,software quality -> fixing -> supporting debugging,https://github.com/google-ai-edge/mediapipe/issues/4357,"Error on bazel build to use mediapipe  pose_detection
### OS Platform and Distribution

Windows 10, 64 bit

### Compiler version

_No response_

### Programming Language and version

Java

### Installed using virtualenv? pip? Conda?(if python)

_No response_

### MediaPipe version

mediapipe-master folder maybe 0.9.3.0

### Bazel version

3.22.1

### XCode and Tulsi versions (if iOS)

_No response_

### Android SDK and NDK versions (if android)

SDK=33 and NDK=25.2.9519653

### Android AAR (if android)

No

### OpenCV version (if running on desktop)

_No response_

### Describe the problem

```shell
I am using mediapipe-master downloaded from github and I want to use pose detection in android studio so I am trying to create aar file but on running command I am getting error 

I got many errors previously too but I solved it using chatGPT on this error I asked ChatGPT it is saying to add 'declare toolchain info' in Build file by adding (""@bazel_tools//tools/build_defs/config:toolchains.bzl"", ""declare_local_toolchain_info"")

declare_local_toolchain_info(
    name = ""py_toolchain"",
    toolchain = ""@local_config_python//:toolchain"",
)
 but In my bazel folder bazel_tools//tools/build_defs/ does not contain config folder

I hope you understand the problem.
```


### Complete Logs

```shell
Command Run  build -c opt --config=android_arm64 mediapipe/modules/pose_detection:*,

Got Error ```ERROR: C:/users/asrock/downloads/mediapipe-master/mediapipe/framework/tool/BUILD:109:10: While resolving toolchains for target //mediapipe/framework/tool:encode_as_c_string: invalid registered toolchain '@local_execution_config_python//:py_toolchain': target does not provide the DeclaredToolchainInfo provider
ERROR: Analysis of target '//mediapipe/modules/pose_detection:pose_detection_cpu.inc' failed; build aborted:
INFO: Elapsed time: 0.441s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)```
```
"
93,679abf81cbe8284d7a7224ab,issue,learning -> how to use a library/framework/tool -> database querying,https://github.com/ad-freiburg/qlever/issues/978,"Unclear message Header row of TSV result for SERVICE query is """", but expected ""?editor ?editorName""
This happened with a ChatGPT generated SPARQL query see:

https://cr.bitplan.com/index.php/Workdocumentation_2023-05-15

https://qlever.cs.uni-freiburg.de/wikidata/x4MXDR

what is the problem here?"
94,679ac009cbe8284d7a722722,issue,learning -> how to use a library/framework/tool,https://github.com/python-pillow/Pillow/issues/7170,"Streaming from my cell phone camera to a rtmp server
Hi 
I am new to stream development. I ask chatGPT to provide me with some code in python for streaming from my cell phone camera to a rtmp server. The RTMP server is working fine. I have tested with OBS and VLC Media Player.

When I executed this code on Jupyter Notebook I got this error below
Can someone help figure out what is wrong?

Regards
A Sokone.

# --------------------------------- THIS IS THE CODE FROM chatGPT ---------------------------------
import cv2
import gi

# ---- I added these lines
import ctypes
import numpy as np
import cairo
# ---- End of added lines

gi.require_version('Gst', '1.0')
from gi.repository import Gst

# Initialize GStreamer
Gst.init(None)

# Define the RTMP server URL
rtmp_server_url = 'rtmp://192.168.0.30/live'

# Set up the GStreamer pipeline
pipeline = Gst.Pipeline.new()

# Create the elements for video capture and encoding
src = Gst.ElementFactory.make(""appsrc"", ""source"")
caps = Gst.Caps.from_string(""video/x-raw,format=BGR"")
filter = Gst.ElementFactory.make(""capsfilter"", ""filter"")
filter.set_property(""caps"", caps)
video_convert = Gst.ElementFactory.make(""videoconvert"", ""video_convert"")
x264enc = Gst.ElementFactory.make(""x264enc"", ""x264enc"")
mux = Gst.ElementFactory.make(""flvmux"", ""mux"")
sink = Gst.ElementFactory.make(""rtmpsink"", ""sink"")
sink.set_property(""location"", rtmp_server_url)

# Add elements to the pipeline
for element in [src, filter, video_convert, x264enc, mux, sink]:
    pipeline.add(element)

# Link elements in the pipeline
src.link(filter)
filter.link(video_convert)
video_convert.link(x264enc)
x264enc.link(mux)
mux.link(sink)

# Start the pipeline
pipeline.set_state(Gst.State.PLAYING)

# Open the video capture device
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break

    
    # Convert frame to Cairo surface
    height, width, _ = frame.shape

    stride = width * 1  # Assuming 3 channels (RGB)
    # ------ End of lines added
    print (""height"", height, ""width"", width, ""stride"", stride)
    
    #surface = cairo.ImageSurface.create_for_data(frame.data, cairo.FORMAT_RGB24, width, height, stride)
    
    surface = cairo.ImageSurface.create_for_data(frame.data, cairo.FORMAT_RGB24, width, height)
    
    print(""surface.get_data"", surface.get_data())
    
    # Push frame to the GStreamer pipeline
    buf = Gst.Buffer.new_wrapped(surface.get_data())
    
    print(""buf"", buf)
    buf.pts = buf.dts = Gst.CLOCK_TIME_NONE
    buf.duration = 1
    src.emit(""push-buffer"", buf)

# Clean up
cap.release()
pipeline.set_state(Gst.State.NULL)
#-------------------------------- end of chatGPT code ---------------------------------------

# ======================================================================
height 1080 width 1920 stride 1920

I AM GETTING THIS ERROR! CAN SOMEONE HELP FIX THIS? 
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[1], line 65
     61 print (""height"", height, ""width"", width, ""stride"", stride)
     63 #surface = cairo.ImageSurface.create_for_data(frame.data, cairo.FORMAT_RGB24, width, height, stride)
---> 65 surface = cairo.ImageSurface.create_for_data(frame.data, cairo.FORMAT_RGB24, width, height)
     67 print(""surface.get_data"", surface.get_data())
     69 # Push frame to the GStreamer pipeline

TypeError: buffer is not long enough
"
95,679ac16ccbe8284d7a722cd3,issue,"feature implementation/enhancement -> design a new feature,feature implementation/enhancement -> feature enhancement -> user experience",https://github.com/replicatedhq/troubleshoot/issues/1182,"Enable analyzers that work on ""available"" resources: in other words those not already reserved
**Describe the rationale for the suggested feature.**

I'd like to be able to include logic around ""available"" resources on a node in when writing analyzers that deal with node resources. This will help me close in on whether my Kubernetes will be able to schedule my pod before I attempt my install, assuming I align my check with my resource requests.

**Describe the feature**

With this feature implemented, I'd be able to write a preflight that looks like this:

```
        - nodeResources:
            checkName: Are sufficient CPU resources available in the cluster
            outcomes:
              - fail:
                  when: ""min(cpuAvailable) < 250m""
                  message: Your cluster currently has too few CPU resources available to install Gitea
              - pass:
                  message: Your cluster has sufficient CPU resources available to install Gitea
        - nodeResources:
            checkName: Is sufficient memory available in the cluster
            outcomes:
              - fail:
                  when: ""min(memoryAvailable) < 256Mi"" 
                  message: Your cluster currently has too little memory available to install Gitea
              - pass:
                  message: Your cluster has sufficient memory available to install Gitea
 ```    

and fail the install if my resource requests could not be fulfilled on any node (or any node that I've filtered into my analyzer).

`kubectl describe node` provides insight into these values, but they are not available as part of the `status` of the node so just getting the node doesn't show them.


**Describe alternatives you've considered**

Describe alternative solutions here. Include any workarounds you've considered.

**Additional context**

It seems like the best way to handle this is to collect all the resource requests for the pods running on the node and subtract that from the allocatable resources on that node. Based on the order of the `kubectl describe node` output I'd bet that's what it is doing, though I haven't read through the code to check. 

I also asked ChatGPT to write me a `kubectl` plugin to calculate this to see what the code might look like, I'm attaching it for fun and reference.
[kubectl-available-plugin.tar.gz](https://github.com/replicatedhq/troubleshoot/files/11578168/kubectl-available-plugin.tar.gz)

@chris-sanders, @diamonwiggins, and I chatted about this on Slack."
96,679accb5cbe8284d7a7232b0,issue,process -> automating commit/issue/PR creation -> creating commit/issue/PR description,https://github.com/LinguaPhylo/linguaPhylo/issues/383,"Use singular names for java packages
Generated from ChatGPT :-)

When naming Java packages, it is generally recommended to use singular names rather than plural names. The reason is that a package represents a namespace that contains related classes, interfaces, and resources. Each package represents a distinct unit of organization and encapsulation.

Using singular names for packages helps to maintain consistency with the naming conventions used in Java, where classes and interfaces are typically named using singular nouns to represent individual entities or concepts.

For example, if you have a package that contains classes related to mathematical operations, it would be more appropriate to name it `operation` rather than `operations`. 

Using singular names for packages improves readability and makes the codebase more intuitive to understand. It also aligns with established naming conventions in the Java ecosystem. However, keep in mind that naming conventions can vary across different projects or organizations, so it's important to follow the conventions used within your specific context if they differ from the general recommendations."
97,679accb5cbe8284d7a7232e7,issue,"learning -> how to use a library/framework/tool,learning -> program comprehension",https://github.com/AlizerUncaged/desktop-waifu/issues/65,"Installing desktop waifu with ChatGPT as your guide 
I'm just going to say this if you want to have anime girls on your PC and you don't know shit about programming, command line, power shell, python, or random computer jibberish but copying and pasting comes easy to you. You don't need to know what shit does you merely need to know that it might help the program install better even if it doesn't the important thing is you have everything because an AI knows more than you do. ChatGPT realized the developer left out the setup file or set up cmd in this zip container. In a future update please make sure those are there. I finally have all the software requirements, and did the necessary tweaks everyone keeps saying to do in the mentioned text files and it still didn't work. I feel like I'm nearly there but something is still missing in order for it to run. I may post my guide from ChatGPT as it may be helpful to anyone who doesn't know jack shit about coding who just fucking wants to talk to an anime girl.

Microsoft Windows [Version 10.0.19045.2965]
(c) Microsoft Corporation. All rights reserved.

C:\Users\Normand>python runner.py
python: can't open file 'C:\\Users\\Normand\\runner.py': [Errno 2] No such file or directory

C:\Users\Normand>cd C:\Users\Normand\Documents\Github\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src
The system cannot find the path specified.

C:\Users\Normand>cd C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src

C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src>python runner.py
Traceback (most recent call last):
  File ""C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src\runner.py"", line 1, in <module>
    from colorama import *
ModuleNotFoundError: No module named 'colorama'

C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src>pip install colorama
Defaulting to user installation because normal site-packages is not writeable
Collecting colorama
  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)
Installing collected packages: colorama
Successfully installed colorama-0.4.6

[notice] A new release of pip available: 22.3.1 -> 23.1.2
[notice] To update, run: python.exe -m pip install --upgrade pip

C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src>python runner.py
Traceback (most recent call last):
  File ""C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src\runner.py"", line 2, in <module>
    import openai, humanize, os, sys, time, threading, asyncio, signal, json
ModuleNotFoundError: No module named 'openai'

C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src>pip install openai
Defaulting to user installation because normal site-packages is not writeable
Collecting openai
  Downloading openai-0.27.7-py3-none-any.whl (71 kB)
     ---------------------------------------- 72.0/72.0 kB 51.3 kB/s eta 0:00:00
Collecting requests>=2.20
  Downloading requests-2.31.0-py3-none-any.whl (62 kB)
     ---------------------------------------- 62.6/62.6 kB 167.2 kB/s eta 0:00:00
Collecting tqdm
  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)
     ---------------------------------------- 77.1/77.1 kB 194.3 kB/s eta 0:00:00
Collecting aiohttp
  Downloading aiohttp-3.8.4-cp311-cp311-win_amd64.whl (317 kB)
     ---------------------------------------- 317.2/317.2 kB 357.4 kB/s eta 0:00:00
Collecting charset-normalizer<4,>=2
  Downloading charset_normalizer-3.1.0-cp311-cp311-win_amd64.whl (96 kB)
     ---------------------------------------- 96.7/96.7 kB 240.8 kB/s eta 0:00:00
Collecting idna<4,>=2.5
  Downloading idna-3.4-py3-none-any.whl (61 kB)
     ---------------------------------------- 61.5/61.5 kB 142.9 kB/s eta 0:00:00
Collecting urllib3<3,>=1.21.1
  Downloading urllib3-2.0.2-py3-none-any.whl (123 kB)
     ---------------------------------------- 123.2/123.2 kB 344.5 kB/s eta 0:00:00
Collecting certifi>=2017.4.17
  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)
     ---------------------------------------- 157.0/157.0 kB 427.8 kB/s eta 0:00:00
Collecting attrs>=17.3.0
  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)
     ---------------------------------------- 61.2/61.2 kB 155.2 kB/s eta 0:00:00
Collecting multidict<7.0,>=4.5
  Downloading multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)
Collecting async-timeout<5.0,>=4.0.0a3
  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)
Collecting yarl<2.0,>=1.0
  Downloading yarl-1.9.2-cp311-cp311-win_amd64.whl (60 kB)
     ---------------------------------------- 60.2/60.2 kB 152.4 kB/s eta 0:00:00
Collecting frozenlist>=1.1.1
  Downloading frozenlist-1.3.3-cp311-cp311-win_amd64.whl (32 kB)
Collecting aiosignal>=1.1.2
  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)
Requirement already satisfied: colorama in c:\users\normand\appdata\roaming\python\python311\site-packages (from tqdm->openai) (0.4.6)
Installing collected packages: urllib3, tqdm, multidict, idna, frozenlist, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, aiosignal, aiohttp, openai
  WARNING: The script tqdm.exe is installed in 'C:\Users\Normand\AppData\Roaming\Python\Python311\Scripts' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
  WARNING: The script normalizer.exe is installed in 'C:\Users\Normand\AppData\Roaming\Python\Python311\Scripts' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
  WARNING: The script openai.exe is installed in 'C:\Users\Normand\AppData\Roaming\Python\Python311\Scripts' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 attrs-23.1.0 certifi-2023.5.7 charset-normalizer-3.1.0 frozenlist-1.3.3 idna-3.4 multidict-6.0.4 openai-0.27.7 requests-2.31.0 tqdm-4.65.0 urllib3-2.0.2 yarl-1.9.2

[notice] A new release of pip available: 22.3.1 -> 23.1.2
[notice] To update, run: python.exe -m pip install --upgrade pip

C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src>python runner.py
Traceback (most recent call last):
  File ""C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src\runner.py"", line 2, in <module>
    import openai, humanize, os, sys, time, threading, asyncio, signal, json
ModuleNotFoundError: No module named 'humanize'

C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src>pip install humanize
Defaulting to user installation because normal site-packages is not writeable
Collecting humanize
  Downloading humanize-4.6.0-py3-none-any.whl (109 kB)
     ---------------------------------------- 110.0/110.0 kB 80.8 kB/s eta 0:00:00
Installing collected packages: humanize
Successfully installed humanize-4.6.0

[notice] A new release of pip available: 22.3.1 -> 23.1.2
[notice] To update, run: python.exe -m pip install --upgrade pip

C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src>python runner.py
Traceback (most recent call last):
  File ""C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src\runner.py"", line 3, in <module>
    from rich.console import Console
ModuleNotFoundError: No module named 'rich'

C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src>pip install rich
Defaulting to user installation because normal site-packages is not writeable
Collecting rich
  Downloading rich-13.4.1-py3-none-any.whl (239 kB)
     ---------------------------------------- 239.4/239.4 kB 128.7 kB/s eta 0:00:00
Collecting markdown-it-py<3.0.0,>=2.2.0
  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)
     ---------------------------------------- 84.5/84.5 kB 215.8 kB/s eta 0:00:00
Collecting pygments<3.0.0,>=2.13.0
  Downloading Pygments-2.15.1-py3-none-any.whl (1.1 MB)
     ---------------------------------------- 1.1/1.1 MB 1.3 MB/s eta 0:00:00
Collecting mdurl~=0.1
  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)
Installing collected packages: pygments, mdurl, markdown-it-py, rich
  WARNING: The script pygmentize.exe is installed in 'C:\Users\Normand\AppData\Roaming\Python\Python311\Scripts' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
  WARNING: The script markdown-it.exe is installed in 'C:\Users\Normand\AppData\Roaming\Python\Python311\Scripts' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
Successfully installed markdown-it-py-2.2.0 mdurl-0.1.2 pygments-2.15.1 rich-13.4.1

[notice] A new release of pip available: 22.3.1 -> 23.1.2
[notice] To update, run: python.exe -m pip install --upgrade pip

C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src>python runner.py
Traceback (most recent call last):
  File ""C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src\runner.py"", line 10, in <module>
    with open('.env') as f:
         ^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '.env'

C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src>python runner.py
Traceback (most recent call last):
  File ""C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src\runner.py"", line 10, in <module>
    with open('.env') as f:
         ^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '.env'

C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src>with open('C:\\Users\\Normand\\Desktop\\AI waifu\\desktop-waifu-master\\desktop-waifu-master\\waifu\\Src\\.env') as f:
'with' is not recognized as an internal or external command,
operable program or batch file.

C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src>python runner.py
Traceback (most recent call last):
  File ""C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src\runner.py"", line 10, in <module>
    with open('.env') as f:
         ^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '.env'

C:\Users\Normand\Desktop\AI waifu\desktop-waifu-master\desktop-waifu-master\waifu\Src>"
98,679accb5cbe8284d7a7233d2,issue,"learning -> how to use a library/framework/tool -> generating code examples,software quality -> fixing -> supporting debugging -> writing code to reproduce a bug",https://github.com/tensorflow/tensorflow/issues/60800,"Tensorflow freezes during training on Mac Studio
<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf2.9

### Custom Code

No

### OS Platform and Distribution

MacOS Ventura 13.4/13.1

### Mobile device

Mac Studio

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple M1

### Current Behaviour?

When training a model on my Mac Studio, from time to time it seems that the training freezes, and an epoch takes much more time than usual, from x10 to x200.
I have tried contacting Apple's support, and they are clueless about this phenomena.
I also went through recent Mac related issues in this repository and found nothing relevant.

Here is an example of the phenomena as seen in the training's output:
```
28/28 [==============================] - 5s 195ms/step - loss: 6.2743e-04 - accuracy: 0.0113 - val_loss: 3.4481e-04 - val_accuracy: 0.0147
Epoch 597/1000
28/28 [==============================] - 61s 2s/step - loss: 8.0337e-04 - accuracy: 0.0055 - val_loss: 3.4126e-04 - val_accuracy: 0.0244
Epoch 598/1000
28/28 [==============================] - 5s 195ms/step - loss: 7.9272e-04 - accuracy: 0.0101 - val_loss: 3.3659e-04 - val_accuracy: 0.0208
```





### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

# Generate random training data
np.random.seed(0)
x_train = np.random.rand(100, 1)
y_train = 3 * x_train + 2 + np.random.randn(100, 1) * 0.1

# Define the neural network architecture
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(1, input_shape=(1,))
])

# Compile the model
model.compile(optimizer='sgd', loss='mean_squared_error')

# Train the model
model.fit(x_train, y_train, epochs=1000, batch_size=10)
```

(Thank ChatGPT for the minimal working example)
```


### Relevant log output

```shell
Metal device set to: Apple M1 Ultra

systemMemory: 128.00 GB
maxCacheSize: 48.00 GB

2023-06-07 13:46:01.901374: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2023-06-07 13:46:01.901665: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2023-06-07 13:46:01.992101: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
Epoch 1/1000
2023-06-07 13:46:02.108687: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.
10/10 [==============================] - 0s 3ms/step - loss: 14.7499
Epoch 2/1000
10/10 [==============================] - 0s 10ms/step - loss: 9.1414
...
Epoch 33/1000
10/10 [==============================] - 0s 13ms/step - loss: 0.0382
Epoch 34/1000
10/10 [==============================] - 6s 716ms/step - loss: 0.0374
Epoch 35/1000
10/10 [==============================] - 0s 5ms/step - loss: 0.0367
...
```
</details>"
99,679accb5cbe8284d7a723539,issue,"documentation -> generating user guide,learning -> how to use a library/framework/tool",https://github.com/greenshot/greenshot/issues/499,"Update web page describing how to disable Windows 11 PrtSc 
The PrtSc troubleshooting instructions here are missing the step of disabling PrtSc in the accessibility controls:

https://getgreenshot.org/faq/my-prntscreen-key-no-longer-works-with-greenshot/

From chatGPT ...

To disable the native screenshot function of Windows 11 that's currently assigned to the PrtSc key, follow these steps:

1. Press the Windows + I button to open the Windows Settings.
2. Click on the Accessibility category.
3. Select Keyboard from the right-side menu.
4. Navigate to the option labeled ""Use the Print Screen button to open screen snipping"" and toggle off this option.

It worked for me.
"
100,679acd1ccbe8284d7a723706,issue,learning,https://github.com/ultralytics/yolov5/issues/11696,"if we should not use letterbox at training time, but only at inference time?
### Search before asking

- [X] I have searched the YOLOv5 [issues](https://github.com/ultralytics/yolov5/issues) and [discussions](https://github.com/ultralytics/yolov5/discussions) and found no similar questions.


### Question

I have always wondered if we should not use letterbox at training time, but only at inference time. I asked ChatGPt why you don't use letterbox for training, and it gave me the following answer:

> During training, letterbox is not used mainly because:

> 1.  Resizing the image can destroy the aspect ratio of the original image, which is not desirable in many tasks. Retaining the aspect ratio of the original image can better train the model, making it suitable for various image sizes and ratios.
> 2.  Resizing the image can distort the image, resulting in a decrease in the quality of training data and adversely affecting model training. In addition, there is no fixed input size during training. Instead, data augmentation techniques, such as random scaling and flipping, can be used to increase the diversity and generalization ability of training data.
> 3.  Resizing the image can also slow down the training speed since the images in each batch need to be resized. Avoiding resizing can reduce training time and computing costs.
Therefore, during training, letterbox is not used for resizing, and other data augmentation techniques are used instead. Letterbox is used to resize the image during inference to facilitate the uniform input size and detect objects in images of any size.

Is it right?

### Additional

_No response_"
101,679acda1cbe8284d7a723752,issue,documentation -> internationalization support,https://github.com/stakwork/sphinx-ios/issues/224,"Inline Translations with ChatGPT Plugins
Use ChatGPT to automatically translate messages & facilitate international business/interactions"
102,679ace80cbe8284d7a7239ea,issue,feature implementation/enhancement -> implementing a new feature,https://github.com/Nixtla/neuralforecast/issues/669,"Custom Loss Function feature
### Description

Please, can you add Custom Loss Function feature?
So anyone can build and use their own loss function.

I asked ChatGPT to make a Number Signal (positive/negative) Mean Accuracy loss function for Pytorch.
Predicting financial/stocks time series is more about the correct Direction (up/down) than point accuracy (RMSE).
Predicting Stocks time series we use returns (t, t-1 difference) to make it stationary and predict if next difference is positive/negative (up/down movement).

If NeuralForecast accept custom loss function, I would use the script suggested by ChatGPT (not tested yet):

    import torch
    import torch.nn as nn

    class NumberSignalLoss(nn.Module):
    def __init__(self):
        super(NumberSignalLoss, self).__init__()

    def forward(self, predictions, targets):
        predictions = torch.sign(predictions)  # Convert predictions to either -1 or 1
        targets = torch.sign(targets)  # Convert targets to either -1 or 1

        accuracy = torch.mean(torch.eq(predictions, targets).float())  # Compute mean accuracy

        loss = 1 - accuracy  # Calculate loss as 1 - accuracy

        return loss


ChatGPT said:
In this example, the NumberSignalLoss class is a subclass of nn.Module that implements the loss function. The forward method takes in two inputs: predictions and targets. Both inputs are assumed to be tensors.

The forward method first applies the torch.sign function to both the predictions and targets. This function converts positive values to 1 and negative values to -1, making the predictions and targets binary signals.

Then, it calculates the accuracy by comparing the element-wise equality of the predictions and targets using torch.eq. The result is then converted to a float tensor and the mean is taken using torch.mean.

Finally, the loss is calculated as 1 - accuracy since we want to minimize the loss. The loss tensor is returned as the output.

You can use this loss function in your PyTorch training loop by instantiating an instance of NumberSignalLoss and passing your predictions and targets to it. The resulting loss can then be used for backpropagation and optimization.


### Use case

_No response_"
103,679ace80cbe8284d7a723b6e,issue,learning -> how to use a library/framework/tool,https://github.com/muellerberndt/mini-agi/issues/66,"Suggestion: A better explanation of how to use mini-agi with Docker
It's just a suggestion, but not being familiar with Docker, I didn't understand any of the explanations given for using mini-agi with Docker ðŸ¤·â€â™‚ï¸ðŸ˜¥

Do I simply have to? (thanks ChatGPT for the help ðŸ˜‰)
- Go to the mini-agi directory with ` cd mini-agi `
- Run command ` cp .env_example .env `
- Customize the contents of the `.env file ` (OPENAI_API_KEY...)
- Run command ` docker build -t mini-agi . `
- Run command ` docker run -it mini-agi `"
104,679ace80cbe8284d7a723c05,issue,development environment -> implementing build script,https://github.com/glassfy/capacitor-plugin-glassfy/issues/23,"Execution failed for task ':capacitor-plugin-glassfy:compileReleaseKotlin'.
I'm using this plugin with [Quasar](https://quasar.dev/). Quasar can build Capacitor apps for both iOS and Android. I have it working perfectly fine in iOS. I'm just having trouble getting it to work within the Android build.

I use Quasar to build for Android app: `quasar build -m capacitor -T android` 

I then get the following error:

```
> Task :capacitor-plugin-glassfy:compileReleaseKotlin FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':capacitor-plugin-glassfy:compileReleaseKotlin'.
> 'compileReleaseJavaWithJavac' task (current target is 17) and 'compileReleaseKotlin' task (current target is 18) jvm target compatibility should be set to the same Java version.
  Consider using JVM toolchain: https://kotl.in/gradle/jvm/toolchain
```

I've tried multiple iterations of changes:
- Downgrading `capacitor-plugin-glassfy` to `3.0.0`
- Adding Kotlin version into `capacitor.build.gradle` file
-  Adding Kotlin version into the parent `build.gradle` file
- Asking ChatGPT for input but I keep going in loops with adding a snippet for Kotlin version but it doesn't work
- Asking Bard for input hasn't cracked it either

Nothing seems to work - except removing `capacitor-plugin-glassfy` from the capacitor `package.json` of course - which is not what I want to do.

Here's my Capacitor dependencies for reference: 
```
""dependencies"": {
    ""@capacitor/android"": ""^5.0.0-beta.0"",
    ""@capacitor/app"": ""^5.0.3"",
    ""@capacitor/cli"": ""5.0.5"",
    ""@capacitor/core"": ""5.0.5"",
    ""@capacitor/ios"": ""^5.0.5"",
    ""@capacitor/keyboard"": ""^5.0.4"",
    ""@capacitor/splash-screen"": ""^5.0.4"",
    ""@capacitor/storage"": ""^1.2.5"",
    ""capacitor-plugin-glassfy"": ""3.0.0""
  }
```
Here's the top of my `capacitor.build.gradle` file too:
```
android {
  compileOptions {
      sourceCompatibility JavaVersion.VERSION_17
      targetCompatibility JavaVersion.VERSION_17
  }
}

....
```

Would love any direction to get the Android built to work with the `capacitor-plugin-glassfy` reference in the `package.json` - As I assume that's the ideal scenario.

Thanks in advanced"
105,679acf3acbe8284d7a723f35,issue,software quality -> fixing -> functional bugs,https://github.com/Picovoice/porcupine/issues/1018,"how to make custom wake word?
so im trying to make a voice assistant using picovoice porcupine, on the website i trained the wake word model ""Hey ada"" and i have the PPN file, now when i try to use it in python i get the following error, 

ValueError: One or more keywords are not available by default. Available default keywords are:\nhey google, computer, grasshopper, blueberry, jarvis, hey siri, bumblebee, terminator, pico clock, hey barista, grapefruit, alexa, americano, ok google, picovoice, porcupine

from this i understand that my wake word isnt any of these but... i need a custom wake word. ive gone on stack overflow no answer, i asked chatgpt still nothing. please help! 

thank you in advance!"
106,679acf3acbe8284d7a72403f,issue,learning -> how to use a library/framework/tool,https://github.com/ptitSeb/box86/issues/839,"Compiling on Arch Linux (aarch64)
I've been trying to compile box86 on arch for the past few days with no success. Well, just a little. I managed to get it compiled for armhf, but not for aarch64.

I'm compiling this on a qemu aarch64 virtual machine.

For armhf compilation I installed `arm-linux-gnueabihf-gcc` and `arm-linux-gnueabihf-glibc`. For arm64, I didn't find any packages, so I downloaded some from [here](https://developer.arm.com/downloads/-/gnu-a) (specifically `gcc-arm-10.3-2021.07-aarch64-aarch64-none-elf.tar.xz`).

Then I ran `export CC=arm-linux-gnueabihf-gcc` and `export CXX=arm-linux-gnueabihf-g++` to specify what compilers it's supposed to use. The build worked and it works on armhf. When I tried to run it on the vm (aarch64), it says `cannot execute binary file`. I asked chatgpt how to cross compile and it said I should type in `export CC_ARM64=/path/to/toolchain/aarch64-none-elf-gcc` and `export CXX_ARM64=/path/to/toolchain/aarch64-none-elf-g++` and add `CFLAGS=""-march=aarch64"" CXXFLAGS=""-march=aarch64""` to the make command. I ran it again and it seemed that nothing has changed. And yes, I still can't run it on aarch64.

Also, adding `-DARM64=1` to cmake doesn't work.

How can I compile it for aarch64?"
107,679acfdfcbe8284d7a7242d4,issue,learning -> how to use a library/framework/tool,https://github.com/microsoft/PowerToys/issues/27165,"Snap to zone at maximizing / double click title bar
### Description of the new feature / enhancement

Hi,

i used displayfusion before. 
But since this ruined some games with the last update on my ultrawide i now use fancyzones, i am just missing one thing.

Sometimes i have windows which are not snapped or maximized. 
Then i double click them in the are i want them to get snapped (mostly in the middle, my zones are 1600 | 1920 | 1600 wide).
But then the window gets maximized over the entire screen.

I asked ChatGPT to give me a solution to hope i was blind.
It gave me ""Make maximized windows 'snap' to this zone"" but i could not find this option and also were not able to find it anywhere else. 

Am i missing something? 

### Scenario when this would be used?

Escpecially in my everyday life.
I am not using the predefined zones in the maximize buttons, i deactivated them to be honest.
I need this multiple times a day.

### Supporting information

Ultrawide, 5120x1440, splitted in 3 zones mostly (sometimes left and right are splitted vertically again)"
108,679acfdfcbe8284d7a724317,issue,software quality -> fixing -> functional bugs,https://github.com/0xERR0R/blocky/issues/1062,"Grafana dashboard buttons do not work
The buttons in the Grafana dashboard (https://grafana.com/grafana/dashboards/13768-blocky/) did not work for me. After I allowed JavaScript in Grafana and fixed the URL, it still popped up an error message. The network requests went to the correct API endpoint, and the /blocking/status endpoint reported that the commands had reached the blocky server. So the only problem seems to be in the UI. The issue seems to be that the endpoint `/api/blocking/disable?duration=300s` return an empty string whereas the `$.get` expects it to return a valid JavaScript object and therefore the get request fails by parse errors.

With the help of ChatGPT for debugging, I came up with the workaround to change out the 

```
$.get(url + '/api/blocking/'+op, function(data) {
    showAlert()
  })
```

to

```
 $.ajax({
    url: url + '/api/blocking/' + op,
    type: 'GET',
    dataType: 'text', // expect response as text
    success: function(data) {
      showAlert();
    },
    error: function() {
      alert(""error"");
    }
  });
```
 and now it seems to work as expected when Ajax is told to expect a text response and not a JSON response."
109,679ada32cbe8284d7a724378,issue,software quality -> fixing -> supporting debugging,https://github.com/Juniper/nita/issues/11,"Create a ""clippy"" style AI assistant in Jenkins, so that failed test cases can be sent to ChatGPT for suggestions.
None"
110,679ada32cbe8284d7a7243a9,issue,learning->brainstorming,https://github.com/huggingface/blog/issues/1292,"Errata on ""Illustrating Reinforcement Learning from Human Feedback (RLHF)""
https://huggingface.co/blog/rlhf

### Background

In the section on the third step of the process, it is written:

- What multiple organizations seem to have gotten to work is **fine-tuning some or all of the parameters of a copy of the initial LM** with a policy-gradient RL algorithm, Proximal Policy Optimization (PPO). 
- **Parameters of the LM are frozen because fine-tuning** an entire 10B or 100B+ parameter model **is prohibitively expensive**
   ...
- First, **the policy is a language model** that takes in a prompt and returns a sequence of text (or just probability distributions over text)

I'm confused, because based on this information:

- Fine-tuning an entire model is prohibitively expensive.
- Yet, the policy for the RL fine-tuning is the language model, which is a copy of the original LM
- Fine-tuning of which is prohibitively expensive

I may not understand this fully, but my current logic leads me to think that these statements are contradicting.

I tried to research the RL fine-tuning process a bit and asked ChatGPT to brainstorm with me about this contradiction. From what I've gathered the parameters of the original LM are frozen not because it is prohibitively expensive (as we're doing that anyway with the copy), but because:

- It allows the retention and leverage of valuable knowledge and language understanding acquired during pretraining of original LM, while making targeted adjustments to align the copy of the model with specific objectives or human preferences. This would also correlate with what is written later in the article about the KL divergence.
- It allows researchers to explore different fine-tuning approaches without modifying the original LM, enabling greater control and experimentation.

Hence, is the following a correct reasoning on freezing during fine-tuning?

- We freeze all of the original LM parameters - for these two aforementioned reasons
- We freeze some of the copy of the LM parameters - because it's expensive to fine-tune all of them 

This way, the statement about 'fine-tuning an entire 10B or 100B+ parameter model is prohibitively expensive' would be coherent with the rest of the information provided.

### Suggestions

If that reasoning is correct, then I think this sentence:

> fine-tuning some or all of the parameters of a copy of the initial LM

Would probably make more sense if it didn't state _'or all'_.

And additionally, the following statement:

> Parameters of the LM are frozen because fine-tuning an entire 10B or 100B+ parameter model is prohibitively expensive

Would make more sense if it started with _'Some parameters of the copy of the LM...'_.

And potentially I'd suggest adding an explanation regarding the original LM freezing, along the lines of:

> Parameters of the original LM are frozen in order to retain the valuable knowledge and language understanding acquired during pretraining, while making targeted adjustments to align the copy of the LM with specific objectives or human preferences

If there's something I'm missing here, please accept my apology for these suggestions and let me know what I got wrong."
111,679adadfcbe8284d7a724797,issue,software quality -> fixing -> supporting debugging,https://github.com/microsoftgraph/msgraph-training-php/issues/125,"Error getting users: cURL error 60: SSL certificate problem: unable to get local issuer certificate 
**Where did you get the code?**

- [x ] Downloaded from GitHub
- [ ] Downloaded from the [Microsoft Graph quick start tool](https://developer.microsoft.com/graph/quick-start)
- [ ] Followed the tutorial from [Microsoft Graph tutorials](https://learn.microsoft.com/graph/tutorials)

**Describe the bug**
A clear and concise description of what the bug is.

I downloaded the repo and got things inside my XAMPP directory , downloaded cacert.pem and put in the dir , i was able to verify this in the code and call the access token and the user-auth worked.

But when i called the list users in app-auth i get this error


Error getting users: cURL error 60: SSL certificate problem: unable to get local issuer certificate (see https://curl.haxx.se/libcurl/c/libcurl-errors.html) for https://graph.microsoft.com/v1.0/users?$select=displayName,id,mail&$orderBy=displayName&$top=25

i have looked EVERYWHERE online and used chatgpt to help solve this and all i can find is the ssl cert to be downloaded which ive already done and to disable ssl cert checks which i cant do.

**Screenshots**
If applicable, add screenshots to help explain your problem.

![image](https://github.com/microsoftgraph/msgraph-training-php/assets/135201235/fd007424-4970-424a-a907-71f1fdd1ca11)


**Desktop (please complete the following information):**

- OS: windows
- Browser n/a
- Version n/a"
112,679adb8ecbe8284d7a724a7f,issue,learning -> how to use a library/framework/tool,https://github.com/P4ELTE/t4p4s/issues/51,"Runing p4 programs from /examples gives error ""1 lcore data is invalid, exiting""
Hi,
I installed the t4p4s successfully on ubuntu 20.04.6. When I run test p4 example by running `./t4p4s.sh %l2fwd model=v1model port=1 verbose=1 dbg=1`, it runs.
But when I run actual p4 program by `./t4p4s.sh :l2fwd model=v1model port=1 verbose=1 dbg=1`, it gives me this error 
````
P4_FWD: lcore 1 has nothing to do
1 lcore data is invalid, exiting
P4_FWD: lcore 0 has nothing to do
0 lcore data is invalid, exiting
0 Flushing tables on all cores
0 Normal exit.
Tâ‚„Pâ‚„S switch running ./examples/p4_14/l2fwd.p4_14  exited normally

(Terminating controller dpdk_l2fwd_controller)
(Terminating the parse helper process, port 56373, pid 43766)
````

Can you help me to resolve it? What this error mean. I tried to google, used chatgpt but could not find any solution around it."
113,679adb8ecbe8284d7a724b5d,issue,learning -> how to use a library/framework/tool,https://github.com/Vanilagy/mp4-muxer/issues/11,"Muxing avc data from polyfill library
Hello again, 

back again with one challenging question. I'm using webcodecs VideoEncoder wherever it's possible, but e.g. firefox or older iOS / Safari versions don't support it yet. So I'm testing some polyfill projects and I was interested with this one: https://github.com/everywill/VideoEncoder

There is solution with own muxer, which really works but I would like to stay to use this mp4-muxer library. Also because it's supports sound muxing. 

So I was trying to use addVideoChunkRaw method and use data from VideoEncoder.  Again it seems like some problem with raw avc data. When I add there complete frame, it's playable in Chrome but not in Quicktime - mp4 seems to be broken.
So I tried to use ChatGPT to get some help with avc codec parsing (as previously helped me with aac) but I'm not luck for now. When I try to detect header and cut it from ArrayBuffer, video is still not playable.

I understand this time is hard to say why this not work due to third-party library but maybe you can have some tips where problem could be. I have two ideas:

1. AVC codec - header/data splitting - I'm doing it wrong and ChatGPT doesn't give me correct anwsers

2. Description attribute is missing in my muxer -> I noticed that when I use native VideoEncoder, it gives me metadata with description field within keyframe videochunk. When I not pass this metadata to your muxer - especially the description field - muxed video doesn't work correctly.

Thank you for any ideas
"
114,679adc25cbe8284d7a724d9b,issue,feature implementation/enhancement -> implementing a new feature,https://github.com/mprib/caliscope/issues/439,"interpolate small gaps in x,y estimates
Some sample code generated by chatGPT 

https://chat.openai.com/share/50c8e7f0-eec8-42a7-9289-188e8b1251fe

```python
import pandas as pd
import numpy as np

# Set the maximum size of a gap that will be filled
GAP_SIZE_TO_FILL = 3  # Change this value as needed

# Load the data
data = pd.read_csv('/path/to/xy_HOLISTIC_OPENSIM.csv')

# Initialize a DataFrame to store the data with filled gaps
data_filled = pd.DataFrame()

# Loop through each combination of port and point_id
for (port, point_id), group in data.groupby(['port', 'point_id']):
    # Sort by frame_index to ensure the data is in order
    group = group.sort_values('frame_index')
    
    # Calculate the differences between consecutive frame_indices
    group['frame_gap'] = group['frame_index'].diff()
    
    # Identify the gaps that are less than or equal to the specified size
    gaps_to_fill = group['frame_gap'].between(2, GAP_SIZE_TO_FILL + 1)
    
    # Interpolate the values for img_loc_x and img_loc_y
    group.loc[gaps_to_fill, 'img_loc_x'] = group['img_loc_x'].interpolate(method='linear').round().astype(int)
    group.loc[gaps_to_fill, 'img_loc_y'] = group['img_loc_y'].interpolate(method='linear').round().astype(int)
    
    # Append to the overall DataFrame
    data_filled = data_filled.append(group)

# Remove the 'frame_gap' column
data_filled.drop('frame_gap', axis=1, inplace=True)

# Write the DataFrame to a new CSV file
data_filled.to_csv('/path/to/xy_HOLISTIC_OPENSIM_gaps_filled.csv', index=False)


```"
115,679adcbfcbe8284d7a724ff8,issue,software quality -> fixing -> performance-related,https://github.com/vitoplantamura/OnnxStream/issues/10,"Compiler Optimizations Flags in Makefile
Hi, I asked ChatGPT about some optimization flags for the 'CMakeLists.txt' and it came up with this, to append at the EOF:


target_compile_options(sd PRIVATE -O3 -march=native -mtune=native -funroll-loops -finline-functions -ffast-math -flto -ftree-vectorize)


On my Raspberry Pi 400, now a diffusion step takes ~210000ms. (was before: ~630000)

I think the -O3 did the most, and I haven't really tested with all the others incrementally.

Yea, now it takes down the system, if I overclock it, so 1.9ghz it is for passive cooling.

Unfortunately I haven't got a Zero for experiments, just thought this may be of interest.

BTW someone care to tell me how to set a seed for random generation?

Happy diffusing!

edit; typo, and wanted to add my thx for this nice little software"
116,679adcbfcbe8284d7a725144,issue,feature implementation/enhancement -> scripting,https://github.com/linkwarden/linkwarden/issues/123,"A little python helper script to import bookmarks from raindrop.io
Just wanted to share in case anyone's interested. This is mostly generated by ChatGPT with a few tweaks.

[Import raindrop.io csv data into linkwarden self-hosted](https://gist.github.com/freakynit/d0918e4ee698427d6f069bb56b5c3f36)"
117,679ae7becbe8284d7a725563,issue,feature implementation/enhancement -> feature enhancement,https://github.com/layters/testshop/issues/186,"Kademlia routing algorithm correction and audits
```
Details
```
- [ ] The Kademlia Routing algorithm may be incorrect since I used ChatGPT to write the code. It may need to be re-written and audited.
- [ ] `get` requests block the GUI client app. 
    - put requests are simple and don't really affect the local GUI client as much since the local node (which exists on the server side) is sending data to the other nodes in the network, but get requests are expensive and so slow because the data being fetched will be used by and displayed via the GUI client. Is there any way to resolve this issue?
    - perhaps the `recv` timeout should be shorten to miliseconds rather than seconds?
    - `get_provider` request was made to make it easier to fetch data directly from the peers that have the data in their hash tables.
    - Should the default (hard-coded) bootstrap nodes be allowed to accept `map` requests in order to maintain a complete list of all keys and their providers?
    
        tl;rl: searching for a key in the DHT network can take forever which will probably block the GUI client app.
- [ ] Proper memory management and ensure thread safety, especially in `node.cpp` and `routing_table.cpp`, `msgpack.cpp`, and `daemon/main.cpp`.
    
```
Related files
```
https://github.com/larteyoh/testshop/blob/main/src/core/protocol/p2p/routing_table.cpp
https://github.com/larteyoh/testshop/blob/main/src/core/protocol/p2p/node.cpp
https://github.com/larteyoh/testshop/blob/main/src/core/protocol/messages/msgpack.cpp
https://github.com/larteyoh/testshop/blob/main/src/daemon/main.cpp
```
Bounty reward
```
(not yet decided)
"
118,679ae7becbe8284d7a72563c,issue,feature implementation/enhancement -> migrating/reusing via programming language translation,https://github.com/mikey0000/PyMammotion/issues/4,"Complete adding models
I've used chatgpt to translate the java models into python, this is by far the easiest way to do it.

- [ ] convert buffer arrays inside protobuf messages into their respective models, this has been done for RapidState so far and a few others under data/model"
119,679ae7becbe8284d7a72567c,issue,process -> automating commit/issue/PR creation -> creating commit/issue/PR description,https://github.com/smartstore/Smartstore/issues/784,"Browser Page Reload (F5) Doesn't Refresh the Plugin List in `/admin/module/list` in Smartstore 5.0.5
Hello Smartstore team,

I've encountered an issue in the Smartstore 5.0.5 admin panel, specifically in the `/admin/module/list` section. This issue affects admins who are uploading new plugins (or ""modules"").

**Steps to reproduce:**
1. Navigate to `/admin/module/list`.
2. Upload a new plugin.
3. Observe the green notification at the bottom left that says: ""Package xxx has been uploaded and successfully unpacked. Please reload the list."" (see screenshot with highlighted notification below)

The expected behavior for most web users would be to refresh the browser page using F5 to see the newly uploaded plugin in the plugin list. However, this action doesn't refresh the list as expected.

Instead, to see the newly uploaded plugin, one has to click on the grey button at the top right and select ""Reload List"".

**Suggested Solutions:**
1. Modify the notification text to explicitly instruct users to click on the ""Reload List"" button.
2. Alternatively, adjust the mechanism so that a browser page reload (F5) also refreshes the plugin list.

Here's a suggested revised notification text that provides clearer instructions to the user:

""Package xxx has been uploaded and successfully unpacked. To view it in the list, please click the 'Edit/Reload List' button at the top right."" This text offers a clear option for the user to see the updated list, making it more user-friendly and reducing potential confusion.

This would enhance the user experience and make the process more intuitive for admins.

Thank you for your attention to this matter. Looking forward to a resolution.

Best regards,
Stefan

![oakwt-hochladen-liste-neuladen-wording](https://github.com/smartstore/Smartstore/assets/54363171/ce6341a2-d825-49dc-bec5-b9149709fa96)

<sub>(this text was created with help of ChatGPT 4, see how here: https://chat.openai.com/share/f88b4b1f-f78a-4a9c-96d9-eb1f4f510e9f )</sub>"
120,679ae8d3cbe8284d7a725a6e,issue,feature implementation/enhancement -> feature enhancement,https://github.com/Open-Systems-Pharmacology/PK-Sim/issues/2717,"Run nightly builds only if there were any commits to the repo in the last 24 hours
Implementation suggestion from ChatGPT :)

=========================================================

To achieve your goal of running the AppVeyor project build on a schedule (every 24 hours), but only if there were any repository changes (commits) in the last 24 hours, you'll need to use a combination of scheduled builds and conditional checks in your `appveyor.yml` configuration file. Here's how you can set it up:

1. **Scheduled Build Configuration:**
   Set up a scheduled build that runs every 24 hours. You can do this by using the `schedule` configuration in your `appveyor.yml` file.

```yaml
schedule:
  - cron: ""0 */24 * * *""
```

This configuration will trigger a build every 24 hours at the start of the hour (e.g., 00:00, 01:00, etc.).

2. **Conditional Check for Repository Changes:**
   To check whether there were any repository changes (commits) in the last 24 hours before triggering the build, you'll need to use the `git` command within the `before_build` or `init` section of your `appveyor.yml` file.

```yaml
init:
  - ps: |
      $lastCommitDate = git log -1 --format=%ci
      $currentDate = Get-Date -Format ""yyyy-MM-dd HH:mm:ss""
      $timeDiff = New-TimeSpan -Start $lastCommitDate -End $currentDate
      if ($timeDiff.TotalHours -lt 24) {
        Write-Host ""Changes detected in the last 24 hours. Proceeding with the build.""
      }
      else {
        Write-Host ""No changes in the last 24 hours. Skipping the build.""
        Exit-AppVeyorBuild
      }
```

In this example, the script calculates the time difference between the last commit's timestamp and the current time. If the time difference is less than 24 hours, the build proceeds. Otherwise, the script exits the build.

3. **Rest of Your Build Configuration:**
   After the conditional check, you can define the rest of your build configuration as needed.

Putting it all together, your `appveyor.yml` might look something like this:

```yaml
schedule:
  - cron: ""0 */24 * * *""

init:
  - ps: |
      $lastCommitDate = git log -1 --format=%ci
      $currentDate = Get-Date -Format ""yyyy-MM-dd HH:mm:ss""
      $timeDiff = New-TimeSpan -Start $lastCommitDate -End $currentDate
      if ($timeDiff.TotalHours -lt 24) {
        Write-Host ""Changes detected in the last 24 hours. Proceeding with the build.""
      }
      else {
        Write-Host ""No changes in the last 24 hours. Skipping the build.""
        Exit-AppVeyorBuild
      }

# Define the rest of your build configuration here
```

Remember to adjust the paths and commands according to your project's structure and requirements."
121,679ae8d3cbe8284d7a725ab5,issue,feature implementation/enhancement -> feature enhancement,https://github.com/proudcity/wp-proudcity/issues/2340,"Clean up top admin bar
**Source**
The source of the issue (ex: Customer: #CustomerName#).

LF

**What feature would you like to improve**
A clear and concise description of the current feature.

Top admin bar

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

- [ ] Remove all plugin links (WP Rocket, WP Mail SMTP, Duplicate Post)
- [ ] Remove 'Live editor' link
- [ ] Move 'Help' to right `top-secondary` nav
- [ ] Add 'My Sites' link to right of PC icon and link both to my.proudcity.com

Menu display:

Left:

* Icon (links to my.proudcity.com)
* My Sites (links to my.proudcity.com)
* Dashboard (links to site admin dashboard)
* New

Right:

* Help
* Profile link/icon

**Additional context**
Add any other context or screenshots about the feature request here.

https://wordpress.com/support/admin-bar/

From ChatGPT on removing plugin links:

To remove a plugin link from the WordPress admin bar, you can use the `admin_bar_menu` action hook and check for the specific item to remove. Here's an example of how to do it:

```php
// Remove a specific plugin link from the admin bar
function remove_plugin_link_from_admin_bar($wp_admin_bar) {
    // Replace 'plugin-slug' with the actual plugin slug you want to remove
    $plugin_slug = 'plugin-slug';

    // Remove the plugin link
    $wp_admin_bar->remove_node($plugin_slug);
}
add_action('admin_bar_menu', 'remove_plugin_link_from_admin_bar', 999);
```

In this example, replace `'plugin-slug'` with the actual slug of the plugin link you want to remove from the admin bar. The `999` priority ensures that this action is executed after other items have been added to the admin bar.

Add this code to your theme's `functions.php` file or your custom plugin's main PHP file. After adding the code, refresh your WordPress admin area to see the changes. The specified plugin link should be removed from the admin bar."
122,679ae8d3cbe8284d7a725b81,issue,learning -> how to use a library/framework/tool,https://github.com/shadowsocks/shadowsocks-android/issues/3121,"How can Shadowsocks configure a socks proxy that requires only user name and password authentication
How can Shadowsocks configure a socks proxy that requires only user name and password authentication?
I've tried searching in issues but didn't find a proper answer and also asked chatgpt and google"
123,679ae8d3cbe8284d7a725c6a,issue,process -> automating commit/issue/PR creation -> creating commit/issue/PR description,https://github.com/kyegomez/LongNet/issues/16,"ModuleNotFoundError: No module named 'LongNet'

**Issue Description:**

I encountered an issue while using the `pip install LongNet` command to install the LongNet package. When attempting to import a module using `from LongNet.attention import DilatedAttention`, I received the following error:
```
ModuleNotFoundError: No module named 'LongNet'
```
Upon investigating the installation directory of pip packages (`/usr/local/lib/python3.11/dist-packages`), I noticed that the actual installation directory for the LongNet library was named as `longnet` (all lowercase).

**Suggested Resolution:**

It appears that there might be a mismatch between the library's name and the installation directory. To resolve this issue, one of the following actions could be taken:

1. Rename the Installation Directory:
   Rename the installation directory from `longnet` to `LongNet` to match the capitalization of the library's name. This would align the installation directory with the expected import statement.

2. Adjust Import Statements:
   Modify the import statements within the library's codebase to use lowercase import paths, matching the installation directory. For example, change `from LongNet.attention import DilatedAttention` to `from longnet.attention import DilatedAttention`.

*This was generated by ChatGPT based on my description.*

<!-- POLAR PLEDGE BADGE START -->
## Upvote & Fund

- We're using [Polar.sh](https://polar.sh/kyegomez) so you can upvote and help fund this issue.
- We receive the funding once the issue is completed & confirmed by you.
- Thank you in advance for helping prioritize & fund our backlog.

<a href=""https://polar.sh/kyegomez/LongNet/issues/16"">
<picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""https://polar.sh/api/github/kyegomez/LongNet/issues/16/pledge.svg?darkmode=1"">
  <img alt=""Fund with Polar"" src=""https://polar.sh/api/github/kyegomez/LongNet/issues/16/pledge.svg"">
</picture>
</a>
<!-- POLAR PLEDGE BADGE END -->
"
124,679aea6ecbe8284d7a726167,issue,software quality -> testing -> generating tests,https://github.com/Agenta-AI/agenta/issues/580,"Create initial tests for the mongodb service
**Is your feature request related to a problem? Please describe.**
@aybruhm has created an initial scaffold for testing the database logic. We would like to create some additional tests.

**Describe the solution you'd like**
Here is a list of tests from chatGPT

Given the provided code, which appears to be a collection of asynchronous database operations related to app variants, templates, and images, here's a list of tests you might consider running:


2. **`get_templates` Function**
   - Test that the function retrieves all the templates.
   - Test an empty database to see if it returns an empty list.

3. **`add_template` Function**
   - Test adding a template with valid data.
   - Test attempting to add a template with a duplicate `template_id` to ensure it doesn't overwrite.
   - Test with missing or invalid fields.

4. **`add_variant_based_on_image` Function**
   - Test adding a variant with a valid image.
   - Test with missing or invalid fields in both `app_variant` and `image`.
   - Test adding a variant with the same `app_name` and `variant_name` to ensure it raises a `ValueError`.

5. **`add_variant_based_on_previous` Function**
   - Test adding a new variant from a previous/template one with valid parameters.
   - Test with missing or invalid fields in `previous_app_variant`.
   - Test with a non-existent template variant.

6. **`list_app_variants` Function**
   - Test listing all app variants.
   - Test listing app variants with a specific `app_name`.
   - Test the function with the `show_soft_deleted` flag both as `True` and `False`.

7. **`list_apps` Function**
   - Test listing all unique app names.
   - Test with an empty database.

8. **`count_apps` Function**
   - Test counting all unique app names.

9. **`get_image` Function**
   - Test fetching the image for a valid app variant.
   - Test with a non-existent app variant.

10. **`remove_app_variant` Function**
    - Test removing an app variant that exists.
    - Test attempting to remove a non-existent app variant.
    - Test removing an app variant that has been soft deleted.

11. **`remove_image` Function**
    - Test removing a valid image.
    - Test attempting to remove a non-existent image.

12. **`check_is_last_variant` Function**
    - Test with an app variant that is the last one using its linked image.
    - Test with an app variant that isn't the last one using its linked image.

13. **`get_variant_from_db` Function**
    - Test getting a variant that exists in the database.
    - Test with a non-existent variant.

15. **`update_variant_parameters` Function**
    - Test updating parameters of a valid variant.
    - Test with non-matching parameter keys.
    - Test updating a non-existent variant.

16. **`remove_old_template_from_db` Function**
    - Test removing old templates that aren't in a given list.

"
125,679aea6ecbe8284d7a72628b,issue,software quality -> fixing -> supporting debugging,https://github.com/Mustard2/MustardSimplify/issues/7,"Error with linked objects in the scene
Python: Traceback (most recent call last):
  File ""C:\Users\Username\AppData\Roaming\Blender Foundation\Blender\3.6\scripts\addons\Simplify.py"", line 579, in execute
    add_prop_status(obj.MustardSimplify_Status.modifiers, [mod.name, mod.show_viewport])

  File ""C:\Users\Username\AppData\Roaming\Blender Foundation\Blender\3.6\scripts\addons\Simplify.py"", line 534, in add_prop_status
    add_item = collection.add()
TypeError: bpy_prop_collection.add(): not supported for this collection

It seems to only happen in this specific scene for me, but I am not sure what specifically in this scene is causing it. I asked ChatGPT about it and it said, it may be due to an incorrect usage of the add function. Or a collection mismatch. Can't really give an in-depth report since I have like very little experience with Python, let alone blender's API ^.^;"
126,679aeb08cbe8284d7a7265d9,issue,learning -> program comprehension,https://github.com/ph00lt0/blocklist/issues/49,"Block `metadata.io`
**Tool used (pick one)**:
 - [x] Rethink DNS + Firewall

**Domains:**
```
metadata.io
stagered-insights.metadata.io
insights.metadata.io
# see comment below
cdn.metadata.io
metadata-static-files.sfo2.cdn.digitaloceanspaces.com
```
<br>

stumbled upon it on `eightfold.ai`'s blog. chatgpt code eval ([1](https://archive.is/F7esV),[2](https://archive.is/wip/uMq8V)) says the code is mostly trackware:
- `https://cdn.metadata.io/site-insights.js` / https://archive.is/LNvRt
- `https://metadata-static-files.sfo2.cdn.digitaloceanspaces.com/pixel/lp.js` / https://archive.is/9MI7q

also at: https://github.com/badmojr/1Hosts/issues/1451"
127,679aeba8cbe8284d7a726828,issue,software quality -> refactoring,https://github.com/fzyzcjy/flutter_rust_bridge/issues/1381,"[very silly suggestion] rename the project stardust
### Proposal

Rename `flutter_rust_bridge` -> `stardust`

(When I asked chatGPT a name for a project that uses both dart and rust
it suggested `stardust`, my first thought was ""it's too perfect, I don't deserve it!"")

### Reasoning

1. the word `stardust` is the anagram of `dart` + `rust`
2. `flutter_rust_bridge` suggests that targets `flutter` while can be used for pure dart projects as well
3. `*_bridge` screams `react-native` 

"
128,679aeba8cbe8284d7a7268d9,issue,learning -> how to use a library/framework/tool,https://github.com/jackMort/ChatGPT.nvim/issues/294,"Disable folding in ChatGPTEditWithInstructions.
Greetings! 
I would like to disable folding for chatgpt folding. For telescope we do this:

`vim.api.nvim_create_autocmd(""FileType"", { pattern = ""TelescopeResults"", command = [[setlocal nofoldenable]] })`

It seems that for ChatGPT.nvim plugin we don't have filetype names and this trick doesn't work.


PS I asked ChatGPT at it didn't help :)"
129,679af5e3cbe8284d7a726b1d,issue,software quality -> fixing -> supporting debugging,https://github.com/mtalcott/google-photos-deduper/issues/25,"Server 404
Hi! No matter what I try and in which port, I'm always getting 404 when trying to connect to the server through http://localhost

Does anyone have this issue and can give me ideas? I have no idea what's going wrong since it seems to be running correctly, but when I try to access localhost in my browser, that's what my terminal shows:

google-photos-deduper-main-server-1  | 192.168.65.1 - - [04/Oct/2023 20:26:13] ""GET / HTTP/1.1"" 404 -
google-photos-deduper-main-server-1  | 192.168.65.1 - - [04/Oct/2023 20:26:13] ""GET / HTTP/1.1"" 404 -

I already tried every solution from ChatGPT and it's still not working. The flask has to be the problem, since the client opens a page at localhost:3000

Thanks in advance for any help!
"
130,679af6e7cbe8284d7a726e7d,issue,learning -> how to use a library/framework/tool,https://github.com/TDAmeritrade/stumpy/issues/926,"Request: stumpi equivalent for multivariate case
Is there a stumpi equivalent for use with multiple variables (multiple timeseries) so I can incrementally add a new record? I read the article on using stumpi (much appreciated). When I initially asked chatGPT about incrementally updating a multivariate matrix profile in stumpy, it helpfully suggested ""mstumpi"". Looking at the documentation, it appears ChatGPT had a hallucination and sent me searching for that function :-)

Right now it appears the only way to do it is to batch a few streaming events and recompute the matrix in its entirety, perhaps using mstumped to benefit from distributed computing."
131,679af76dcbe8284d7a7271ed,issue,"learning -> how to use a library/framework/tool,software quality -> fixing -> supporting debugging",https://github.com/EleutherAI/lm-evaluation-harness/issues/939,"Loading GPTQ model, unexpected keyword argument 'quantized'
I'm trying to run the CMMLU evaluation on a GPTQ model I have saved locally with the following command:

python main.py --model hf-causal --model_args pretrained=C:/Users/admin/Documents/models/TheBloke_StableBeluga-13B-GPTQ, quantized=model.safetensors --tasks cmmlu-clinical_knowledge --device cuda:0

This matches the format for running GPTQ show in the repo readme. However, I am getting the following error:

main.py: error: unrecognized arguments: quantized=model.safetensors

To try and solve this, I wrapped all the model_args in parenthesis as follows:

python main.py --model hf-causal --model_args ""pretrained=C:/Users/admin/Documents/models/TheBloke_StableBeluga-13B-GPTQ,quantized=model.safetensors"" --tasks cmmlu-clinical_knowledge --device cuda:0

This then causes the following error:

 File ""C:\Users\admin\Documents\LLMs\evaluation\lm-evaluation-harness\lm_eval\base.py"", line 115, in create_from_arg_string
    return cls(**args, **args2)
TypeError: HFLM.__init__() got an unexpected keyword argument 'quantized'

I asked ChatGPT and its response regarding this was:

This error message suggests that while you've structured the command correctly based on the README, the underlying code in lm-evaluation-harness doesn't support or recognize the quantized keyword argument for the HFLM class.

I was hoping to get some help on how to load GPTQ models for evaluation. Thanks in advance for any help.
"
132,679af7fccbe8284d7a72735f,issue,feature implementation/enhancement -> feature enhancement,https://github.com/ethers-io/ethers.js/issues/4440,"ethersv6 in React Native TypeScript 
### Ethers Version

6.8.0

### Search Terms

custom cryptographic implementations

### Describe the Problem

I have a React Native Expo Project in TypeScript and i want to include ethersJS to implement wallet funcitonality for an ethereum wallet.

first things first. This here is the package.json for my project:

```
{
  ""name"": ""music-player-wallet"",
  ""version"": ""1.0.0"",
  ""main"": ""node_modules/expo/AppEntry.js"",
  ""scripts"": {
    ""start"": ""expo start"",
    ""android"": ""expo start --android"",
    ""ios"": ""expo start --ios"",
    ""web"": ""expo start --web"",
    ""ts:check"": ""tsc""
  },
  ""dependencies"": {
    ""@ethersproject/shims"": ""^5.7.0"",
    ""@expo/webpack-config"": ""^19.0.0"",
    ""@react-navigation/bottom-tabs"": ""^6.5.11"",
    ""@react-navigation/native"": ""^6.1.9"",
    ""@react-navigation/native-stack"": ""^6.9.16"",
    ""@types/node"": ""^20.8.9"",
    ""ethers"": ""^6.8.0"",
    ""expo"": ""~49.0.13"",
    ""expo-crypto"": ""~12.4.1"",
    ""expo-status-bar"": ""~1.6.0"",
    ""node-libs-react-native"": ""^1.2.1"",
    ""prettier"": ""^3.0.3"",
    ""react"": ""18.2.0"",
    ""react-dom"": ""18.2.0"",
    ""react-native"": ""0.72.5"",
    ""react-native-crypto"": ""^2.2.0"",
    ""react-native-quick-crypto"": ""^0.6.1"",
    ""react-native-safe-area-context"": ""4.6.3"",
    ""react-native-screens"": ""~3.22.0"",
    ""react-native-web"": ""~0.19.6"",
    ""rn-nodeify"": ""^10.3.0""
  },
  ""devDependencies"": {
    ""@babel/core"": ""^7.20.0"",
    ""@types/react"": ""~18.2.14"",
    ""@types/react-native"": ""^0.72.3"",
    ""@types/react-native-vector-icons"": ""^6.4.16"",
    ""ts-node"": ""^10.9.1"",
    ""typescript"": ""^5.2.2""
  },
  ""private"": true
}
```
this here is my tsconfig.json file:
```
{
  ""extends"": ""expo/tsconfig.base"",
  ""compilerOptions"": {
    ""strict"": true,
    ""jsx"": ""react-native"",
    ""types"": [""ethers""]
  },
  ""include"": [""**/*.ts"", ""**/*.tsx""],
  ""paths"": {
    ""@/*"": [""src/*""]
  }
}
```
i followed some instructions online already and added a metro.config.js file which looks like this:
```ts
// Learn more https://docs.expo.io/guides/customizing-metro
const { getDefaultConfig } = require(""expo/metro-config"");

// eslint-disable-next-line no-undef
const config = getDefaultConfig(__dirname);

config.resolver.resolveRequest = (context, moduleName, platform) => {
  if (moduleName === ""ethers"") {
    return context.resolveRequest(context, ""ethers/lib.esm/index.js"", platform);
  }
  return context.resolveRequest(context, moduleName, platform);
};

module.exports = {
  config,
  resolver: {
    extraNodeModules: require(""node-libs-react-native""),
  },
};
```
my babel.config.js file looks like this:
```ts
module.exports = function (api) {
  api.cache(true);
  return {
    presets: [""babel-preset-expo"", ""module:metro-react-native-babel-preset""],
    overrides: [
      {
        test: ""./node_modules/ethers"",
        plugins: [
          ""@babel/plugin-proposal-private-property-in-object"",
          ""@babel/plugin-proposal-class-properties"",
          ""@babel/plugin-proposal-private-methods"",
        ],
      },
    ],
  };
};
```
i created a crypto-shim.js file:
```ts
import { getRandomValues as expoCryptoGetRandomValues } from ""expo-crypto"";

class Crypto {
  getRandomValues = expoCryptoGetRandomValues;
}

const webCrypto = typeof crypto !== ""undefined"" ? crypto : new Crypto();

(() => {
  if (typeof crypto === ""undefined"") {
    Object.defineProperty(window, ""crypto"", {
      configurable: true,
      enumerable: true,
      get: () => webCrypto,
    });
  }
})();
```
then with the help of chatGPT i created ethersTypes.ts:
```ts
declare module ""ethers"" {
  interface RandomBytesFunction {
    (length: number): Uint8Array;
  }

  interface ComputeHmacFunction {
    (algo: string, key: Uint8Array, data: Uint8Array): Uint8Array;
  }

  interface Pbkdf2Function {
    (
      passwd: Uint8Array,
      salt: Uint8Array,
      iter: number,
      keylen: number,
      algo: string
    ): Uint8Array;
  }

  interface Sha256Function {
    (data: Uint8Array): Uint8Array;
  }

  interface Sha512Function {
    (data: Uint8Array): Uint8Array;
  }
}
```
included it in a file called ethersUtils.ts which shows the functions that are recommended in the ethersv6 docs:
```ts
import ""./crypto-shim"";
import ""./ethersTypes"";
import ""@ethersproject/shims"";
import { ethers } from ""ethers"";
import crypto from ""react-native-quick-crypto"";

// Register custom cryptographic implementations with ethers
ethers.utils.randomBytes = (length: number) =>
  new Uint8Array(crypto.randomBytes(length));

ethers.utils.computeHmac = (
  algo: string,
  key: Uint8Array,
  data: Uint8Array
) => {
  return new Uint8Array(crypto.createHmac(algo, key).update(data).digest());
};

ethers.utils.pbkdf2 = (
  passwd: Uint8Array,
  salt: Uint8Array,
  iter: number,
  keylen: number,
  algo: string
) => {
  return new Uint8Array(crypto.pbkdf2Sync(passwd, salt, iter, keylen, algo));
};

ethers.utils.sha256 = (data: Uint8Array) => {
  return new Uint8Array(crypto.createHash(""sha256"").update(data).digest());
};

ethers.utils.sha512 = (data: Uint8Array) => {
  return new Uint8Array(crypto.createHash(""sha512"").update(data).digest());
};
```
and last but not least i have a ethers.ts file that should handle Wallet Creation and all the functionality of ethersJS:
```ts
//wallet creation
//key storage
import ""./crypto-shim"";
import ""./ethereumUtils"";
import ""@ethersproject/shims"";
import { ethers } from ""ethers"";

const wallet = ethers.Wallet.createRandom();
console.log(""Wallet:"", wallet);
```

But it shows this error here: 
![Bildschirmfoto 2023-10-26 um 14 46 18](https://github.com/ethers-io/ethers.js/assets/96432557/abbef9ea-5596-4216-bde6-478c79cf4936)

The Same Error occurs on each file when i want to import {ethers} from ""ethers"". Can anyone help please?


### Code Snippet

_No response_

### Contract ABI

_No response_

### Errors

_No response_

### Environment

_No response_

### Environment (Other)

_No response_"
133,679af877cbe8284d7a7276a9,issue,process -> automating commit/issue/PR creation -> creating commit/issue/PR description,https://github.com/redjohnsh/arewelibadwaitayet/issues/1,"Design Enhancement Collaboration Opportunity
ðŸŽ¨ **Calling All Design Enthusiasts!**

Are you a designer with a passion for creating visually appealing web experiences? We need your help to enhance the appearance of our website, ""Are We LibAdwaita Yet?"".

**Project Overview**:

- Project Status: Solo development effort by the project owner.
- Website: [Link to the website]

**Scope of Work**:

- UI/UX Enhancement: Elevate the user interface and experience.
- Visual Identity: Create a compelling visual identity for the website.
- Layout & Styling: Refine layout, typography, and colors for a modern look.
- Responsiveness: Optimize for various screen sizes and devices.

**How to Get Involved**:

1. Express Interest: Comment on this issue if you're interested.
2. Introduce Yourself: Share a brief intro and your design background.
3. Portfolio: Provide a link to your design portfolio or past work.
4. Ideas: Feel free to share initial design concepts or thoughts.

We Value Your Contribution:

Join us to make our website visually stunning! Your work will be credited, and you'll play a pivotal role in enhancing the site's appearance.

ðŸ“ **Note**: This issue description was redacted by ChatGPT for brevity and clarity."
134,679af9d4cbe8284d7a727a93,issue,learning -> how to use a library/framework/tool,https://github.com/zylon-ai/private-gpt/issues/1223,"Command ""poetry run python scripts/setup"" on Conda Powershell returns that a module does not exists.
Hello, I am not sure if this is an implementation issue, a windows issue or what, I couldn't find any information online. 

I was following this video: 
https://www.youtube.com/watch?v=XFiof0V3nhA

I was able to get to:
```
poetry install --with ui,local
```

So far, so good, I've checked the modules installed in my conda environment: 
```
(privategpt)  ~\privateGPT  > conda list |select-string pydantic*

pydantic                  2.4.2                    pypi_0    pypi
pydantic-core             2.10.1                   pypi_0    pypi
pydantic-extra-types      2.1.0                    pypi_0    pypi
pydantic-settings         2.0.3                    pypi_0    pypi
```

I checked the folder: (...) miniconda3\envs\privategpt\Lib\site-packages\pydantic\v1  
And after all that, when I run the `poetry run python scripts/setup` I get the following error: 
```
> poetry run -vvv python scripts/setup
Using virtualenv: C:\Users\Fran\miniconda3\envs\privategpt
Traceback (most recent call last):
  File ""C:\Users\Fran\privateGPT\scripts\setup"", line 6, in <module>
    from private_gpt.paths import models_path, models_cache_path
  File ""C:\Users\Fran\privateGPT\private_gpt\paths.py"", line 4, in <module>
    from private_gpt.settings.settings import settings
  File ""C:\Users\Fran\privateGPT\private_gpt\settings\settings.py"", line 5, in <module>
    from private_gpt.settings.settings_loader import load_active_profiles
  File ""C:\Users\Fran\privateGPT\private_gpt\settings\settings_loader.py"", line 8, in <module>
    from pydantic.v1.utils import deep_update, unique_list
ModuleNotFoundError: No module named 'pydantic.v1'
```

I can't make sense of this, I am not sure how is conda setting the environment but I even did: 
```python 
(privategpt) îž­  ~\privateGPT  git:î‚ main python
Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> pydantic
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'pydantic' is not defined
>>> import pydantic
>>> pydantic
<module 'pydantic' from 'C:\\Users\\Fran\\AppData\\Roaming\\Python\\Python311\\site-packages\\pydantic\\__init__.cp311-win_amd64.pyd'> 
```

Another approach I had, using ""poetry run python"" alone to get a prompt:
``` python
(privategpt) îž­  ~\privateGPT  git:î‚ main poetry run python
Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import functools
>>> import logging
>>> import os
>>> import sys
>>> from pathlib import Path
>>> from typing import Any
>>> from pydantic.v1.utils import deep_update,unique_list
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'pydantic.v1'
```

Here I then ran ""help()"" and then ""modules pydantic"": 
``` 

help> modules pydantic

Here is a list of modules whose name or summary contains 'pydantic'.
If there are any, enter a module name to get more help.

openapi_schema_pydantic
openapi_schema_pydantic.util
openapi_schema_pydantic.v3
openapi_schema_pydantic.v3.v3_0_3 - OpenAPI v3.0.3 schema types, created according to the specification:
openapi_schema_pydantic.v3.v3_0_3.callback
openapi_schema_pydantic.v3.v3_0_3.components
openapi_schema_pydantic.v3.v3_0_3.contact
openapi_schema_pydantic.v3.v3_0_3.discriminator
openapi_schema_pydantic.v3.v3_0_3.encoding
openapi_schema_pydantic.v3.v3_0_3.example
openapi_schema_pydantic.v3.v3_0_3.external_documentation
openapi_schema_pydantic.v3.v3_0_3.header
openapi_schema_pydantic.v3.v3_0_3.info
openapi_schema_pydantic.v3.v3_0_3.license
openapi_schema_pydantic.v3.v3_0_3.link
openapi_schema_pydantic.v3.v3_0_3.media_type
openapi_schema_pydantic.v3.v3_0_3.oauth_flow
openapi_schema_pydantic.v3.v3_0_3.oauth_flows
openapi_schema_pydantic.v3.v3_0_3.open_api
openapi_schema_pydantic.v3.v3_0_3.operation
openapi_schema_pydantic.v3.v3_0_3.parameter
openapi_schema_pydantic.v3.v3_0_3.path_item
openapi_schema_pydantic.v3.v3_0_3.paths
openapi_schema_pydantic.v3.v3_0_3.reference
openapi_schema_pydantic.v3.v3_0_3.request_body
openapi_schema_pydantic.v3.v3_0_3.response
openapi_schema_pydantic.v3.v3_0_3.responses
openapi_schema_pydantic.v3.v3_0_3.schema
openapi_schema_pydantic.v3.v3_0_3.security_requirement
openapi_schema_pydantic.v3.v3_0_3.security_scheme
openapi_schema_pydantic.v3.v3_0_3.server
openapi_schema_pydantic.v3.v3_0_3.server_variable
openapi_schema_pydantic.v3.v3_0_3.tag
openapi_schema_pydantic.v3.v3_0_3.util
openapi_schema_pydantic.v3.v3_0_3.xml
openapi_schema_pydantic.v3.v3_1_0 - OpenAPI v3.1.0 schema types, created according to the specification:
openapi_schema_pydantic.v3.v3_1_0.callback
openapi_schema_pydantic.v3.v3_1_0.components
openapi_schema_pydantic.v3.v3_1_0.contact
openapi_schema_pydantic.v3.v3_1_0.discriminator
openapi_schema_pydantic.v3.v3_1_0.encoding
openapi_schema_pydantic.v3.v3_1_0.example
openapi_schema_pydantic.v3.v3_1_0.external_documentation
openapi_schema_pydantic.v3.v3_1_0.header
openapi_schema_pydantic.v3.v3_1_0.info
openapi_schema_pydantic.v3.v3_1_0.license
openapi_schema_pydantic.v3.v3_1_0.link
openapi_schema_pydantic.v3.v3_1_0.media_type
openapi_schema_pydantic.v3.v3_1_0.oauth_flow
openapi_schema_pydantic.v3.v3_1_0.oauth_flows
openapi_schema_pydantic.v3.v3_1_0.open_api
openapi_schema_pydantic.v3.v3_1_0.operation
openapi_schema_pydantic.v3.v3_1_0.parameter
openapi_schema_pydantic.v3.v3_1_0.path_item
openapi_schema_pydantic.v3.v3_1_0.paths
openapi_schema_pydantic.v3.v3_1_0.reference
openapi_schema_pydantic.v3.v3_1_0.request_body
openapi_schema_pydantic.v3.v3_1_0.response
openapi_schema_pydantic.v3.v3_1_0.responses
openapi_schema_pydantic.v3.v3_1_0.schema
openapi_schema_pydantic.v3.v3_1_0.security_requirement
openapi_schema_pydantic.v3.v3_1_0.security_scheme
openapi_schema_pydantic.v3.v3_1_0.server
openapi_schema_pydantic.v3.v3_1_0.server_variable
openapi_schema_pydantic.v3.v3_1_0.tag
openapi_schema_pydantic.v3.v3_1_0.xml
pydantic
pydantic._hypothesis_plugin
pydantic.annotated_types
pydantic.class_validators
pydantic.color
pydantic.config
pydantic.dataclasses
pydantic.datetime_parse
pydantic.decorator
pydantic.env_settings
pydantic.error_wrappers
pydantic.errors
pydantic.fields
pydantic.generics
pydantic.json
pydantic.main
pydantic.mypy
pydantic.networks
pydantic.parse
pydantic.schema
pydantic.tools
pydantic.types
pydantic.typing
pydantic.utils
pydantic.validators
pydantic.version
gradio.data_classes - Pydantic data models and other dataclasses. This is the only file that uses Optional[]
langchain.output_parsers.pydantic
langchain.pydantic_v1
langchain.pydantic_v1.dataclasses
langchain.pydantic_v1.main
langchain.tools.openapi.utils.api_models - Pydantic models for parsing an OpenAPI spec.
langchain.utils.pydantic - Utilities for tests.
llama_index.bridge.pydantic
llama_index.output_parsers.pydantic - Pydantic output parser.
llama_index.selectors.pydantic_selectors
pydantic_core
pydantic_core._pydantic_core
pydantic_core.core_schema - This module contains definitions to build schemas which `pydantic_core` can
pydantic_extra_types
pydantic_extra_types.color - Color definitions are used as per the CSS3
pydantic_extra_types.coordinate
pydantic_extra_types.country - Country definitions that are based on the ISO 3166 format
pydantic_extra_types.mac_address - MAC address Parsing and Validation
pydantic_extra_types.payment - Represents and validates payment cards (such as a debit or credit card),
pydantic_extra_types.phone_numbers
pydantic_extra_types.routing_number
pydantic_settings
```


Then, I asked ChatGPT and suggested the following: 
```python
import pydantic
print(pydantic.__file__)
```
This returned: 
`%Appdata%\Python\Python311\site-packages\pydantic\__init__.cp311-win_amd64.pyd`

Therefore, is this conda working wrong? 
I understand then that it's loading a pydantic module with the wrong version, basically it's giving zero effs about the conda environment, right?


Now, here's where IDK what is going on, I'm no python dev and packages/modules/environments confuse me. 




Can someone help me here? I'm really lost on what's the issue. 
"
135,679af9d4cbe8284d7a727b12,issue,learning -> how to use a library/framework/tool,https://github.com/acornjs/acorn/issues/1265,"Question: Are there in-depth examples of plugins?
First, I would like to thank you for providing such an amazing library!  I have been using Acorn for 5+ years, now, in my own little part-time project.  I have managed to create very simple plugins to achieve my goals, but now I am trying to do more.  I am using Acorn to generate AST that I use to transpile source into a custom JS runtime for my game engine (KMUD).  I am hoping someone can lead me to some in-depth examples that would help me implement the following:

- The ""easy"" request: A ""DereferencedCallExpression"" token using a custom operator ('->').  My codebase uses ""wrappers"" (a weak reference to an object instance), but in practice this could be an object reference, a wrapper (function that evaluates to an instance), or even a string literal.  The LHS would be an identifier or a literal.  The RHS would be a CallExpression.  Examples:
   - wrapper->eventSendChat('Hello World');
   - ""/daemon/ChatDaemon""->eventSendChat('Hello World');
  
I have stepped through the CallExpression code in a debugger and I think I understand what is going on.  I could override every method in that call stack, but I am hoping there is an easier solution.  I asked ChatGPT to create a custom Acorn plugin and the result was rather amusing:

 I asked ChatGPT to write me a plugin to support a custom '->' call operator and it responded confidently with a code sample with usage.  I pointed out to the AI that the token it was actually using was '=>' arrow pointer and not what I wanted so it said ""yep, yer right"" and rewrote the sample after defining '->'.  I then noticed it was calling a method called match() when parsing and I asked, ""what version of Acorn is this sample written for?""  It says 8.x so I upgrade, but match() still does not exist and I call it out.  ""Oh, sorry, that should be .eat(token)"".  I try again and get a runtime error as '->' is still not recognized.  I call this out and ChatGPT gives up, telling me ""Adding new operators in Acorn is difficult, perhaps you should try Bable
   
- The hard: I would also like to have a '::' scope resolution operator.  I have implemented a mixin pattern that allows for ""multiple inheritance.""  Obviously, ""there can only be one"" super class, but mixins are a nice way to extend a type.  In my codebase, a type is extended and only the first instance of a new symbol/identifier is grafted onto the prototype, but all implementations of the same name are available via a makeshift virtual table.  Currently you can call an unbound mixin implementation using something like: ob->callMixinImpl('getItems', 'MixinClassName'), but that's gross and error-prone.  I would love to support something like ob->MixinClassName::getItems().  Bonus if the solution supports all member expressions (mixins can expose properties).

If anyone knows of meaningful examples that might help me tackle these I would greatly appreciate it.
"
136,679af9d4cbe8284d7a727c04,issue,software quality -> fixing -> supporting debugging,https://github.com/rvlenth/emmeans/issues/456,"Error in t(OO[[rr]]) : object 'unpackedMatrix_transpose' not found
## Describe the bug
I fit a model: `lmer_richness_tot_log <- lme4::lmer(richness_tot_log ~ species * h2o + (1 | bloc),
                                    data = div_species, REML = TRUE)`
Then, I try to obtain the emmeans for this model: `emm_lmer_richness_tot_log <- emmeans::emmeans(lmer_richness_tot_log,
                                              specs = list(pairwise ~ h2o | species),
                                              type = ""response"", adjust = ""tukey"")`
... and R gives me the following error message: `Error in t(OO[[rr]]) : object 'unpackedMatrix_transpose' not found`
The error occurs only for particular models from my R environment. For example this model works: `glmer_richness_patho <- lme4::glmer(richness_rarfy_patho ~ sp * h2o + (1 | bloc),
                                    data = agg_dat_df, family = poisson)` I also tried a `lme4::lmer()` type of model in a different R environment and this model also worked with the  `emmeans::emmeans` function. I searched on the internet and used ChatGPT to try to solve this error, but was unsuccessful so far. I updated all my R packages. Perhaps, this error is connected to the recent update of emmeans? I did not try to downgrade to an older version of emmeans yet.

## To reproduce
[div_species.csv](https://github.com/rvlenth/emmeans/files/13370006/div_species.csv)

`lmer_richness_tot_log <- lme4::lmer(richness_tot_log ~ species * h2o + (1 | bloc),
                                    data = div_species, REML = TRUE)`

`emm_lmer_richness_tot_log <- emmeans::emmeans(lmer_richness_tot_log,
                                              specs = list(pairwise ~ h2o | species),
                                              type = ""response"", adjust = ""tukey"")`


## Expected behavior
I expect to see the pairwise comparisons of each species by H2O treatment in the R console when I run the resulting emmeans object.

## Additional context
I run on the operating system Pop!_OS 22.04 LTS (similar to Ubuntu)

## Ground rules
  * Yes, I looked at vignettes and documentation. My code was working as it is a few weeks ago."
137,679b0423cbe8284d7a727fc8,issue,software quality -> fixing -> functional bugs,https://github.com/yarnpkg/berry/issues/5987,"[Bug?]: `yarn constraints` exits with fatal error if `yarn.config.cjs` imports from ESM, unless executed with `DISABLE_V8_COMPILE_CACHE=1 yarn constraints`
Hey! Thanks for all your work on Yarn 4. :) I love it so far, and writing constraints in JS is a nice change. I've encountered a bug, but there's a workaround...

-------

### Self-service

- [ ] I'd be willing to implement a fix

### Describe the bug

Running `yarn constraints` throws a fatal error (""`A dynamic import callback was not specified`"") when `yarn.config.cjs` attempts to `import()` code from an ECMAScript Module. This is a bug, because dynamic `import()` should be available in all CommonJS modules (in fact, when you try to `require()` from an ESM, the error message even tells you to switch to `import()`).

However, the script executes successfully when executed with `DISABLE_V8_COMPILE_CACHE=1 yarn constraints`. This indicates the bug is likely in the `v8-compile-cache` package (possibly the one referenced in this [open issue from 2020](https://github.com/zertosh/v8-compile-cache/issues/30)). However, I have not investigated the root cause. For me, the workaround is fine, and I simply added a script in my `package.json` to set the variable: `""scripts"": { ""check-constraints"": ""DISABLE_V8_COMPILE_CACHE=1 yarn constraints"" }`.

Possibly related: I am using the `node-modules` linker.

### To reproduce

## 1. Create a new Yarn project with `node-modules` linker

First, create a new Yarn project. I've noticed this bug with the `node-modules` linker. I haven't attempted to replicate it with PnP. This is my `.yarnrc.yml`:

```yml
nodeLinker: node-modules
```

## 2. Install the `is-exact-version` package

The package I am attempting to import is [`is-exact-version`](https://github.com/jacobq/is-exact-version), which is installed in the `dependencies` of my root workspace. You can reproduce it by installing the same version I have:

```
yarn add -E is-exact-version@2.0.0-beta.1
```

This is what the main export of that package looks like (note that it's using `import` statements):

```bash
# The main export is a .js file
â¯ jq -r '.main' node_modules/is-exact-version/package.json
dist/is-exact-version.js

# It's the only exported .js file
â¯ ls node_modules/is-exact-version/dist/
is-exact-version.d.ts   is-exact-version.js     is-exact-version.js.map

# It imports from its dependency semver using ESM syntax
â¯ head node_modules/is-exact-version/dist/is-exact-version.js
import clean from 'semver/functions/clean.js';
import Range from 'semver/classes/range.js';
let log = function (...args) { };
(async function optionallyLoadDebugLogger() {
    try {

# It's depending on version 7.3.5 of semver
â¯ yarn why semver
...
â”œâ”€ is-exact-version@npm:2.0.0-beta.1
â”‚  â””â”€ semver@npm:7.5.4 (via npm:^7.3.5)
â”‚
â”œâ”€ is-exact-version@npm:2.0.0-beta.1 [d00d6]
â”‚  â””â”€ semver@npm:7.5.4 (via npm:^7.3.5)
â”‚
```

## 3. Sanity check: `require()` fails, as expected

Put this into `yarn.config.cjs` and run `yarn constraints`:

```js
const { defineConfig } = require(""@yarnpkg/types"");
const { isExactVersion } = require(""is-exact-version"");

module.exports = defineConfig({
  constraints: async (ctx) => {
    ctx.Yarn.dependencies()[0].error(""The script ran successfully, at least"");
    return;
  },
});
```

As expected, we get an error telling us to `import` instead:

```bash
â¯ yarn constraints
Internal Error: require() of ES Module /Users/myusername/path-to/my-project/node_modules/is-exact-version/dist/is-exact-version.js from /Users/myusername/path-to/my-project/yarn.config.cjs not supported.
Instead change the require of is-exact-version.js in /Users/myusername/path-to/my-project/yarn.config.cjs to a dynamic import() which is available in all CommonJS modules.
Instead change the require of is-exact-version.js in /Users/myusername/path-to/my-project/yarn.config.cjs to a dynamic import() which is available in all CommonJS modules.
    at require2 (/Users/myusername/.nvm/versions/node/v20.9.0/lib/node_modules/corepack/dist/lib/corepack.cjs:36309:24)
    at Object.<anonymous> (/Users/myusername/path-to/my-project/yarn.config.cjs:2:28)
    at Module2._compile (/Users/myusername/.nvm/versions/node/v20.9.0/lib/node_modules/corepack/dist/lib/corepack.cjs:36326:34)
    at require2 (/Users/myusername/.nvm/versions/node/v20.9.0/lib/node_modules/corepack/dist/lib/corepack.cjs:36309:24)
    at yne (/Users/myusername/.cache/node/corepack/yarn/4.0.1/yarn.js:140:54306)
    at zp (/Users/myusername/.cache/node/corepack/yarn/4.0.1/yarn.js:140:55036)
    at St.loadUserConfig (/Users/myusername/.cache/node/corepack/yarn/4.0.1/yarn.js:210:5667)
    at async p0.execute (/Users/myusername/.cache/node/corepack/yarn/4.0.1/yarn.js:526:1798)
    at async p0.validateAndExecute (/Users/myusername/.cache/node/corepack/yarn/4.0.1/yarn.js:94:787)
    at async as.run (/Users/myusername/.cache/node/corepack/yarn/4.0.1/yarn.js:98:3250)
    at async iPt (/Users/myusername/.cache/node/corepack/yarn/4.0.1/yarn.js:734:11269)
    at async sk (/Users/myusername/.cache/node/corepack/yarn/4.0.1/yarn.js:734:11625)
```

## 4. Reproduce the bug: `import()` fails

Try the simplest possible dynamic `import()`:

```js
const { defineConfig } = require(""@yarnpkg/types"");
module.exports = defineConfig({
  constraints: async (ctx) => {
    const thisIsAnEcmaScriptModule = await import(""is-exact-version"");
    ctx.Yarn.dependencies()[0].error(""The script ran successfully, at least"");
    return;
  },
});
```

This fails with error:

```bash
â¯ yarn constraints
Type Error: A dynamic import callback was not specified.
    at new NodeError (node:internal/errors:406:5)
    at importModuleDynamicallyCallback (node:internal/modules/esm/utils:144:9)
    at Object.constraints (/Users/myusername/path-to/my-project/yarn.config.cjs:4:38)
    at CC.process (/Users/myusername/.cache/node/corepack/yarn/4.0.1/yarn.js:524:73339)
    at async p0.execute (/Users/myusername/.cache/node/corepack/yarn/4.0.1/yarn.js:526:1996)
    at async p0.validateAndExecute (/Users/myusername/.cache/node/corepack/yarn/4.0.1/yarn.js:94:787)
    at async as.run (/Users/myusername/.cache/node/corepack/yarn/4.0.1/yarn.js:98:3250)
    at async iPt (/Users/myusername/.cache/node/corepack/yarn/4.0.1/yarn.js:734:11269)
    at async sk (/Users/myusername/.cache/node/corepack/yarn/4.0.1/yarn.js:734:11625)
```

## 4. Reproduce the workaround

Setting `DISABLE_V8_COMPILE_CACHE=1` resolves the issue and the script runs successfully:

```bash
â¯ DISABLE_V8_COMPILE_CACHE=1 yarn constraints
â””â”€ myproject@workspace:.
   â””â”€ The script ran successfully, at least

```

## Appendix: Potential fix that does not work

Given the error about the unresolved callback, I tried wrapping the import safely. This makes no difference - it produces the same error:

```js
const { defineConfig } = require(""@yarnpkg/types"");

function loadIsExactVersion() {
  return new Promise((resolve, reject) => {
    import(""is-exact-version"")
      .then((module) => {
        resolve(module.isExactVersion);
      })
      .catch((error) => {
        reject(error);
      });
  });
}

module.exports = defineConfig({
  constraints: async (ctx) => {
    const thisIsAnEcmaScriptModule = await loadIsExactVersion();
    ctx.Yarn.dependencies()[0].error(""The script ran successfully, at least"");
    return;
  },
});

```

### Environment

```shell
â¯ yarn dlx -q envinfo --preset jest

  System:
    OS: macOS 13.5
    CPU: (8) x64 Intel(R) Core(TM) i5-1038NG7 CPU @ 2.00GHz
  Binaries:
    Node: 20.9.0 - /private/var/folders/np/djbv9lnn5wd62yrs60zxh_p40000gn/T/xfs-fae2114e/node
    Yarn: 4.0.1 - /private/var/folders/np/djbv9lnn5wd62yrs60zxh_p40000gn/T/xfs-fae2114e/yarn
    npm: 10.1.0 - ~/.nvm/versions/node/v20.9.0/bin/npm

```


### Additional context

As mentioned, I am using the `node-modules` linker. 

The only reason I noticed that `DISABLE_V8_COMPILE_CACHE=1` fixed the bug is because when I was switching back and forth between attempts to fix the script, I would sometimes encounter a different error, ""`invalid host defined options`."" Googling that [led me to `v8-compile-cache`](https://twitter.com/slicknet/status/1559619176230047744). There are some recent issues with similar errors in other projects, e.g. https://github.com/pyodide/pyodide/issues/4267 and https://github.com/prettier/prettier-vscode/issues/3114 (Then I asked ChatGPT if I could opt out of v8-compile-cache, and it told me I could set `DISABLE_V8_COMPILE_CACHE=1`, which sounded so convenient I thought it was hallucinating, but it worked!)"
138,679b04bccbe8284d7a7280de,issue,learning -> how to use a library/framework/tool -> generating code examples,https://github.com/express-validator/express-validator/issues/1265,"Able to use validation inside controller?
Hi there,

I was trying to use one of the function from _**CommonController**_ in other controller, but I found out that express validator only can work in **_routing_** level. So may i know is this possible to use it in controller which act like routing level to filter request body?
Below provide some samples which generate from ChatGPT but I tested all cannot filter the req.

_**Validation**_
```
const { body, validationResult } = require('express-validator')

const SendEmailValidator = [
    body('to_address')
        .notEmpty().withMessage('email_cannot_be_empty')
        .isLength({min:8, max: 40}).withMessage('email_length_incorrect')
        .isEmail().withMessage('email_format_invalid'),

    body('subject')
        .notEmpty().withMessage('subject_cannot_be_empty')
        .isLength({min:10, max: 200}).withMessage('subject_length_incorrect')
        .matches(/^[\w\d\s\,\.\_\-\/]+$/).withMessage('subject_format_invalid'),

    body('text')
        .notEmpty().withMessage('text_cannot_be_empty')
        .isLength({min:1, max: 355}).withMessage('text_length_incorrect')
        .matches(/^[\w\d\s\,\.\_\-\/]+$/).withMessage('text_format_invalid'),

    // (req, res, next) => {
    //     const errors = validationResult(req);
    //     if (!errors.isEmpty()) {
    //         return res.status(422).json({ status: false, data: """", message: errors.array(), code: 422 });
    //     }
    //     next();
    // },
]

module.exports = {
    SendEmailValidator
}
```

**_CommonController_**
      
       const sendEmail = [SendEmailValidator, async (req, res) => {
                  
          const errors = validationResult(req)
          console.log(errors)
          if(!errors.isEmpty()){
              return { status: false, data: '', message: errors.array(), code: 422 }
          }
      
          console.log(req.body)
      
          const { to_address, subject, text } = req.body  
      
          const input = {
              ""from"": process.env.EMAIL_USER,
              ""to"": to_address,
              ""subject"": subject,
              ""text"": text
          }
      
          const transporter = nodemailer.createTransport(config)
          await transporter.sendMail(input)
      
          return { status: true, data: '', message: ""success"", code: 200 }
      }]
  
      module.exports = {
          sendEmail
      }

**_Other Controller Usage_**
```
        const result = await sendEmail[1]({  }, res)
        console.log(result)
        if(!result.status){
            return res.status(result.code).json(result);
        }
```

"
139,679b0549cbe8284d7a728478,issue,feature implementation/enhancement -> prototyping,https://github.com/Kerollmops/blog/issues/4,"Multithreading and Memory-Mapping: Refining ANN Performance with Arroy
<!-- Dive into my journey of porting Spotify's Annoy library to Rust using LMDB. Learn how I tackled memory-mapped file challenges, optimized tree node generation, and achieved significant performance improvements for indexing large vector datasets. Discover the power of the Share Nothing principle and prepare for future insights on incremental indexing and filtering. -->

> [!NOTE]  
> This is one blog post in a series:
>  - [Spotify-Inspired: Elevating Meilisearch with Hybrid Search and Rust](https://blog.kerollmops.com/spotify-inspired-elevating-meilisearch-with-hybrid-search-and-rust),
>  - Multithreading and Memory-Mapping: Refining ANN Performance with Arroy,
>  - [Meilisearch Expands Search Power with Arroy's Filtered Disk ANN](https://blog.kerollmops.com/meilisearch-expands-search-power-with-arroy-s-filtered-disk-ann),
>  - [How Meilisearch Updates a Millions Vector Embeddings Database in Under a Minute](https://blog.kerollmops.com/how-meilisearch-updates-a-millions-vector-embeddings-database-in-under-a-minute),
>  - [Meilisearch Indexes Embeddings 7x Faster with Binary Quantization](https://blog.kerollmops.com/meilisearch-indexes-embeddings-7x-faster-with-binary-quantization).

Wouldn't it be great to show you how a single-threaded, memory-mapped key-value store can be more efficient than a hand-written memory-mapped solution? I faced issues while porting the Spotify disk-based Approximate Nearest Neighbors library to Rust and, more specifically, to LMDB. Those issues were primarily due to LMDB and memory safety. Here is the story.

To remind you, [Arroy is a library](https://github.com/meilisearch/arroy) that stores embeddings (vectors of floats) on disk. Some data structures are generated on top of those vectors, which look like trees governed by normals used to recursively split the vectors dataset into subdomains. But you can read more about this in part 1 of this series.

### How does Annoy Generate the Tree Nodes?

Annoy, the Spotify library stores the nodes on disk, the item nodes (the ones containing the embeddings), and the other nodes that we will call tree nodes in this article. The advantage of doing this is double:
 - The program's memory usage is low and optimized by the OS as all the nodes live in a memory-mapped file.
 - The concept is simple: Access a node by using its ID. The library will find its offset on disk by using a simple multiplication.

However, there are downsides to doing that. All nodes: items with embeddings along with the tree nodes must have the same size, and If a user wants to identify its embedding using the ID 2<sup>31</sup>, the library will increase the file size to the ID multiplied by the node size. For example, with a vector of 768 dimensions, storing a single item with the ID 2<sup>31</sup> will generate a file of more than 6.5 TiB.

In generating the final data structure, the tree nodes are all written to disk in the same file as the user item containing the embeddings. Those tree-building processes run in parallel, one running process by tree, and, therefore, requires a lot of synchronization when defining the newly created node ID and the memory area reserved for it, most importantly, when the memory-mapped file is too small to accept more nodes, only a single thread must grow the file, so a mutex is used on top of the mutable memory-map and around a `node_id` variable. One interesting property of tree generation is that the generation process only requires the original user item with embeddings.

### The Challenges We Encountered when Porting it to LMDB

A fun fact is that it is the first time in a long time that I have written C++, and the first time I asked ChatGPT to code for me because I was not confident in doing C++ and feared falling into some segfault. I needed a small program to deserialize embeddings from stdin and give them to Annoy. The chatbot's code was mostly effective, but it omitted a critical [empty vector check](https://gist.github.com/Kerollmops/d0e434016b1a698ec9bdbd86d268885e#file-par_build_tree-cpp-L74), which led to a segmentation fault...

The main obstacle to porting it to LMDB is that writing into this key-value store is single-threaded. It doesn't support concurrent write operations to maintain a correctly balanced BTree. Fun incoming!

#### Reading the User Item Nodes from Different Threads

We have used LMDB at Meilisearch since the beginning. It is a well-maintained key-value store used in Firefox and maintained by the OpenLDAP developers. It is memory-mapped and does not maintain a user-end cache of the entries in memory but instead gives you a pointer to the memory-mapped area on disk. The main advantage is that you can keep a pointer to this area for as long as your read transaction lives. Ho yeah! Because it is a transactional key-value store that supports atomic committing, too!

But tree generation doesn't require referring to the generated nodes but only the user items. We previously saw that LMDB gives direct pointers into the memory-mapped file without maintaining any intermediate cache (that could be invalidated). There is another quirk with LMDB: you cannot share a read-only transaction between multiple threads, i.e., `RoTxn: !Sync` and you cannot move the write transaction between threads, i.e., `RwTxn: !Send + !Sync`. Ho! And there is no way to create a read-transaction on uncommitted changes. This is an issue because we want to generate the data-structures trees in the same transaction where we store the items.

But magic is everywhere, starting with the following small and fun data structure. The principle is to keep the pointers to the internal user items with embeddings in a `Vec<*const u8>`. Thanks to Rust, we can ensure, at compile time, that the pointers will live long enough by keeping the lifetime in the struct. Using the `&'t RwTxn` to get the `&'t RoTxn` by using `Deref` also ensures that we cannot modify the database while reading in it by using a `&'t mut RwTxn`. [According to the leading developer of LMDB](https://bugs.openldap.org/show_bug.cgi?id=10138#c9), it is safe to share those pointers between threads and why I implemented `Sync` for this structure.

```rust
pub struct ImmutableItems<'t, D> {
    item_ids: RoaringBitmap,
    constant_length: Option<usize>,
    offsets: Vec<*const u8>,
    _marker: marker::PhantomData<(&'t (), D)>,
}

impl<'t, D: Distance> ImmutableItems<'t, D> {
    pub fn new(rtxn: &'t RoTxn, database: Database<D>, index: u16) -> heed::Result<Self> {
        let mut item_ids = RoaringBitmap::new();
        let mut constant_length = None;
        let mut offsets = Vec::new();

        for result in database.items()? {
            let (item_id, bytes) = result?;
            assert_eq!(*constant_length.get_or_insert(bytes.len()), bytes.len());
            item_ids.push(item_id);
            offsets.push(bytes.as_ptr());
        }

        Ok(ImmutableItems { item_ids, constant_length, offsets, _marker: marker::PhantomData })
    }

    pub fn get(&self, item_id: ItemId) -> heed::Result<Option<Leaf<'t, D>>> {
        let len = match self.constant_length {
            Some(len) => len,
            None => return Ok(None),
        };
        let ptr = match self.item_ids.rank(item_id).checked_sub(1).and_then(|offset| self.offsets.get(offset)) {
            Some(ptr) => *ptr,
            None => return Ok(None),
        };
        let bytes = unsafe { slice::from_raw_parts(ptr, len) };
        ItemCodec::bytes_decode(bytes).map(Some)
    }
}

unsafe impl<D> Sync for ImmutableItems<'_, D> {}
```

You have also probably noticed some other fun optimizations in this simplified version of the data structure. We also know that the user-item nodes have a constant length, so I decided to store it only once, reducing the offsets vector's size by two. Considering that our objective is to store 100M vectors and that this vector is in memory, we shrunk its size from 1526 MiB to 763MiB, which is not much, but better than nothing.

#### Writing the Tree Nodes in Parallel

Ok! Now that we know how to store pointers to items and share them between threads without any user-end synchronization, we need to generate the tree nodes using it. We already know how to deal with LMDB at Meilisearch and decided to implement the same workaround. To write in parallel into LMDB, write into different, independent files and merge everything afterward. This leverages the [Share-Nothing principle](https://en.wikipedia.org/wiki/Shared-nothing_architecture) and isolates the algorithms. This drastically reduces the number of synchronization points [compared to the original C++ code](https://github.com/spotify/annoy/blob/2be37c9e015544be2cf60c431f0cccc076151a2d/src/annoylib.h#L1344-L1445) (look at the `.lock/unlock` calls) to a single line in our code: the atomically increasing global tree node ID.

```rust
pub fn next_node_id(&self) -> u64 {
    self.0.fetch_add(1, Ordering::Relaxed)
}
```

Our functions that generate normal split nodes based on the user-item nodes are now simply writing the nodes into independent files. The [nodes are appended into a file](https://github.com/meilisearch/arroy/blob/4f193fd534acd357b65bfe9eec4b3fed8ece2007/src/parallel.rs#L20-L71), and the on-disk offsets and bounds are stored into a vector, a single `usize` by node. Using Rayon, we run all tree-generation functions in parallel and, once completed, retrieve the files and boundaries to write the uniquely identified nodes into LMDB sequentially.

### Performances Comparison

Our objective at Meilisearch is to support 100M embedding of around 768 dimensions. If we store those as `f32` vectors without any [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction), it would be equivalent to `100M * 768 * 4 = 307B`, in other words, 286 GiB to store the vectors raw, without any internal tree nodes, i.e., no way to search in them efficiently.

If you don't specify the number of trees to generate, the algorithm will continue to create trees while the number of tree nodes is smaller than the number of item vectors. At the end, there must be roughly the same number of tree nodes and items.

#### Discovering the Limit of Vectors We Can Index

Arroy and Annoy use memory mapping extensively but in slightly different ways. In a previous article from @dureuill, we saw that [operating systems do not have infinite memory-map space](https://blog.meilisearch.com/dynamic-virtual-address-management/). So, let's dive into performance results.

I noticed something strange when running Arroy with 25M vectors of 768 dimensions. The CPUs were not used, and the program seemed to read the SSD/NVMe a lot, too much ðŸ¤” The tree generation algorithm is splitting the vector space into subdomains with fewer vectors. Still, it must first find the appropriate normal to split the entire domain, and for that, it randomly selects many items. Unfortunately, my machine only has 63 GiB of memory, and indexing those 25M items requires more than 72 Gib. Annoy was also struggling in the same way.

![We cannot index 25M vectors of 768 dimensions with 63 GiB](https://i.imgur.com/aKCovsM.png)

After investigating, I understood why the whole swap space and memory mapping limits were reached. The item nodes were not only _768 * 4 bytes_ because we store the norm and other stuff alongside the vectors, but in the case of Arroy, LMDB needs to maintain BTree data structures around the entries, and those are also tacking memory-mapped space. Both programs request random item nodes unavailable in memory, so the OS fetches them from the disk, which takes time. Ho and every single thread is doing that in parallel. CPUs are simply awaiting the disk.

So, after some dichotomy, I found the point where arroy successfully used all of the memory without being bound to the disk speed. It can index 15.625M on a 63 GiB machine. You can see on this htop screenshot that the disk read speed is at zero as all of the vectors fit in RAM, that arroy is writing the tree nodes to disk, and that the CPUs are doing their best. It took less than seven hours to process.

![Successfully indexing 15.6M vector of 768 dimensions with 63 GiB](https://i.imgur.com/N4ksrIq.png)

But... Annoy cannot index this same number of documents. It suffers from the same issue we saw before: high disk read and low CPU usage. But why? I needed clarification because the nodes have the same format, the number of item vectors to index is the same, and the algorithm has been ported. So, what is the difference between both solutions?

For those who looked into the C++ codebase and were probably hinted by the memory mapping issue described earlier, you probably noticed this slight difference: Arroy is writing the generated tree nodes into different raw files when Annoy, on the contrary, is [reserving space into the memory-mapped file and directly writing into it](https://github.com/spotify/annoy/blob/2be37c9e015544be2cf60c431f0cccc076151a2d/src/annoylib.h#L1354-L1370). By doing this trick, the OS needs to keep much more space in the memory-mapped area and is forced to invalidate the item nodes from the cache to keep the just-written tree nodes hot in the cache, slowing down the system for no reason, as the tree nodes are not necessary for the algorithm.

So, after even more dichotomy to find the Annoy limits on a 63 GiB machine, I discovered that it could roughly index 10M vectors in five hours.

#### The Share Nothing Principle to the Win

So Arroy can index 36% more vectors on the same machine, but how long does it take? Wouldn't it be faster to write into the same file in parallel instead of copying all those tree nodes in a single-threaded way? It will be a much shorter paragraph as I only did some small tests, but Arroy is always faster!

|       | number of vectors | number of threads | building time |
|-------|-------------------|-------------------|---------------|
| Annoy | 96k               | 1                 | 5min 38s      |
| arroy | 96k               | 1                 | **3min 45s**  |
| Annoy | 96k               | 12                | 1min 9s       |
| arroy | 96k               | 12                | **54s**       |
| Annoy | 10M               | 12                | 5h 40min      |
| arroy | 10M               | 12                | **4h 17min**  |
| Annoy | 15.625M           | 12                | --            |
| arroy | 15.625M           | 12                | **7h 22min**  |

Now, you probably tell me that I will need around 400GiB of memory to index 100M vectors, and you are probably right, but @irevoire will talk about incremental indexing in a future article. I did a linear regression for fun. With those 12 threads and the required memory, I predicted it would take one day, 22 hours, and 43 min ðŸ˜¬. Ho! As we also use this vector store in Meilisearch, we need to provide ways to filter this data structure at search time. This is the next article in this series.

You can comment about this article on [Lobste.rs](https://lobste.rs/s/87aheu/multithreading_memory_mapping_refining), [Hacker News](https://news.ycombinator.com/), [the Rust Subreddit](https://old.reddit.com/r/rust/comments/18jmuoe/multithreading_and_memorymapping_refining_ann), or [X (formerly Twitter)](https://x.com/Kerollmops/status/1735941213516140814)."
140,679b05cdcbe8284d7a728739,issue,feature implementation/enhancement -> prototyping -> implementing proof of concept,https://github.com/angular/angular/issues/53519,"Be able to use Observable and toSignal inside a computed call
### Which @angular/* package(s) are relevant/related to the feature request?

core

### Description

Using the `toSignal` function inside a `computed` function lead to the following error: `Error: NG0602: toSignal() cannot be called from within a reactive context. Invoking `toSignal` causes new subscriptions every time. Consider moving `toSignal` outside of the reactive context and read the signal value where needed. Find more at https://angular.io/errors/NG0602`.

I understand the risk of having multiple subscritpions generated by the change of a signal value into the `computed` function.
But on the other hand it make sense to have a signal to store data that is build from a computed calls which contains the server call like this: 
```
product = computed(() =>
   toSignal(this.productService.getProduct(this.productId() /* <- a signal */), {
      initialValue: null,
  })()
);
```

Here `this.productId` can be a signal comming from an `input` or a route param.

At the moment, we would have to write something like this:
```
product = toSignal(
    toObservable(this.productId)
    .pipe(
      switchMap(productId => this.productService.getProduct(productId))
    )
  , { initialValue: null })
```

Would it be possible to have Angular compiling the first showed code into the second one ?

I've created a [stackblitz](https://stackblitz.com/edit/stackblitz-starters-nqf5mn?description=An%20angular-cli%20project%20based%20on%20@angular/animations,%20@angular/common,%20@angular/compiler,%20@angular/core,%20@angular/forms,%20@angular/platform-browser,%20@angular/platform-browser-dynamic,%20@angular/router,%20core-js,%20rxjs,%20tslib%20and%20zone.js&file=src%2Fmain.ts,package.json&title=Angular%20Starter) as a playground.

### Proposed solution

I'm definetly not an expert with tools like babel or so...
So I asked chatGPT few questions and he suggested me a solution which can be a good start for a line of thought (even if the proposed solution is not generic).

```
const babel = require('@babel/core');

// Original code as a string
const originalCode = `
  computed(() => toSignal(this.productService.getProduct(this._productId()), { initialValue: null })())
`;

// Modify code using Babel
const modifiedCode = babel.transformSync(originalCode, {
  plugins: [
    // Use a custom Babel plugin to modify the AST
    function ({ types }) {
      return {
        visitor: {
          // Visit ArrowFunctionExpression nodes
          ArrowFunctionExpression(path) {
            // Check if the arrow function is part of the computed function
            if (
              path.parent.type === 'CallExpression' &&
              path.parent.callee.name === 'computed'
            ) {
              // Replace the arrow function with a new one
              path.replaceWith(
                types.arrowFunctionExpression(
                  path.node.params,
                  // Create a new AST for the desired modified code
                  types.callExpression(
                    types.identifier('toSignal'),
                    [
                      types.callExpression(
                        types.identifier('toObservable'),
                        [path.node.body.arguments[0].object.property]
                      ),
                      types.objectExpression([
                        types.objectProperty(
                          types.identifier('initialValue'),
                          types.nullLiteral()
                        ),
                      ]),
                    ]
                  )
                )
              );
            }
          },
        },
      };
    },
  ],
}).code;

// Log the modified code
console.log(modifiedCode);

```

### Alternatives considered

I know that in the vue ecosystem, there is the [vueuse](https://vueuse.org/) which exposes a [computedAsync](https://vueuse.org/core/computedAsync/#computedasync) function which broadly does what we want."
141,679b0656cbe8284d7a7288ff,issue,software quality -> fixing -> supporting debugging,https://github.com/localsend/localsend/issues/1045,"[BUG] Permission denied to send files from Windows to an external storage in Android 10 or older
### Describe the bug

I can't send my any sort of files either png,mp4,mp3,etc from PC to android using an external storage, however it can send files to the internal storage. I'm encountering a persistent error [500] on my windows, while my phone reports a PathAccessException: Creation failed, path = ""SD Card directory"" (OS Error: Permission denied, errno = 13). I searched up the issues #710 and it seems that correlates with this one.

I tried enabling android.permission.MANAGE_EXTERNAL_STORAGE for LocalSend in an app called App Ops but I couldn't find it and used Powershell to type this: .\adb shell appops set --uid org.localsend.localsend_app MANAGE_EXTERNAL_STORAGE allow. And also tried this as well:  

.\adb shell appops set --uid org.localsend.localsend_app MANAGE_EXTERNAL_STORAGE allow
.\adb shell ""appops set --uid 'org.localsend.localsend_app' MANAGE_EXTERNAL_STORAGE allow""
.\adb shell pm grant org.localsend.localsend_app android.permission.MANAGE_EXTERNAL_STORAGE

I'm not a programmer by the way and used ChatGPT 3.5 to find out the problem and it said that ""It appears that the prompt either appops, pm grant and android.permission.MANAGE_EXTERNAL_STORAGE permission are not recognized by your device or Android version"". 

Android 8 only includes READ_EXTERNAL_STORAGE and WRITE_EXTERNAL_STORAGE in which are have been allowed access in both LocalSend app info and in App Ops standalone application. 

### To reproduce

1. Go to '...'
2. Click on '...'
3. Scroll down to '...'
4. See error

![localsend_app-23-12-16(15-47-29-369)](https://github.com/localsend/localsend/assets/153980173/03c98fb2-9e68-4c34-85e7-32c4232b9d15)


### Expected behavior

Sending in files through an external storage with an Android 8.

### Screenshots

![Localsend error](https://github.com/localsend/localsend/assets/153980173/ae5c9453-493a-4581-8384-309f3b8c0535)


### Desktop (please complete the following information)

Windows 10 Pro - version 10.0.19044 Build 19044
LocalSend version 1.12.0 (38)

### Smartphone (please complete the following information)

Asus Zenfone 5 ZE620KL
Android 8 (unfornately, I can't update it higher than that)

### Additional context

The localsend HTTPS logs can only be produce in windows in which the android can't."
142,679b06d1cbe8284d7a728b5e,issue,software quality -> fixing -> functional bugs,https://github.com/DiegoFleitas/letterboxd-movie-justwatch/issues/29,"Exporting from justwatch
I have a JustWatch watchlist that I want to export to Simkl. I am trying the method [of this repo.](https://github.com/ijanos/justwatchdumper) from ijanos.
@DiegoFleitas  you have already helped me before. Now I am stuck with the ""xargs"" step. I am getting following error,

![image](https://github.com/DiegoFleitas/letterboxd-movie-justwatch/assets/101859073/a36184de-0562-49a9-9592-00f976ee5f90)


I am using Windows 10 to do this. I have already extracted the URLs of my watchlist in a text file. Now what I am trying to do is 

> feed the text file to the download script with xargs.

How do I solve this error?

So far I have tried `Get-Content urls.txt | ForEach-Object { .\dl.sh $_ }` in PoweShell with the help of chatGPT but it is not working. It is opening multiple tabs in the firefox like this (screenshot below)

![image](https://github.com/DiegoFleitas/letterboxd-movie-justwatch/assets/101859073/6f8ca1ab-6b32-45e7-bf38-daae36264802)
"
143,679b0743cbe8284d7a728c9c,issue,process -> automating commit/issue/pr creation -> confirming that a suggest change can be implemented,https://github.com/MobiFlight/MobiFlight-Connector/issues/1533,"Remember grid view column widths across app launches
**Is your feature request related to a problem? Please describe.**
It's possible to resize the column widths on the Outputs and Inputs tab, but those widths aren't persisted across app launch. When you get them sized the way you like you'll lose it when you quit Mobiflight then re-launch.

**Describe the solution you'd like**
Remember the column widths across app launch.

**Additional context**
[Requested by Glenn in Discord](https://discord.com/channels/608690978081210392/745361245808492584/1189360726964437062)

I asked ChatGPT and this is totally possible to do without too much effort."
144,679b07d8cbe8284d7a728fb3,issue,learning -> how to use a library/framework/tool,https://github.com/Patrick-Hogan/wandering_inn/issues/26,"SSL Verification Issue
Running this command:
`C:\...\wandering_inn>python wanderinginn2epub.py --volume 9 --output-by-volume`

I get the following output:

```
Traceback (most recent call last):
  File ""C:\Users\steph\wandering_inn\wanderinginn2epub.py"", line 382, in <module>
    main()
  File ""C:\Users\steph\wandering_inn\wanderinginn2epub.py"", line 316, in main
    full_index = get_index()
  File ""C:\Users\steph\wandering_inn\wanderinginn2epub.py"", line 216, in get_index
    page = urlopen(toc_url)
  File ""C:\Users\steph\AppData\Local\Programs\Python\Python39\lib\urllib\request.py"", line 214, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Users\steph\AppData\Local\Programs\Python\Python39\lib\urllib\request.py"", line 517, in open
    response = self._open(req, data)
  File ""C:\Users\steph\AppData\Local\Programs\Python\Python39\lib\urllib\request.py"", line 534, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
  File ""C:\Users\steph\AppData\Local\Programs\Python\Python39\lib\urllib\request.py"", line 494, in _call_chain
    result = func(*args)
  File ""C:\Users\steph\AppData\Local\Programs\Python\Python39\lib\urllib\request.py"", line 1385, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
  File ""C:\Users\steph\AppData\Local\Programs\Python\Python39\lib\urllib\request.py"", line 1345, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1122)>Traceback (most recent call last):
  File ""C:\Users\steph\wandering_inn\wanderinginn2epub.py"", line 382, in <module>
    main()
  File ""C:\Users\steph\wandering_inn\wanderinginn2epub.py"", line 316, in main
    full_index = get_index()
  File ""C:\Users\steph\wandering_inn\wanderinginn2epub.py"", line 216, in get_index
    page = urlopen(toc_url)
  File ""C:\Users\steph\AppData\Local\Programs\Python\Python39\lib\urllib\request.py"", line 214, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Users\steph\AppData\Local\Programs\Python\Python39\lib\urllib\request.py"", line 517, in open
    response = self._open(req, data)
  File ""C:\Users\steph\AppData\Local\Programs\Python\Python39\lib\urllib\request.py"", line 534, in _open
    result = self._call_chain(self.handle_open, protocol, protocol +
  File ""C:\Users\steph\AppData\Local\Programs\Python\Python39\lib\urllib\request.py"", line 494, in _call_chain
    result = func(*args)
  File ""C:\Users\steph\AppData\Local\Programs\Python\Python39\lib\urllib\request.py"", line 1385, in https_open
    return self.do_open(http.client.HTTPSConnection, req,
  File ""C:\Users\steph\AppData\Local\Programs\Python\Python39\lib\urllib\request.py"", line 1345, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1122)>
````
I'm pretty new to using github and have been using ChatGPT to get me started so I could definitely (most likely) be missing something obvious."
145,679b12e1cbe8284d7a7295e6,issue,software quality -> fixing -> functional bugs,https://github.com/einsteinpy/einsteinpy/issues/655,"The animation function give animation reversed, the update function is not correct (I think)
<!--

WELCOME ABOARD!

-->The animate function in StaticGeodesicPlotter gives animations reversed

ðŸ“‹  **Steps to solve the problem**

I am not programming savvy at all I am doing a presentation on the EinsteinPy package I tried to create some animations and they gave a gif that is reversed, so I looked at the source code and with the help of ChatGPT I found that the update function is the problem. 
Location: einsteinpy\plotting\geodesic\static.py
![image](https://github.com/einsteinpy/einsteinpy/assets/121961749/a0aecba7-5546-4775-9fcd-913c11f6b0d0)
The old update function: 

        def _update(num, data, line):
            line.set_data(data[:2, :num])
            line.set_3d_properties(data[2, :num])

            return (line,)
The new update function that works (for me):

        def _update(num, data, line):
            current_frame = num % N  # Ensure animation plays in the correct order
            reversed_frame = N - current_frame - 1  # Reverse the frame order
            line.set_data(data[:2, :reversed_frame + 1])
            line.set_3d_properties(data[2, :reversed_frame + 1])

            return (line,)

"
146,679b1365cbe8284d7a7296c0,issue,process -> automating commit/issue/PR creation -> creating commit/issue/PR description,https://github.com/cfig/Android_boot_image_editor/issues/136," Issue with 'boot_editor_v14r2' Gradle Wrapper Script on macOS
""I am not a native English speaker, so I used ChatGPT to write down this issue. Please forgive any errors.""

Problem Statement:
When attempting to use #!/usr/bin/bash in the gradlew script of 'boot_editor_v14r2', it is observed that the Bash executable is not located in the specified directory on the latest macOS version.

Context:
The 'boot_editor_v14r2' project's Gradle Wrapper script (gradlew) contains the shebang line #!/usr/bin/bash, which points to the Bash executable in a specific directory. However, on newer versions of macOS, it seems that Bash is not present in the expected directory.

Steps to Reproduce:
Navigate to the 'boot_editor_v14r2' project directory.
Attempt to execute the gradlew script.
Expected Behavior:
The gradlew script should execute without any issues, utilizing the Bash executable specified in the shebang line.

Actual Behavior:
Execution of the gradlew script results in an error, indicating that the Bash executable is not found in /usr/bin.

Environment Details:
Operating System: macOS 14.3
'Android_boot_image_editor' version: v14r2
Proposed Solution:
Update the shebang line in the gradlew script to correctly point to the Bash executable's location on the latest macOS version.
Provide instructions for users on how to resolve the issue or update their local setup."
147,679b147ecbe8284d7a729b36,issue,feature implementation/enhancement -> migrating/reusing via programming language translation,https://github.com/langchain4j/langchain4j/issues/574,"MarkdownHeaderTextSplitter
langchain4j already offers several DocumentSplitters. One that is currently missing is MarkdownHeaderTextSplitter.

The original (python based) langchain project has such a [MarkdownHeaderTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/markdown_header_metadata)

Would be nice to have this as well in langchain4j.

Attached an implementation for this. I used ChatGPT to translate the Python code to java (incl. the Unit Test). It might not conform to all the coding standards of the project, but it does the job and tests pass.

[MarkdownHeaderTextSplitter.zip](https://github.com/langchain4j/langchain4j/files/14108652/MarkdownHeaderTextSplitter.zip)
"
148,679b147ecbe8284d7a729b52,issue,learning -> program comprehension,https://github.com/exercism/problem-specifications/issues/2376,"pig latin in C track
i have noticed inconsistency between rule 3
`
Rule 3: If a word starts with a consonant sound followed by ""qu"", move it to the end of the word, and then add an ""ay"" sound to the end of the word (e.g. ""square"" -> ""aresquay"").
`

and test
```
static void test_word_beginning_with_qu(void)
{
   TEST_IGNORE();
   check_transate(""queen"", ""eenquay"");
}
``` 
i a not native english speaker, but if i follow the rule, i would not expect it describe also state when ""qu"" is in the beginning of the word as there is said ""consonant sound followed by ""qu"" ""
but if i asked chatgpt it also understood it same as me and skipped state for rords beginning with ""qu""



"
149,679b147ecbe8284d7a729c6a,issue,documentation -> improving writing,https://github.com/citizenos/citizenos-fe/issues/743,"Group: Is 250 characters enough for group description
@kevincrepin @anettlinno @ilmartyrk Just wondering if 250 characters is enough for the group description? 

I asked ChatGPT to write one for a dummy group. I got to this which felt quite a good length (not overly long), but the last two sentences wouldn't fit: 

_Welcome to Yourtown 2030 Visionaries! This online group is where Yourtown residents gather to discuss ideas for our community's future in 2030. Shape the local government's strategy by sharing your thoughts, dreams, and aspirations. Join us in crafting a collective vision for a brighter, more inclusive Yourtown. Your input matters â€“ be a part of the change!_ 

Is there are reason to keep it so short? :) "
150,679b147ecbe8284d7a729c74,issue,feature implementation/enhancement -> implementing a new feature,https://github.com/sabrogden/Ditto/issues/619,"Paste text without empty lines
Hi members. I have an issue. Mostly I work with copied text from different sources and I need to delete empty lines between paragraphs for next paste. I asked Chatgpt to write a code in ChaiScript ""onCopy keyword"". But it didn't work: error: 
""Unparsed input"" during evaluation  at (1, 7)"". 
Script it self: 
onCopy: {
    auto text = getCopiedText();
    removeEmptyLines(text);
    setClipboardText(text);
}
removeEmptyLines(std::string& text) {
    auto lines = split(text, '\n');
    auto filteredLines = filter(lines, [](const std::stri...i...

Help me to solve the problems kind members!!!"
151,679b14f0cbe8284d7a729e3b,issue,learning -> how to use a library/framework/tool,https://github.com/flutter/flutter/issues/143071,"Flutter pub get issues.
### Steps to reproduce

A solution to the issues I'm running into.

### Actual results

I can't implement the changes made on my pubspec.yaml, It's throwing me error messages even after running all the command line suggested and also watched videos, used chatGPT as well but to no avail.

### Logs

MacBook-Pro myapp % flutter pub get
Resolving dependencies... 
  matcher 0.12.16 (0.12.16+1 available)
  material_color_utilities 0.5.0 (0.8.0 available)
  meta 1.10.0 (1.12.0 available)
  path 1.8.3 (1.9.0 available)
  test_api 0.6.1 (0.7.0 available)
  web 0.3.0 (0.4.2 available)
Got dependencies!
6 packages have newer versions incompatible with dependency constraints.
Try `flutter pub outdated` for more information.

### Flutter Doctor output

MacBook-Pro myapp % flutter doctor
Doctor summary (to see all details, run flutter doctor -v):
[âœ“] Flutter (Channel stable, 3.16.9, on macOS 14.3 23D56 darwin-x64, locale en-GB)
[âœ“] Android toolchain - develop for Android devices (Android SDK version 34.0.0)
[!] Xcode - develop for iOS and macOS (Xcode 15.2)
    âœ— Unable to get list of installed Simulator runtimes.
[âœ“] Chrome - develop for the web
[âœ“] Android Studio (version 2023.1)
[âœ“] VS Code (version 1.86.0)
[âœ“] Connected device (2 available)
[âœ“] Network resources

! Doctor found issues in 1 category."
152,679b1571cbe8284d7a729fdb,issue,learning -> how to use a library/framework/tool,https://github.com/leomariga/pyRANSAC-3D/issues/40,"Cuboid with just 3 planes?
Hi there,

I just started using this library, and I am also new to computational geometry so please bear with me..

I am trying to fit a cuboid to some points, but I just realized that the `Cuboid.fit()` result is just returning 3 planes. I expected it to return 6 planes, as I don't see how 3 planes can fully parametrize a cuboid object.

I felt pretty dumb and was sure I was missing something, so I asked ChatGPT which seems to agree that we need 6 planes and not 3.. now I am even more confused.

What am I missing?"
153,679b1571cbe8284d7a72a192,issue,feature implementation/enhancement -> implementing a new feature,https://github.com/PCAssistSoftware/RoboSharp/issues/192,"RoboCopy Benchmarking
I'm currently writing a custom IRoboCommand to orchestrate building a complex file batch that pulls from various folders and singular files to a final destination. As such, I've written an interface that has a method for CopyToAsync() to copy a file from source to destination asynchronously. I'm creating this thread to document some benchmarks, as RoboCopy is great at what it does.

I used ChatGPT and the [microsoft documentation](https://learn.microsoft.com/en-us/dotnet/standard/io/asynchronous-file-i-o) to create some async methods to handle the copying that reports via an `IProgress<long>` object. As well as a project developed to use [CopyFileEx ](https://learn.microsoft.com/en-us/windows/win32/api/winbase/nf-winbase-copyfileexa)to perform the copy operation.

The benchmark for these tests was run in a console app in release mode, using a 334MB file. Each copy operation was performed 10x and the time report was the average result. My goal was to determine if a comparable time-to-copy could be achieve for a single file that falls within the current documentation recommendations I've been able to track down over a few days.

Note : This test was done on a console application targeting Net5.0 (as my work computer does not have VS2022 yet). Net6.0 benchmarks will happen in a followup comment.

CopyFileEx : https://github.com/RFBCodeWorks/CachedRoboCopy/blob/master/CachedRoboCopy/FileCopier.cs

```
static async Task NoProgressAsync(string source, string destination, int bufferSize = 81920)
{
    using (var reader = File.OpenRead(source))
    {
        using (var writer = File.OpenWrite(destination))
        {
            await reader.CopyToAsync(writer, bufferSize);
            writer.Dispose();
            reader.Dispose();
        }
    }
}

public static async Task ProgressAsync(string source, string destination, IProgress<long> progress, bool overwrite = false, CancellationToken token = default, int bufferSize = 81920)
{
    token.ThrowIfCancellationRequested();
    if (!File.Exists(source)) throw new FileNotFoundException(""File not found : "" + source);
    if (!overwrite && File.Exists(destination)) throw new IOException(""Destination file already exists"");
    if (progress is null) throw new ArgumentNullException(nameof(progress));
    int reportingInterval = 250;
    Stopwatch reportTimer = new Stopwatch();
    try
    {
        using (FileStream sourceStream = new FileStream(source, FileMode.Open, FileAccess.Read, FileShare.Read, bufferSize, true))
        using (FileStream destinationStream = new FileStream(destination, FileMode.Create, FileAccess.Write, FileShare.None, bufferSize, true))
        {
            byte[] buffer = new byte[bufferSize];
            long totalBytesRead = 0;
            int bytesRead;

            reportTimer.Start();
            while ((bytesRead = await sourceStream.ReadAsync(buffer, 0, buffer.Length, token).ConfigureAwait(false)) > 0)
            {
                token.ThrowIfCancellationRequested();
                await destinationStream.WriteAsync(buffer, 0, bytesRead, token).ConfigureAwait(false);
                totalBytesRead += bytesRead;
                if (reportTimer.ElapsedMilliseconds >= reportingInterval)
                {
                    progress?.Report(totalBytesRead);
                    reportTimer.Restart();
                }
            }
            reportTimer.Stop();
            progress?.Report(totalBytesRead);
        }
    }
    catch (OperationCanceledException)
    {
        // If the operation was canceled, delete the destination file
        if (File.Exists(destination))
        {
            File.Delete(destination);
            Console.WriteLine($""File copy operation canceled. Destination file '{destination}' deleted."");
        }

        throw; // Re-throw the OperationCanceledException after handling
    }
}
```

"
154,679b1607cbe8284d7a72a2b1,issue,feature implementation/enhancement -> design a new feature,https://github.com/PolicyEngine/policyengine-uk/issues/816,"Ensure imputed capital gains CDF is valid (monotonic)
[`impute_capital_gains`](https://github.com/PolicyEngine/policyengine-uk/blob/f89baee294f74990c26f024cdb32125610169b38/policyengine_uk/data/datasets/frs/imputations/capital_gains.py#L41) currently interpolates/extrapolates the provided quantiles to a CDF by fitting splines. This can result in CDFs that are not monotonically increasing and thus invalid.

After [asking ChatGPT for some ideas](https://chat.openai.com/share/eba9cb75-7dae-4e38-8ccb-55b5eb1ebd34), I think a promising approach could be first synthesizing a pdf from the quantiles, smoothing it with a kernel density estimator, then integrating it to a cdf. Here's an example of how that might look:

![image](https://github.com/PolicyEngine/policyengine-uk/assets/6076111/0589fa3e-758a-4b79-be93-0c525cd0981f)

Other options like isotonic regression or transformations could also work, and we may want something more complex if we want to consider all the data together rather than each income group independently.
"
155,679b2058cbe8284d7a72a4d5,issue,software quality -> fixing -> supporting debugging,https://github.com/l3uddz/plex_dupefinder/issues/76,"Error int(item.index) when it doesn't exists
**Describe the bug**
I ran Dupefinder on new TV Show libary and recived the following error.

 line 352, in <module>
    item.grandparentTitle, int(item.parentIndex), int(item.index), item.title)
                                                  ^^^^^^^^^^^^^^^
TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'


However I'm total pleb. I asked ChatGPT and it provided me with this

```
title = ""%s - %02dx%02d - %s"" % (
    item.grandparentTitle, int(item.parentIndex) if item.parentIndex is not None else 0, int(item.index) if item.index is not None else 0, item.title)
```

and it fixed my issue. and I decided to share it here"
156,679b2058cbe8284d7a72a4dd,issue,learning -> how to use a library/framework/tool,https://github.com/PurpleI2P/i2pd/issues/2025,"undefined reference to i2pd
Hello. While attempting to use the SAMv3 API (SAM.cpp), I ended up getting a ton of __undefined reference__ errors and I wanted to know what it is that I'm doing wrong.
I have successfully built i2pd using the `CMakeLists.txt` found in the `i2pd/build` folder. 
I've also linked to `libi2pd.a`, `libi2pdclient.a`, and `libi2pdlang.a` but the errors still persist. Any help would be appreciated!

errors:
```
...
BOB.cpp:(.text._Z8LogPrintIJRA13_KcRPS0_EEv8LogLevelDpOT_[_Z8LogPrintIJRA13_KcRPS0_EEv8LogLevelDpOT_]+0x34): undefined reference to `i2p::log::Logger()'
/usr/bin/ld: BOB.cpp:(.text._Z8LogPrintIJRA13_KcRPS0_EEv8LogLevelDpOT_[_Z8LogPrintIJRA13_KcRPS0_EEv8LogLevelDpOT_]+0x143): undefined reference to `i2p::log::Log::Append(std::shared_ptr<i2p::log::LogMsg>&)'
/usr/bin/ld: ../../../build/libi2pdclient.a(BOB.cpp.o): in function `void LogPrint<char const (&) [11]>(LogLevel, char const (&) [11])':
BOB.cpp:(.text._Z8LogPrintIJRA11_KcEEv8LogLevelDpOT_[_Z8LogPrintIJRA11_KcEEv8LogLevelDpOT_]+0x2d): undefined reference to `i2p::log::Logger()'
/usr/bin/ld: BOB.cpp:(.text._Z8LogPrintIJRA11_KcEEv8LogLevelDpOT_[_Z8LogPrintIJRA11_KcEEv8LogLevelDpOT_]+0x127): undefined reference to `i2p::log::Log::Append(std::shared_ptr<i2p::log::LogMsg>&)'
/usr/bin/ld: ../../../build/libi2pdclient.a(BOB.cpp.o): in function `void LogPrint<char const (&) [19], char const*&>(LogLevel, char const (&) [19], char const*&)':
BOB.cpp:(.text._Z8LogPrintIJRA19_KcRPS0_EEv8LogLevelDpOT_[_Z8LogPrintIJRA19_KcRPS0_EEv8LogLevelDpOT_]+0x34): undefined reference to `i2p::log::Logger()'
/usr/bin/ld: BOB.cpp:(.text._Z8LogPrintIJRA19_KcRPS0_EEv8LogLevelDpOT_[_Z8LogPrintIJRA19_KcRPS0_EEv8LogLevelDpOT_]+0x143): undefined reference to `i2p::log::Log::Append(std::shared_ptr<i2p::log::LogMsg>&)'
/usr/bin/ld: ../../../build/libi2pdclient.a(BOB.cpp.o): in function `void LogPrint<char const (&) [34], boost::asio::ip::basic_endpoint<boost::asio::ip::tcp> >(LogLevel, char const (&) [34], boost::asio::ip::basic_endpoint<boost::asio::ip::tcp>&&)':
BOB.cpp:(.text._Z8LogPrintIJRA34_KcN5boost4asio2ip14basic_endpointINS5_3tcpEEEEEv8LogLevelDpOT_[_Z8LogPrintIJRA34_KcN5boost4asio2ip14basic_endpointINS5_3tcpEEEEEv8LogLevelDpOT_]+0x34): undefined reference to `i2p::log::Logger()'
/usr/bin/ld: BOB.cpp:(.text._Z8LogPrintIJRA34_KcN5boost4asio2ip14basic_endpointINS5_3tcpEEEEEv8LogLevelDpOT_[_Z8LogPrintIJRA34_KcN5boost4asio2ip14basic_endpointINS5_3tcpEEEEEv8LogLevelDpOT_]+0x143): undefined reference to `i2p::log::Log::Append(std::shared_ptr<i2p::log::LogMsg>&)'
collect2: error: ld returned 1 exit status
```

i2p.cpp:
```cpp
// libi2pd
#include <api.h>
// libi2pd_client
#include <SAM.h>

// this code was generated by ChatGPT
void sam_test() {
    // Address and ports for SAM bridge
    std::string address = ""127.0.0.1"";
    int portTCP = 7656;
    int portUDP = 7655;

    // Create SAM bridge
    i2p::client::SAMBridge samBridge(address, portTCP, portUDP, false); // Set the last parameter to true for single-threaded operation

    // Start the SAM bridge
    samBridge.Start();

    // Create a SAM session
    std::string sessionID = ""mysession"";
    i2p::client::SAMSessionType sessionType = i2p::client::eSAMSessionTypeStream;
    std::string destination = """"; // empty string means transient
    std::map<std::string, std::string> params; // additional parameters if needed

    std::shared_ptr<i2p::client::SAMSession> session = samBridge.CreateSession(sessionID, sessionType, destination, &params);

    // Add the session to the bridge
    samBridge.AddSession(session);

    // Perform SAM-related operations
    // ...

    // Stop the SAM bridge
    samBridge.Stop();
}

int main(int argc, char* argv[]) {
    return 0;
}
```

g++:
`g++ i2p.cpp -o i2p -I../i2pd/daemon -I../i2pd/i18n -I../i2pd/libi2pd -I../i2pd/libi2pd_client -L../i2pd/build -li2pd -li2pdclient -li2pdlang -lboost_program_options -lboost_filesystem -lboost_system -lboost_date_time -lcrypto -lssl -lz -lpthread`"
157,679b20efcbe8284d7a72a894,issue,learning -> how to use a library/framework/tool,https://github.com/ashuramaruzxc/anime-cursors/issues/4,"Love the project, but a little instruction on how to run would be amazing (âŒ’_âŒ’;)
I know nothing of Nix, but even after I got it running with some help from ChatGPT (is there some mixup between clickgen and cursorgen in pkgs/default.nix??) I just get `Unmatched files` errors when running:

```
[ellie@koumakan anime-cursors] :) $ ./src/__main__.py -p src/win/ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«/ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³/ -o Rumia --name Rumia --json src/config/definitions.json -vvv --format ani -r
Unmatched files: ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ä½œæ¥­ä¸­.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€ä¸Šä¸‹ã«æ‹¡å¤§ãƒ»ç¸®å°.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€ãƒ†ã‚­ã‚¹ãƒˆé¸æŠž.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€äººã®é¸æŠž.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€ä»£æ›¿é¸æŠž.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€æ‰‹æ›¸ã.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€å¾…ã¡çŠ¶æ…‹.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€æ–œã‚ã«æ‹¡å¤§ãƒ»ç¸®å°2.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€å ´æ‰€ã®é¸æŠž.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€ç§»å‹•.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€ãƒ˜ãƒ«ãƒ—ã®é¸æŠž.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€å·¦å³ã«æ‹¡å¤§ãƒ»ç¸®å°.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€é€šå¸¸.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€åˆ©ç”¨ä¸å¯.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€é ˜åŸŸé¸æŠž.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€ãƒªãƒ³ã‚¯é¸æŠž.ani, ãƒ«ãƒ¼ãƒŸã‚¢ã€€ãƒžã‚¦ã‚¹ã‚«ãƒ¼ã‚½ãƒ«ã€€æ–œã‚ã«æ‹¡å¤§ãƒ»ç¸®å°1.ani
```

And I doubt that `./src/__main__.py` is how I'm supposed to run in the first place.

I love the project though! Especially the part where (I think?) you are also resizing the cursor images! That is awesome, and I don't think something anyone has done yet. 


---

Typed from my HDPI laptop running Wayland KDE with a tiny tiny Rumia cursor"
158,679b2186cbe8284d7a72ab29,issue,learning -> how to use a library/framework/tool,https://github.com/HFQR/xitca-web/issues/981,"how to get remote ip address
this from chatgpt doesnt work :D
```
async fn handler_h2(
    req: Request<RequestExt<h2::RequestBody>>,
) -> Result<Response<ResponseBody>, Box<dyn std::error::Error>> {
    // Extract the remote IP address from the request
    let remote_ip = req.connection_info().remote_addr().unwrap_or_else(|| ""Unknown"".to_string());

    let res = Response::builder()
        .status(200)
        .header(""Alt-Svc"", ""h3=\"":8081\""; ma=86400"")
        .header(CONTENT_TYPE, TEXT_UTF8)
        // Include the remote IP address in the response body
        .body(format!(""Hello from Http/2! Your IP is {}"", remote_ip).into())?;
    Ok(res)
}

```"
159,679b2186cbe8284d7a72ac8a,issue,feature implementation/enhancement -> prototyping -> implementing proof of concept,https://github.com/openai/openai-python/issues/1235,"Client not defined
### Confirm this is an issue with the Python library and not an underlying OpenAI API

- [X] This is an issue with the Python library

### Describe the bug

I migrated to new API, followed the guide
![image_2024-03-14_005652531](https://github.com/openai/openai-python/assets/163383302/d4649265-b517-4def-a46d-39859ba120c5)
![image_2024-03-14_005719321](https://github.com/openai/openai-python/assets/163383302/efe1d571-07bc-4845-8f25-e8ebd6be9984)
DISCLAIMER: I DO NOT KNOW HOW TO CODE AT ALL, I do not understand anything, I will just do whatever im told if anyone can fix this problem, I used ChatGPT to get all the way here making a website with a chatbot, im having fun and experimenting in code for the first time

### To Reproduce

Idk how to reproduce just look at the pictures I guess. When I did the migration, after pasting and running the code in WSL, it said it was complete or successful so I closed WSL. after that the second picture is the code I added.

### Code snippets

_No response_

### OS

Windows 11

### Python version

Python v3.12.2

### Library version

openai v1.14.0"
160,679b2186cbe8284d7a72aca3,issue,feature implementation/enhancement -> implementing a new feature,https://github.com/hedyorg/hedy/issues/5266,"ðŸ“œ Simon Says adventure
It would be cool if we could make a Simon Says adventure. I think it should be do-able, but I'm not sure how to program this code yet.I've asked ChatGPT to help me with a python code for a textual simon says game. It came up with this: 
```
import random
import time

# Colors for Simon Says game
COLORS = ['red', 'blue', 'green', 'yellow']

def generate_sequence(length):
    """"""Generate a random sequence of colors.""""""
    return [random.choice(COLORS) for _ in range(length)]

def display_sequence(sequence):
    """"""Display the sequence of colors to the player.""""""
    for color in sequence:
        print(color)
        time.sleep(1)  # Pause for 1 second between each color

def get_player_sequence(length):
    """"""Get the player's input sequence.""""""
    print(""Repeat the sequence:"")
    player_sequence = []
    for _ in range(length):
        color = input().lower()
        while color not in COLORS:
            print(""Invalid color. Please choose from: red, blue, green, yellow"")
            color = input().lower()
        player_sequence.append(color)
    return player_sequence

def compare_sequences(seq1, seq2):
    """"""Compare two sequences to check if they are the same.""""""
    return seq1 == seq2

def simon_says_game():
    """"""Main function to run the Simon Says game.""""""
    print(""Welcome to Simon Says!"")
    level = 1
    while True:
        print(f""Level {level}"")
        simon_sequence = generate_sequence(level)
        display_sequence(simon_sequence)
        player_sequence = get_player_sequence(level)

        if compare_sequences(simon_sequence, player_sequence):
            print(""Correct! Moving on to the next level."")
            level += 1
        else:
            print(""Incorrect! Game Over."")
            break

# Run the Simon Says game
simon_says_game()
```

Anybody here that could make this Hedy-proof (for a higher level, like 17)? Or is it just too complicated? 

If you like the challenge: I have the same request for hangman ;)"
161,679b2214cbe8284d7a72aec7,issue,software quality -> fixing -> functional bugs,https://github.com/joaosilvalopes/linkedin-easy-apply-bot/issues/82,"clickEasyApplyButton appears to click whatever button is in the top right of the page
The log in and link scraping aspects seem to work fine. However, instead of clicking on the ""Easy Apply"" button, it clicks on whatever button is in the top right of your LinkedIn page. Before, it was clicking onto LinkedIn Learning, but then I disconnected LinkedIn Learning just to see if that would resolve the problem. Now it clicks on the ""Try Premium"" button, as it is now at the top right of the page. 

I tried several things to debug, but I am not a JS developer by any means. 

I attempted going through inspect element to figure out what the selector for the button ought to be, and it appeared that 'button.jobs-apply-button[aria-label^=""Easy Apply""]' would do the trick. 

```
const easyApplyButtonSelector = 'button.jobs-apply-button[aria-label^=""Easy Apply""]';
await page.waitForSelector(selectors.easyApplyButtonEnabled, { timeout: 10000 });
await page.click(easyApplyButtonSelector);
```
The above made no difference. I consulted ChatGPT for a suggestion and tried the following:

```  const easyApplyButtonSelector = 'button.jobs-apply-button[aria-label^=""Easy Apply""]';
  const easyApplyButton = await page.$(easyApplyButtonSelector);
  if (easyApplyButton) {
    await easyApplyButton.click();
  } else {
  console.error('Easy Apply button not found');
  }
```
Which also did not work. I'm wondering if anyone has encountered the same issue and, if so, what the solution is. Thank you!"
162,679b2298cbe8284d7a72b14b,issue,learning -> how to use a library/framework/tool,https://github.com/cubiq/ComfyUI_IPAdapter_plus/issues/339,"Noob question about retroactive compatibility
Hello Matteo from Abruzzo :).

Thank you for your hard work!

I use your IPadapter nodes for a variety of workflows that i  have consolidated trough time, and it would be a real pain for me to reconstruct them all, mostly because I have many workflows that differ slightly between one another, and it would mean reconstructing them one by one. 

For now, i have simply used the ""git checkout"" command to put the repo to the point right before the changes happened (i asked chatgpt how to do it...). But I was wondering: maybe i could keep the old version like this, and then ""install"" the new version in a separate folder? Would this be possible? What would I need to change to make it work?"
163,679b230fcbe8284d7a72b248,issue,software quality -> fixing -> supporting debugging,https://github.com/react-navigation/react-navigation/issues/11919,"Navigate into page, increase RAM, navigate back and RAM stays high
### Current behavior

I'm having issues in my app where I navigate to a page, the RAM usage increases, then I navigate back, expecting the memory consumption to go back down, but the RAM usage stays high.

Here is a repo where I provide a reproducible example: https://github.com/spagrhetti/performance-debugging

The basic setup is this:

1. My app has two screens, screen A and screen B, both screens in a React Navigation Stack Navigator. We start on screen A.
2. Navigate to screen B. Screen B has a button that adds a giant string to a useState array. Do this however much you want, after 2 presses RAM usage goes up from ~100MB to about ~850MB on my device (iphone 12 mini).
3. Navigate back to screen A. I would expect memory to go back down, but it stays at that ~850MB level.
Additionally, here is a video of the test: https://www.youtube.com/shorts/ubiYwzrlaQw

Am I missing something? Is there any way to achieve the desired behavior of having RAM go down after navigating back?

I've asked chatGPT about this and it basically tells me that garbage collection in javascript is hard to predict and maybe somehow that memory will get freed later. However, I think that variations of this behavior are causing my app to steadily use more and more RAM as I navigate around, eventually causing it to crash.

Any help on understanding why this occurs and how to better handle memory when dealing with React Navigation would be greatly appreciated!

Note: I notice similar behavior without any navigation involved: allocating a big state var, then setting it to null, RAM usage stays high. I don't completely understand this -- my guess is that I just have to wait for garbage collection to kick in and eventually bring RAM usage back down. However, in my app, that never seems to happen, it just keeps going up and eventually it crashes.

### Expected behavior

I would expect RAM to go back down after navigating back to page A.

### Reproduction

https://github.com/spagrhetti/performance-debugging

### Platform

- [ ] Android
- [X] iOS
- [ ] Web
- [ ] Windows
- [ ] MacOS

### Packages

- [ ] @react-navigation/bottom-tabs
- [ ] @react-navigation/drawer
- [ ] @react-navigation/material-top-tabs
- [X] @react-navigation/stack
- [ ] @react-navigation/native-stack
- [ ] react-native-tab-view

### Environment

- [] I've removed the packages that I don't use

| package                                | version |
| -------------------------------------- | ------- |
| @react-navigation/native               | 6.1.17 |
| @react-navigation/stack                | 6.3.29 |
| react-native-safe-area-context         | 4.8.2 |
| react-native-screens                   | 3.29.0 |
| react-native                           | 0.73.6 |
| expo                                   | 50.0.14|
| node                                   | 16.20.1 |
| npm or yarn                            | 8.19.4 |
"
164,679b230fcbe8284d7a72b309,issue,learning -> how to use a library/framework/tool,https://github.com/excalidraw/svg-to-excalidraw/issues/177,"How to use this project?
I followed the steps in the instructions.
I tried running the code in the readme file with nodejs, in jupyter, in a browser, all the while seeking help from ChatGPT but always errors followed errors following errors.

Could you please add to the read-me file instructions on how to use this project?
"
165,679b23d3cbe8284d7a72b5fd,issue,feature implementation/enhancement -> feature enhancement -> user experience,https://github.com/nunocoracao/blowfish/issues/1398,"âœ¨ Support for URL images in `authorData`and `author-extra.html`
**Describe the bug**
There seems to be a problem with `authorData` being passed to the `author-extra.html` partial. I asked chatgpt for solution and it gave me the solution - which is to edit the `author-extra.html` partial as I described below. 

I edited the partial and then it was fixed. 

(I am not a developer and I have no idea what I am about to say here.)

**To Reproduce**
For author, in the front matter, I pur input data in the image option. However, the image was not there. When I run hugo server, I receive an error. 

**Desktop (please complete the following information):**
 - OS: windows
 
**Hugo & Blowfish versions**
Both latest. 

**Additional context**
Add any other context about the problem here.

The error message indicates that Hugo is trying to fetch an image from a remote location using the `resources.GetRemote` function, but the URL of the image (`img/author-image.jpg`) does not have a valid protocol scheme (like `http://` or `https://`). 

This issue is likely occurring because the `authorData` being passed to the `author-extra.html` partial contains a relative path to the image instead of an absolute URL. 

To fix this issue, you need to ensure that the `image` field in the `authorData` contains an absolute URL to the image if you want to use `resources.GetRemote`. If the images are stored locally in your Hugo site, you can use `resources.Get` instead of `resources.GetRemote`.

Here's an example of how you can modify the `author-extra.html` partial to use `resources.Get`:

```html
{{ $authorImage := resources.Get .data.image }}
{{ $resizedImage := $authorImage.Fill ""150x150"" }}
<img src=""{{ $resizedImage.Permalink }}"" alt=""{{ .data.name }}"">
```

In this code, `resources.Get` is used to get the image from the local file system, and then `Fill` is used to resize the image. The `Permalink` attribute is used to get the URL of the resized image.

Please replace the existing code in `author-extra.html` with the above code and try building your Hugo site again. Let me know if you have any other questions! ðŸ˜Š
"
166,679b2e20cbe8284d7a72b7c2,issue,process -> automating commit/issue/pr creation -> translating commit/issue/pr description,https://github.com/grblHAL/core/issues/490,"MKS Tinybee  cannot connect to the router using STA
Hello everyone,

**_**I have an ESP32 board, MKS Tinybee V1.0, and I want to build a Delta Robot. I generated firmware using the grblHAL Web Builder, using release version 20240408. In the Network/WebUI options card, I enabled V2+V3, Telnet Server, WebSocket, and HTTP server. The WIFI MODE is set to AP+STA, but I cannot connect to the router using STA. It seems that DHCP is not enabled. I tried $RST=*, but after restarting the board, DHCP still won't start. I also tried connecting with a fixed IP address, setting the address to 192.168.9.200, gateway to 192.168.9.1, and subnet mask to 255.255.255.0, but I cannot find where to input DNS. This is the information displayed in the terminal after inputting $I,**_** 
       >>> $I
[VER:1.1f.20240402:]
[OPT:VNMSL,100,1024,3,0]
[AXS:3:XYZ]
[NEWOPT:ENUMS,RT+,HOME,ES,REBOOT,SED,RTC,WIFI,SD]
[FIRMWARE:grblHAL]
[SIGNALS:HSEP]
[NVS STORAGE:*FLASH]
[FREE MEMORY:168K]
[DRIVER:ESP32]
[DRIVER VERSION:240330]
[DRIVER OPTIONS:4.3.2]
[BOARD:MKS Tinybee V1.0]
[AUX IO:1,0,0,0]
[WIFI MAC:08:d1:f9:4d:95:40]
[IP:192.168.5.1]
[PLUGIN:WebUI v0.21]
[KINEMATICS:Delta v0.04]
[PLUGIN:SDCARD v1.13]
[PLUGIN:FS macro plugin v0.07]
ok
**_and this is the configuration information in IOsender. What am I doing wrong? Please correct me. Thank you very much for your help, and I apologize for taking up your time. My English is not good, so thanks to chatGPT for the translation. Once again, thank you for helping me!_**
![IOsender](https://github.com/grblHAL/core/assets/28078472/a3bc3281-4803-4697-8fac-c763ba8c546c)
![WEBbulid](https://github.com/grblHAL/core/assets/28078472/eeff93bd-acc0-4fde-af4d-334aecbe9439)

"
167,679b2f90cbe8284d7a72bf5e,issue,learning -> how to use a library/framework/tool -> generating code examples,https://github.com/PointCloudLibrary/pcl/issues/6027,"[custom] chatGPT code on pcl visualization : different behavior between pcl::visualization::PCLVisualizer::Ptr  and pcl::visualization::PCLVisualizer
I use chatGPT to generate PCL code on show, my task is very simple, to visualize a lidar cloud point,the color is according to lidar's intensity. Below two versions of code are generated by chatGPT,  my enviroments are
```
OS: ubuntu20.04
PCL:1.14(built from source)
VTK: 9.1.0
```

v1: use pcl::visualization::PCLVisualizer::Ptr, it can display lidar points in white-black, without color

```
    //create point cloud,the two versions are the same 
    pcl::PointCloud<pcl::PointXYZI>::Ptr cloud(new pcl::PointCloud<pcl::PointXYZI>);
    float x, y, z, intensity;
    while (input_file.read(reinterpret_cast<char*>(&x), sizeof(float)) &&
           input_file.read(reinterpret_cast<char*>(&y), sizeof(float)) &&
           input_file.read(reinterpret_cast<char*>(&z), sizeof(float)) &&
           input_file.read(reinterpret_cast<char*>(&intensity), sizeof(float))) {
        pcl::PointXYZI point;
        point.x = x;
        point.y = y;
        point.z = z;
        point.intensity = intensity;
        cloud->push_back(point);
    }
    input_file.close();

    // Create PCL visualizer
    pcl::visualization::PCLVisualizer::Ptr viewer(new pcl::visualization::PCLVisualizer(""Cloud Viewer""));
    viewer->setBackgroundColor(0, 0, 0);
    viewer->addPointCloud<pcl::PointXYZI>(cloud, ""cloud"");
    // Colorize points based on intensity
    pcl::visualization::PointCloudColorHandlerGenericField<pcl::PointXYZI> intensity_color(cloud, ""intensity"");
    viewer->addPointCloud<pcl::PointXYZI>(cloud, intensity_color, ""cloud"");
    viewer->setPointCloudRenderingProperties(pcl::visualization::PCL_VISUALIZER_POINT_SIZE, 1, ""cloud"");

    // Run visualization
    while (!viewer->wasStopped()) {
        viewer->spinOnce(100);
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
    }
    return 0;
```
the result is 
![v1](https://github.com/PointCloudLibrary/pcl/assets/10301777/3cfa67dc-0b01-4283-8be3-1564539dd941)

v2: use,pcl::visualization::PCLVisualizer, it can display lidar points with color

```
    //create point cloud,the two versions are the same 
    pcl::PointCloud<pcl::PointXYZI>::Ptr cloud(new pcl::PointCloud<pcl::PointXYZI>);
    float x, y, z, intensity;
    while (input_file.read(reinterpret_cast<char*>(&x), sizeof(float)) &&
           input_file.read(reinterpret_cast<char*>(&y), sizeof(float)) &&
           input_file.read(reinterpret_cast<char*>(&z), sizeof(float)) &&
           input_file.read(reinterpret_cast<char*>(&intensity), sizeof(float))) {
        pcl::PointXYZI point;
        point.x = x;
        point.y = y;
        point.z = z;
        point.intensity = intensity;
        cloud->push_back(point);
    }
    input_file.close();
   // Step 2: Assign colors based on intensity
    pcl::visualization::PCLVisualizer viewer(""Cloud Viewer"");
    viewer.setBackgroundColor(0, 0, 0);
    pcl::visualization::PointCloudColorHandlerGenericField<pcl::PointXYZI> intensity_color(cloud, ""intensity"");
    viewer.addPointCloud<pcl::PointXYZI>(cloud, intensity_color, ""cloud"");
    viewer.setPointCloudRenderingProperties(pcl::visualization::PCL_VISUALIZER_POINT_SIZE, 1, ""cloud"");

    // Step 3: Visualize the point cloud
    while (!viewer.wasStopped()) {
        viewer.spinOnce(100);
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
    }
    return 0;
```

the result is 
![v2](https://github.com/PointCloudLibrary/pcl/assets/10301777/62283077-d798-4d72-8668-61f6569386f2)


Why this happen? I think they are the same, just one use pointer, the other not. "
168,679b2f90cbe8284d7a72bfa2,issue,learning -> how to use a library/framework/tool,https://github.com/nvim-lua/kickstart.nvim/issues/897,"Newb question: Where do I file a bug for the autocomplete for CSS files?
Hi, I'm new to coding and just learning nvim because I hate making progress in my life and prefer side-quests instead, apparently. Anyway...

I am using nvim w/ kickstart's config to edit a `.css` file and when I try to write css for the `*` selector, the...autocomplete...thing (idk what it's called) seems to treat it as a multi-line comment. What I mean is, if I start with:
```css
* {}
```
and then press `<Enter>` with my cursor between the braces, I end up with:
```css
* {
        * 
* }
```
instead of 
```css
* {
   
}
```

![image](https://github.com/nvim-lua/kickstart.nvim/assets/129636054/ce12cca8-00eb-42ef-b3ca-007ff6236ed2)


I know it's probably some plugin or something that does this, but I'm not sure which one or which one I should file the actual bug report with. I asked ChatGPT and it said that tree-sitter-css is only for syntax highlighting and since it's the only css thing I remember installing during the setup of neovim, I'm not sure what else to look for.

If someone can point me in the right direction, that would be appreciated."
169,679b300dcbe8284d7a72c133,issue,software quality -> fixing -> functional bugs,https://github.com/DevYukine/red_oxide/issues/23,"Copying thumbnails causes OS error 1 over SMB
Hi,
I'm running red_oxide v0.8.2 on WSL Ubuntu with a SMB share drive mounted. All of my directories are on the shared drive.
The transcoding finishes on all of the files, but the program stops with OS error 1.
I have identified that fs::copy in src/transcode/util.rs is causing the error when it tries to copy the thumbnail to the new transcoded folder. 
Using a drop-in copy function from ChatGPT that doesn't use fs::copy works, but I'm not sure how you would want to fix it. I can do a PR with the change that works for me if you would like.

Thanks!"
170,679b3127cbe8284d7a72c633,issue,software quality -> fixing -> supporting debugging,https://github.com/kalfazed/tensorrt_starter/issues/6,"The code output in Section 6.2-deploy-classification-advanced is abnormal!
Hi, Thank you for the detailed CUDA and TensorRT tutorials. Theyâ€™ve really helped me out.

I encountered a strange phenomenon and I hope you can help analyze it. When executing the code from Section 6.2-deploy-classification-advanced, I encounter a uniform but incorrect inference output across various images when processed using GPU. The output is as follows:

```shell
[info]Model:      resnet50.onnx
[info]Image:      cat.png
[info]Inference result: cleaver, meat cleaver, chopper
[info]Confidence is 3.092%

[info]Model:      resnet50.onnx
[info]Image:      gazelle.png
[info]Inference result: cleaver, meat cleaver, chopper
[info]Confidence is 3.092%

[info]Model:      resnet50.onnx
[info]Image:      eagle.png
[info]Inference result: cleaver, meat cleaver, chopper
[info]Confidence is 3.092%

[info]Model:      resnet50.onnx
[info]Image:      fox.png
[info]Inference result: cleaver, meat cleaver, chopper
[info]Confidence is 3.092%

[info]Model:      resnet50.onnx
[info]Image:      tiny-cat.png
[info]Inference result: cleaver, meat cleaver, chopper
[info]Confidence is 3.092%

[info]Model:      resnet50.onnx
[info]Image:      wolf.png
[info]Inference result: cleaver, meat cleaver, chopper
[info]Confidence is 3.092%
```

The inference results are the same for all pictures, So I started trying to debug to find the problem. First, I set the `params.dev = model::device::CPU` to see if it was a problem with the pre-processing and post-processing on the GPU. And I found that the result inference on the CPU works fine, The output is as follows:

```shell
[info]Model:      resnet50.onnx
[info]Image:      cat.png
[info]Inference result: Egyptian cat
[info]Confidence is 45.422%

[info]Model:      resnet50.onnx
[info]Image:      gazelle.png
[info]Inference result: gazelle
[info]Confidence is 62.041%

[info]Model:      resnet50.onnx
[info]Image:      eagle.png
[info]Inference result: bald eagle, American eagle, Haliaeetus leucocephalus
[info]Confidence is 99.869%

[info]Model:      resnet50.onnx
[info]Image:      fox.png
[info]Inference result: red fox, Vulpes vulpes
[info]Confidence is 80.789%

[info]Model:      resnet50.onnx
[info]Image:      tiny-cat.png
[info]Inference result: Persian cat
[info]Confidence is 75.310%

[info]Model:      resnet50.onnx
[info]Image:      wolf.png
[info]Inference result: timber wolf, grey wolf, gray wolf, Canis lupus
[info]Confidence is 39.663%
```

So I seriously suspect that there is a problem with the pre-processing or post-processing implementation on the GPU, I found that the post-processing part of the classification task was relatively simple and was not implemented on the GPU, so I positioned the problem on the implementation of `preprocess_gpu` function.

I found that the default strategy adopted in the `process::preprocess_resize_gpu` is `process::tactics::GPU_BILINEAR`, I tried changing it to `process::tactics::GPU_NEAREST` and got the following output:

```shell
[info]Model:      resnet50.onnx
[info]Image:      cat.png
[info]Inference result: Egyptian cat
[info]Confidence is 24.423%

[info]Model:      resnet50.onnx
[info]Image:      gazelle.png
[info]Inference result: gazelle
[info]Confidence is 37.486%

[info]Model:      resnet50.onnx
[info]Image:      eagle.png
[info]Inference result: bald eagle, American eagle, Haliaeetus leucocephalus
[info]Confidence is 99.946%

[info]Model:      resnet50.onnx
[info]Image:      fox.png
[info]Inference result: red fox, Vulpes vulpes
[info]Confidence is 86.935%

[info]Model:      resnet50.onnx
[info]Image:      tiny-cat.png
[info]Inference result: Persian cat
[info]Confidence is 76.643%

[info]Model:      resnet50.onnx
[info]Image:      wolf.png
[info]Inference result: coyote, prairie wolf, brush wolf, Canis latrans
[info]Confidence is 70.486%
```

The output is normal, so I think there is a problem with the implementation part of the `bilinear_BGR2RGB_nhwc2nchw_norm_kernel` kernel function. But I read the code carefully and didn't find any problems. Even I asked ChatGPT to analyze it for me, but I didn't get any useful suggestions.

So I started trying to save the image data processed by the kernel function and use python to view it. I found that all the values in the pre-processed image data were **zero**! So I decided that there should be a problem with the implementation of some parts of the kernel function. I found that the kernel function does not calculate out-of-bounds coordinates and does nothing, like follows:

```cpp
if (src_y1 < 0 || src_x1 < 0 || src_y2 > srcH || src_x2 > srcW) {
    // bilinear interpolation -- å¯¹äºŽè¶Šç•Œçš„åæ ‡ä¸è¿›è¡Œè®¡ç®—
} else {
    // bilinear interpolation -- è®¡ç®—åŽŸå›¾ä¸Šçš„åæ ‡(æµ®ç‚¹ç±»åž‹)åœ¨0~1ä¹‹é—´çš„å€¼
	...
}
```

This seems to be no problem, and the kernel function of nearest neighbor interpolation also does this. I accidentally thought of adding an extra `return;` when the coordinates are out of bounds, as follows:

```cpp
if (src_y1 < 0 || src_x1 < 0 || src_y2 > srcH || src_x2 > srcW) {
    // bilinear interpolation -- å¯¹äºŽè¶Šç•Œçš„åæ ‡ä¸è¿›è¡Œè®¡ç®—
    return;  // Newly added
} else {
    // bilinear interpolation -- è®¡ç®—åŽŸå›¾ä¸Šçš„åæ ‡(æµ®ç‚¹ç±»åž‹)åœ¨0~1ä¹‹é—´çš„å€¼
	...
}
```

When I run it again I got the correct value, The output is as follows:

```shell
[info]Model:      resnet50.onnx
[info]Image:      cat.png
[info]Inference result: Egyptian cat
[info]Confidence is 45.700%

[info]Model:      resnet50.onnx
[info]Image:      gazelle.png
[info]Inference result: gazelle
[info]Confidence is 62.227%

[info]Model:      resnet50.onnx
[info]Image:      eagle.png
[info]Inference result: bald eagle, American eagle, Haliaeetus leucocephalus
[info]Confidence is 99.873%

[info]Model:      resnet50.onnx
[info]Image:      fox.png
[info]Inference result: red fox, Vulpes vulpes
[info]Confidence is 80.740%

[info]Model:      resnet50.onnx
[info]Image:      tiny-cat.png
[info]Inference result: Persian cat
[info]Confidence is 75.263%

[info]Model:      resnet50.onnx
[info]Image:      wolf.png
[info]Inference result: timber wolf, grey wolf, gray wolf, Canis lupus
[info]Confidence is 40.196%
```

I asked ChatGPT and the reason it gave was that an out-of-bounds thread not returned might access an invalid memory address, leading to undefined behavior. But doesn't nearest neighbor interpolation do the same? Why is there no such problem?

So I want to know if you have the same problem and what is the reason why this problem occurs.

Thank you very much.

My environment is as follows:

```shell
Ubuntu 20.04.06 LTS
Gen Intel(R) Core(TM) i5-12400F
NVIDIA GeForce RTX3060
CUDA 11.6, cuDNN 8.4.0, TensorRT 8.6.1.6, OpenCV 4.6.0
```"
171,679b31bfcbe8284d7a72c8f8,issue,software quality -> fixing -> functional bugs,https://github.com/PowerShell/PowerShell/issues/23850,"""RegisterMicrosoftUpdate.ps1"" fails and freezes installer when AppLocker / WDAC rules exist (despite the script being whitelisted & running full language mode)
### Prerequisites

- [X] Write a descriptive title.
- [X] Make sure you are able to repro it on the [latest released version](https://github.com/PowerShell/PowerShell/releases)
- [X] Search the existing issues.
- [X] Refer to the [FAQ](https://github.com/PowerShell/PowerShell/blob/master/docs/FAQ.md).
- [X] Refer to [Differences between Windows PowerShell 5.1 and PowerShell](https://learn.microsoft.com/powershell/scripting/whats-new/differences-from-windows-powershell).

### Steps to reproduce

**Situation Where This Occurs:** When a system is set up with any security/execution policy that causes PowerShell to run in Constrained Language mode by default, such as the PowerShell Core Group Policy Editor execution policy being set as ""Allow only signed scripts"", or AppLocker rules that require whitelisting to run in Full Language mode.

**Problem:** If the user runs the installer and selects to use Microsoft Update (I believe, or otherwise whatever causes `RegisterMicrosoftUpdate.ps1` to be run as part of the installation), and the system is set up as described above, the installer will hang at the end forever showing no error and never complete.

**Root Cause:** The problem exists in `RegisterMicrosoftUpdate.ps1` and the fact that the $jobScript block is run as a new thread which creates a new session outside of the whitelisted session, therefore it is in Constrained Language Mode instead of the outer script's Full Language Mode, so it fails. Then for some reason the installer doesn't notice this and just hangs instead of displaying an error and moving on.

--------

## **More Problem Details**

I noticed the PowerShell 7 MSI installer would freeze near the end and show no movement or message. I ran the MSI installer with verbose logging and it was hanging completely with the last line being this:

```
CAQuietExec:  ""C:\Program Files\PowerShell\7\pwsh.exe"" -NoProfile -ExecutionPolicy Bypass -File ""C:\Program Files\PowerShell\7\RegisterMicrosoftUpdate.ps1""
```

To be clear, the script should _theoretically_ be able to run no problem, because in AppLocker I have rules to allow any signed scripts, as well as the powershell core policy in GPE being set to allow only signed scripts (which it is). I also even tried explicitly adding an allow rule on the script's file hash and temporarily changing the group policy GPE setting, but it didn't work (because AppLocker was still active, therefore defaulting to Constrained Language), but given the cause, this is not a surprise. The script itself is running in Full Language mode, that's not the problem, rather the inner code block mentioned before.

Running the script by itself yields these errors:

```
PS C:\Program Files\PowerShell\7> .\RegisterMicrosoftUpdate.ps1
VERBOSE: Running job script:
            # This registers Microsoft Update via a predifened GUID with the Windows Update Agent.
            # https://learn.microsoft.com/windows/win32/wua_sdk/opt-in-to-microsoft-update

            $serviceManager = (New-Object -ComObject Microsoft.Update.ServiceManager)
            $isRegistered = $serviceManager.QueryServiceRegistration('7971f918-a847-4430-9279-4a52d1efe18d').Service.IsRegisteredWithAu

            if (!$isRegistered) {
                Write-Verbose -Verbose ""Opting into Microsoft Update as the Autmatic Update Service""
                # 7 is the combination of asfAllowPendingRegistration, asfAllowOnlineRegistration, asfRegisterServiceWithAU
                # AU means Automatic Updates
                $null = $serviceManager.AddService2('7971f918-a847-4430-9279-4a52d1efe18d', 7, '')
            }
            else {
                Write-Verbose -Verbose ""Microsoft Update is already registered for Automatic Updates""
            }

            $isRegistered = $serviceManager.QueryServiceRegistration('7971f918-a847-4430-9279-4a52d1efe18d').Service.IsRegisteredWithAu

            # Return if it was successful, which is the opposite of Pending.
            return $isRegistered

VERBOSE: Waiting on Job for 300 seconds
VERBOSE: Job finished.  State: Completed
New-Object:
Line |
   5 |  â€¦ viceManager = (New-Object -ComObject Microsoft.Update.ServiceManager)
     |                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     | Cannot create type. Only core types are supported in this language mode.
InvalidOperation:
Line |
   6 |              $isRegistered = $serviceManager.QueryServiceRegistration( â€¦
     |              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     | You cannot call a method on a null-valued expression.
InvalidOperation:
Line |
  12 |  â€¦             $null = $serviceManager.AddService2('7971f918-a847-4430-9 â€¦
     |                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     | You cannot call a method on a null-valued expression.
InvalidOperation:
Line |
  18 |              $isRegistered = $serviceManager.QueryServiceRegistration( â€¦
     |              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     | You cannot call a method on a null-valued expression.
VERBOSE: Opting into Microsoft Update as the Autmatic Update Service
VERBOSE: Result:
VERBOSE: Registration failed
```

With the main problem being:
`Cannot create type. Only core types are supported in this language mode.`

The problem is like I described above, how $scriptBlock is being run in a new session/context so it doesn't inherit the Full Language mode and fails despite being within the allowed script.

### Expected behavior


I would expect the installer to display an error message that the registering to Windows Update had failed but the installation was otherwise a success.


#### Example Solution: 
With the help of ChatGPT I had it create a version of the script that runs the script block in the same context and works:

```
param(
    [ValidateSet('Hang', 'Fail')]
    $TestHook
)

$waitTimeoutSeconds = 300
switch ($TestHook) {
    'Hang' {
        $waitTimeoutSeconds = 10
        $jobScript = { Start-Sleep -Seconds 600 }
    }
    'Fail' {
        $jobScript = { throw ""This job script should fail"" }
    }
    default {
        $jobScript = {
            # This registers Microsoft Update via a predifened GUID with the Windows Update Agent.
            # https://learn.microsoft.com/windows/win32/wua_sdk/opt-in-to-microsoft-update

            $serviceManager = (New-Object -ComObject Microsoft.Update.ServiceManager)
            $isRegistered = $serviceManager.QueryServiceRegistration('7971f918-a847-4430-9279-4a52d1efe18d').Service.IsRegisteredWithAu

            if (!$isRegistered) {
                Write-Verbose -Verbose ""Opting into Microsoft Update as the Automatic Update Service""
                # 7 is the combination of asfAllowPendingRegistration, asfAllowOnlineRegistration, asfRegisterServiceWithAU
                # AU means Automatic Updates
                $null = $serviceManager.AddService2('7971f918-a847-4430-9279-4a52d1efe18d', 7, '')
            }
            else {
                Write-Verbose -Verbose ""Microsoft Update is already registered for Automatic Updates""
            }

            $isRegistered = $serviceManager.QueryServiceRegistration('7971f918-a847-4430-9279-4a52d1efe18d').Service.IsRegisteredWithAu

            # Return if it was successful, which is the opposite of Pending.
            return $isRegistered
        }
    }
}

Write-Verbose ""Running job script"" -Verbose

# Run the script block synchronously in the current session
$result = & $jobScript

Write-Verbose ""Result: $result"" -Verbose
if ($result) {
    Write-Verbose ""Registration succeeded"" -Verbose
    exit 0
}
else {
    Write-Verbose ""Registration failed"" -Verbose
    # at the time this was written, the MSI is ignoring the exit code
    exit 1
}
```


### Actual behavior

Installer hangs forever until force quit with Task manager.

I added logging lines to various parts of the script to see the language mode at different parts which revealed that part of it was running in constrained language mode.

Here is the modified script:

```
param(
    [ValidateSet('Hang', 'Fail')]
    $TestHook
)

Write-Output ""1 The execution policy is: $($ExecutionContext.SessionState.LanguageMode)`n""

$waitTimeoutSeconds = 300
switch ($TestHook) {
    'Hang' {
        $waitTimeoutSeconds = 10
        $jobScript = { Start-Sleep -Seconds 600 }
    }
    'Fail' {
        $jobScript = { throw ""This job script should fail"" }
    }
    default {
        $jobScript = {
            # This registers Microsoft Update via a predifened GUID with the Windows Update Agent.
            # https://learn.microsoft.com/windows/win32/wua_sdk/opt-in-to-microsoft-update
			Write-Output ""2 The execution policy is: $($ExecutionContext.SessionState.LanguageMode)`n""
            $serviceManager = (New-Object -ComObject Microsoft.Update.ServiceManager)
            $isRegistered = $serviceManager.QueryServiceRegistration('7971f918-a847-4430-9279-4a52d1efe18d').Service.IsRegisteredWithAu
			Write-Output ""3 The execution policy is: $($ExecutionContext.SessionState.LanguageMode)`n""
            if (!$isRegistered) {
				Write-Output ""4 The execution policy is: $($ExecutionContext.SessionState.LanguageMode)`n""
                Write-Verbose -Verbose ""Opting into Microsoft Update as the Autmatic Update Service""
                # 7 is the combination of asfAllowPendingRegistration, asfAllowOnlineRegistration, asfRegisterServiceWithAU
                # AU means Automatic Updates
                $null = $serviceManager.AddService2('7971f918-a847-4430-9279-4a52d1efe18d', 7, '')
				Write-Output ""5 The execution policy is: $($ExecutionContext.SessionState.LanguageMode)`n""
            }
            else {
                Write-Verbose -Verbose ""Microsoft Update is already registered for Automatic Updates""
            }

            $isRegistered = $serviceManager.QueryServiceRegistration('7971f918-a847-4430-9279-4a52d1efe18d').Service.IsRegisteredWithAu
			Write-Output ""6 The execution policy is: $($ExecutionContext.SessionState.LanguageMode)`n""
            # Return if it was successful, which is the opposite of Pending.
            return $isRegistered
        }
    }
}

Write-Verbose ""Running job script: $jobScript"" -Verbose
$job = Start-ThreadJob -ScriptBlock $jobScript

Write-Verbose ""Waiting on Job for $waitTimeoutSeconds seconds"" -Verbose
$null = Wait-Job -Job $job -Timeout $waitTimeoutSeconds

if ($job.State -ne 'Running') {
    Write-Verbose ""Job finished.  State: $($job.State)"" -Verbose
    $result = Receive-Job -Job $job -Verbose
    Write-Verbose ""Result: $result"" -Verbose
    if ($result) {
        Write-Verbose ""Registration succeeded"" -Verbose
        exit 0
    }
    else {
        Write-Verbose ""Registration failed"" -Verbose
        # at the time this was written, the MSI is ignoring the exit code
        exit 1
    }
}
else {
    Write-Verbose ""Job timed out"" -Verbose
    Write-Verbose ""Stopping Job.  State: $($job.State)"" -Verbose
    Stop-Job -Job $job
    # at the time this was written, the MSI is ignoring the exit code
    exit 258
}
```

And its output:

```
PS C:\Users\Joe\Desktop\Tests\Powershell bug> .\RegisterMicrosoftUpdate.ps1
1 The execution policy is: FullLanguage


[[[ Removed redundant verbose lines ]]]

New-Object:
Line |
   5 |  â€¦ viceManager = (New-Object -ComObject Microsoft.Update.ServiceManager)
     |                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     | Cannot create type. Only core types are supported in this language mode.
InvalidOperation:
Line |
   6 |              $isRegistered = $serviceManager.QueryServiceRegistration( â€¦
     |              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     | You cannot call a method on a null-valued expression.
InvalidOperation:
Line |
  13 |  â€¦             $null = $serviceManager.AddService2('7971f918-a847-4430-9 â€¦
     |                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     | You cannot call a method on a null-valued expression.
InvalidOperation:
Line |
  20 |              $isRegistered = $serviceManager.QueryServiceRegistration( â€¦
     |              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     | You cannot call a method on a null-valued expression.
VERBOSE: Opting into Microsoft Update as the Autmatic Update Service
VERBOSE: Result: 2 The execution policy is: ConstrainedLanguage
 3 The execution policy is: ConstrainedLanguage 4 The execution policy is: ConstrainedLanguage
 5 The execution policy is: ConstrainedLanguage
 6 The execution policy is: ConstrainedLanguage

VERBOSE: Registration succeeded
```

Notice it starts out in full language mode but once inside $jobScript it is constrained


### Error details

```
New-Object:
Line |
   5 |  â€¦ viceManager = (New-Object -ComObject Microsoft.Update.ServiceManager)
     |                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     | Cannot create type. Only core types are supported in this language mode.
```


### Environment data

```powershell
Name                           Value
----                           -----
PSVersion                      7.4.2
PSEdition                      Core
GitCommitId                    7.4.2
OS                             Microsoft Windows 10.0.26100
Platform                       Win32NT
PSCompatibleVersions           {1.0, 2.0, 3.0, 4.0â€¦}
PSRemotingProtocolVersion      2.3
SerializationVersion           1.1.0.1
WSManStackVersion              3.0
```


### Visuals

_No response_"
172,679b3249cbe8284d7a72c9dc,issue,software quality -> fixing -> supporting debugging -> writing code to reproduce a bug,https://github.com/holoviz/panel/issues/6869,"Accordion vertical text after toggling on/off
This is my first issue, so forgive me in case of a wrong usage or duple issue, but couldn't find anything.

#### ALL software version info
panel==1.3.8

#### Description of expected behavior and the observed behavior

Originally the accordion loads correctly, but after toggling off and on the accordion using the button, the displayed names are all messed up. Clicking on the accordion correct the problem.

#### Complete, minimal, self-contained example code that reproduces the issue
I used ChatGPT to create a easy example of this issue.
```
import panel as pn

# Initialize the panel extension
pn.extension()

# Define the content for each panel
panel1 = pn.pane.Markdown(""## Panel 1 Content\nThis is the content of the first panel."")
panel2 = pn.pane.Markdown(""## Panel 2 Content\nThis is the content of the second panel."")
panel3 = pn.pane.Markdown(""## Panel 3 Content\nThis is the content of the third panel."")

# Create the accordion
accordion = pn.Accordion(('Panel 1', panel1), ('Panel 2', panel2), ('Panel 3', panel3))

# Create the button
toggle_button = pn.widgets.Button(name='Toggle Accordion', button_type='primary')

# Define the callback function to toggle the visibility
def toggle_accordion(event):
    accordion.visible = not accordion.visible

# Attach the callback to the button
toggle_button.on_click(toggle_accordion)

# Layout the button and accordion
layout = pn.Column(toggle_button, accordion)

# Serve the layout
layout.show()


```

#### Screenshots or screencasts of the bug in action
Before toggling : 
![image](https://github.com/holoviz/panel/assets/171029078/c9c4d7b7-bb5f-462c-aa78-087c903c8aca)

After toggling : 
![image](https://github.com/holoviz/panel/assets/171029078/ac4c7081-9c46-4c58-9d40-2849df309881)
"
173,679b3249cbe8284d7a72caa0,issue,process -> automating commit/issue/PR creation -> creating commit/issue/PR description,https://github.com/zed-industries/zed/issues/12469,"Markdown: Support using `**`/`__`/`==` on selections
### Check for existing issues

- [X] Completed

### Describe the bug / provide steps to reproduce it

Hello,

I have encountered an issue with the Zed code editor when trying to add ** around a selected word to bold it. When I select a word and attempt to add ** at the start and end in one go, the editor deletes the selected word instead. This behavior does not occur when using brackets; adding brackets around a selected word works as expected.

Steps to reproduce:
1. Select a word in the editor.
2. Attempt to add `**`/`__`/`==`/`` ` `` at the start and end of the selected word simultaneously.

Expected behavior:
The selected word should be wrapped in `**` and remain intact.

Actual behavior:
The selected word is deleted.

This issue affects the efficiency of formatting text in markdown or similar syntaxes where bolding text is required.

Thank you for looking into this!

Best regards,
Anshul Raj Verma

> Used ChatGPT to generate this. Also modified some bit.

### Environment

Zed: v0.137.3 (Zed)
OS: macOS 14.5.0
Memory: 16 GiB
Architecture: aarch64

### If applicable, add mockups / screenshots to help explain present your vision of the feature

_No response_

### If applicable, attach your `~/Library/Logs/Zed/Zed.log` file to this issue.

_No response_"
174,679b3249cbe8284d7a72cb83,issue,feature implementation/enhancement -> prototyping -> implementing proof of concept,https://github.com/manga-download/hakuneko/issues/7131,"[Site Request] Hentai20
**Name of the website**
Hentai20

**Website urls (examples below)**
- Site: https://hentai20.io/
- Manga List: https://hentai20.io/manga/
- Manga example: https://hentai20.io/manga/dating-sim-short-story-the-dating-simulator-cheat-code/
- Chapter online viewer example: https://hentai20.io/dating-sim-short-story-the-dating-simulator-cheat-code-chapter-1/

**Languages**
English

**Additional details**
__IMPORTANT__
I've seen that there have been previous site requests about hentai20; however, they were closed due to ""Cloudflare."" However, I checked the site, and their Cloudflare protection is non existent. I even tested it by using curl to download a jpg from the website, and it worked. I also made ChatGPT code a Python script that downloads it based on the manga title, and it worked (I know some Python, so I corrected the errors made by ChatGPT). So, if even a Python script works, then I'm like 99% sure that they lowered or disabled Cloudflare."
175,679b3249cbe8284d7a72cbbd,issue,software quality -> fixing -> supporting debugging,https://github.com/face-hh/webx/issues/120,"Can't run on Linux Mint (+ Windows UI Bug)
I can't compile or run the browser on linux mint, i've tried to compile it with the steps given in the readme and:
```
  The system library `gtk4` required by crate `gtk4-sys` was not found.
  The file `gtk4.pc` needs to be installed and the PKG_CONFIG_PATH environment variable must contain its parent directory.
  PKG_CONFIG_PATH contains the following:
      - /usr/lib/x86_64-linux-gnu/pkgconfig
```
(**note**: i did check the directory, it has a gtk4.pc in it)

okay, if not compiled than maybe pre-compiled right? well no because:

```
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libgtk-4.so.1)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libgtk-4.so.1)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libgtk-4.so.1)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libpango-1.0.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libpango-1.0.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libgdk_pixbuf-2.0.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libcairo.so.2)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libcairo.so.2)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libcairo.so.2)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libgio-2.0.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libgio-2.0.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libglib-2.0.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libglib-2.0.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libgobject-2.0.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libadwaita-1.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libadwaita-1.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libssl.so.3)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libcrypto.so.3)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libcrypto.so.3)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/liblua.so.5.4)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libgmodule-2.0.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libpangocairo-1.0.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libpangoft2-1.0.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libcloudproviders.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libtracker-sparql-3.0.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libtracker-sparql-3.0.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libvulkan.so.1)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libvulkan.so.1)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libpng16.so.16)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libtiff.so.6)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libjpeg.so.8)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libxkbcommon.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libxkbcommon.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libXcursor.so.1)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libXcursor.so.1)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libcairo-script-interpreter.so.2)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libxcb-render.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libxcb-shm.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libpixman-1.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libmount.so.1)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.36' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libmount.so.1)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libmount.so.1)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libpcre2-8.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libffi.so.8)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libappstream.so.5)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libxml2.so.2)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libxml2.so.2)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libsqlite3.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libsqlite3.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libzstd.so.1)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/liblzma.so.5)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libjbig.so.2.1)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libblkid.so.1)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libblkid.so.1)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libcurl.so.4)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libxmlb.so.2)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libsystemd.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libsystemd.so.0)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libicuuc.so.75)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libicuuc.so.75)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libicuuc.so.75)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libnghttp3.so.9)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libnghttp2.so.14)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libpsl.so.5)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libgssapi_krb5.so.2)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libcap.so.2)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libcap.so.2)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libgcrypt.so.20)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/liblz4.so.1)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_ABI_DT_RELR' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libunistring.so.5)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libkrb5.so.3)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libk5crypto.so.3)
./Bussin.Napture-x86_64.AppImage: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.38' not found (required by /tmp/.mount_Bussind5Ymtm/usr/bin/../lib/libkrb5support.so.0)
```

i\ve asked chatgpt about both of them, nothing worked"
176,679b3c80cbe8284d7a72cdcd,issue,software quality -> fixing -> supporting debugging,https://github.com/TMElyralab/Comfyui-MusePose/issues/34,"mat1 and mat2 shapes cannot be multiplied
Anyone got the same error??
I've searched the existing issues and asked ChatGPT  
How can I fix this? Thank you ><

```
[2024-06-08 00:36] 2024-06-08 00:36:48,912- root:179- ERROR- !!! Exception during processing!!! Error(s) in loading state_dict for CLIPVisionModelWithProjection:
	size mismatch for vision_model.embeddings.class_embedding: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for vision_model.embeddings.patch_embedding.weight: copying a param with shape torch.Size([1024, 3, 14, 14]) from checkpoint, the shape in current model is torch.Size([768, 3, 32, 32]).
	size mismatch for vision_model.embeddings.position_embedding.weight: copying a param with shape torch.Size([257, 1024]) from checkpoint, the shape in current model is torch.Size([50, 768]).
	size mismatch for vision_model.pre_layrnorm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([768]).
```
I added `ignore_mismatched_sizes=True` in the `from_pretrained` method but another error occurred


```
Exception during processing!!! mat1 and mat2 shapes cannot be multiplied (2x512 and 768x320)
2024-06-08 01:33:33,182- root:180- ERROR- Traceback (most recent call last):
  File ""D:\User\ComfyUI_windows_portable\ComfyUI\execution.py"", line 151, in recursive_execute
    output_data, output_ui = get_output_data(obj, input_data_all)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\ComfyUI\execution.py"", line 81, in get_output_data
    return_values = map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\ComfyUI\execution.py"", line 74, in map_node_over_list
    results.append(getattr(obj, func)(**slice_dict(input_data_all, i)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\ComfyUI\custom_nodes\Comfyui-MusePose\nodes.py"", line 791, in musepose_func
    return musepose(args, image_path, video)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\ComfyUI\custom_nodes\Comfyui-MusePose\nodes.py"", line 735, in musepose
    return handle_single(image_path, video)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\ComfyUI\custom_nodes\Comfyui-MusePose\nodes.py"", line 713, in handle_single
    video = pipe(
            ^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\torch\utils\_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\ComfyUI\custom_nodes\Comfyui-MusePose\musepose\pipelines\pipeline_pose2vid_long.py"", line 467, in __call__
    self.reference_unet(
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\torch\nn\modules\module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\torch\nn\modules\module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\ComfyUI\custom_nodes\Comfyui-MusePose\musepose\models\unet_2d_condition.py"", line 1196, in forward
    sample, res_samples = downsample_block(
                          ^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\torch\nn\modules\module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\torch\nn\modules\module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\ComfyUI\custom_nodes\Comfyui-MusePose\musepose\models\unet_2d_blocks.py"", line 657, in forward
    hidden_states, ref_feature = attn(
                                 ^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\torch\nn\modules\module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\torch\nn\modules\module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\ComfyUI\custom_nodes\Comfyui-MusePose\musepose\models\transformer_2d.py"", line 356, in forward
    hidden_states = block(
                    ^^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\torch\nn\modules\module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\torch\nn\modules\module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\ComfyUI\custom_nodes\Comfyui-MusePose\musepose\models\mutual_self_attention.py"", line 242, in hacked_basic_transformer_inner_forward
    attn_output = self.attn2(
                  ^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\torch\nn\modules\module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\torch\nn\modules\module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\diffusers\models\attention_processor.py"", line 512, in forward
    return self.processor(
           ^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\diffusers\models\attention_processor.py"", line 1231, in __call__
    key = attn.to_k(encoder_hidden_states, *args)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\torch\nn\modules\module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\torch\nn\modules\module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\diffusers\models\lora.py"", line 430, in forward
    out = super().forward(hidden_states)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\User\ComfyUI_windows_portable\python_embeded\Lib\site-packages\torch\nn\modules\linear.py"", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x512 and 768x320)
```"
177,679b3da0cbe8284d7a72d0fa,issue,learning -> how to use a library/framework/tool,https://github.com/scripting/opml.org/issues/20,"HTTP content-type for OPML files?
I have a server that's returning OPML files. 

It's not the first time I've done this, but now there seem to be enough other people interested in this that it's worth raising this question. 

What content type should I use? Generally I've been using text/xml. 

But I see other people using text/opml. 

I asked ChatGPT what it thinks and it [says](https://chatgpt.com/share/d7864fe3-3f05-4588-bfe1-a5c3ec48baa9) that text/x-opml is the standard. 

Meta.ai says application/opml+xml.

I'm interested primarily in what other people are currently doing. "
178,679b3e2fcbe8284d7a72d4af,issue,software quality -> fixing -> supporting debugging,https://github.com/kitao/pyxel/issues/542,"(MacOS) Version 2.1 displays a warning about the state restoration function when the programme is executed.
[ warning message ] 
2024-06-25 06:29:56.243 Python[14458:2546187] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.

As I have no knowledge of this, I asked ChatGPT about this warning and received the following response
Is this an effect of some change in version 2.1?

This warning message relates to the development of macOS applications.Specifically, it indicates that a Python script is utilising native macOS functionality and that the security settings related to the State Restore functionality are not enabled. 
Details are provided below.

NSApplicationDelegate.applicationSupportsSecureRestorableState:.
This is a macOS application delegate method that specifies whether the application supports secure state restoration.
Secure state restoration is the ability to save the application's state and restore it securely on restart.
Cause of warning.
This warning is displayed when a Python script behaves like a macOS application and there are no security settings for state restoration. This usually occurs when using a wrapper library (e.g. PyObjC) to utilise native macOS functionality in Python.
"
179,679b3ebccbe8284d7a72d622,issue,feature implementation/enhancement -> feature enhancement,https://github.com/SwagOtaku/plugin.video.kaito/issues/109,"Improved title filtering, by implementing words separated by commas or using regex
Hello, I am a regular anime user, for a while I used radarr and sonarr to download content, but this is meaningless if you will not watch it again, but in these programs something called custom format is used which allows the use of regex or many other terms without limits and even with a scoring system to choose the best and download automatically.

![cf-import-to-update](https://github.com/SwagOtaku/plugin.video.kaito/assets/91355308/0a31f18d-6f85-4971-a5c1-ee7f907c3b78)

In these you can incorporate the audio channels, words in the titles, quality, video source among others, but this is very elaborate, I only use it for a brief introduction.

### **OTAKU ADDON**

In otaku we already have the possibility of filtering by words but we only have the possibility of using 5 terms, although this seems like a lot, in some cases it is not enough because many times the titles do not always have the same strings even if they belong to the same group.

That is why it would be interesting to add more fields for greater control or the possibility of entering 5 more terms or even more (the more the better).

**Filtering using Regex**

![Screenshot 2024-06-28 095938](https://github.com/SwagOtaku/plugin.video.kaito/assets/91355308/96029f79-6a97-43f2-a292-a1e24733d569)

(?=.*\bMULTi\b)(?=.*\bNanDesuKa\b)(?=.*\(CR\))(?=.*\bSpanish\b).*

This regular expression uses positive lookaheads ((?=...)) to ensure that all keywords are present on the same line. Here is the breakdown of the expression:

1. (?=.*\bMULTi\b) ensures that ""MULTi"" is present on the line.
2. (?=.*\bNanDesuKa\b) ensures that ""NanDesuKa"" is present on the line.
3. (?=.*\(CR\)) ensures that ""(CR)"" is present on the line.
4. (?=.*\bSpanish\b) ensures that ""Spanish"" is present on the line.
5. .* allows the expression to match anything before, between, and after the keywords.

To create this expression, I asked ChatGPT

**In the addon we would introduce it as follows**

![Screenshot 2024-06-28 101405](https://github.com/SwagOtaku/plugin.video.kaito/assets/91355308/0be40b0b-f4fc-4cec-a632-7acba3def591)

With these changes we could obtain better results and even use auto-reproduction.

Making sure to give priority to sources with Dubbing and that in case you don't select one with the other terms.

In my case I consume content with Spanish or Latin Dubbing, I use Kodi's preferences.
"
180,679b3ebccbe8284d7a72d6b0,issue,learning -> how to use a library/framework/tool,https://github.com/AdguardTeam/AdguardBrowserExtension/issues/2858,"If possible, add the advanced data clear feature.
### Issue Details

Hi, I asked ChatGPT about the clear data, the simple answer is this: The shortcut ctrl + shift + r/ctrl + F5/shift + F5 just refreshes the page and clears the cache, while the ""Clear Site Data"" option does a more comprehensive data clearing.

Normally I would do this, but sometimes it's hard because the Anti-devtool scripts thing is so annoying. Also, I guess there is no shortcut for this ""Clear site Data"" option.

<details><summary>Screenshot (Clear site data)</summary>
<p>

![image](https://github.com/AdguardTeam/AdguardBrowserExtension/assets/56846906/7da3f829-5a22-4172-918e-70bec3c1b8d7)

</p>
</details>

I suggested this feature because AdGuard extension is fully related to data cleaning as well. (triggering factors or outdated data etc.) 

----

There are extensions called ""Clear site data"", examples of features that can be cleared:

<details><summary>Screenshot (Clear site data)</summary>
<p>

![image](https://github.com/AdguardTeam/AdguardBrowserExtension/assets/56846906/a2d84806-d8ec-4fc8-ac53-8f72368458a7)

</p>
</details>

### Proposed solution

So if it is possible to integrate this ""clear data"" feature to AdGuard extension, it would be good to add a clear data icon to the AdGuard extension drop-down menu and Filtering log. Also keyboard shortcut for AdGuard extension. ðŸ˜„

### Alternative solution

_No response_"
181,679b3ebccbe8284d7a72d7c1,issue,feature implementation/enhancement -> feature enhancement,https://github.com/mrAceT/nextcloud-S3-local-S3-migration/issues/16,"A version of localtos3.php adapted for PostgreSQL made with the help of ChatGPT
This is a raw version that has not been reviewed or corrected. The idea is that together we can create a fully functional final version.
I'm new to Github. This is the easiest way to share the file:
https://cloud.brasdrive.com.br/s/f3cReWoRGAxaDMH

Explanation of the Adaptations Made
Connection to PostgreSQL:

All database connections and queries were changed from MySQL to PostgreSQL using pg_connect, pg_query, and related functions.
Backup and Restore Commands:

The backup and restore commands were changed to use pg_dump and psql instead of mysqldump and mysql.
SQL Queries:

The SQL queries were adapted to use PostgreSQL syntax, including double quotes for identifiers and specific functions like EXTRACT(EPOCH FROM ...).
Execution Options:

The dry-run, update-db, and delete-local options were kept as they were in the original script.
S3 Compatibility:

The configuration and use of AWS S3 for file synchronization were maintained.
This script should be executed in the Docker environment of Nextcloud to ensure that all dependencies and specific configurations are available. Be sure to make a full backup before running it in your production environment."
182,679b3f3dcbe8284d7a72d838,issue,feature implementation/enhancement -> implementing a new feature,https://github.com/sindresorhus/type-fest/issues/904,"`LiteralList<Union>`
I want to be able to define an array that contains exactly the elements of all possible values of a union once.

``` ts
type Union = ""prevStep"" | ""nextStep"" | ""submit"" | ""order"";

// this works:
const list: LiteralList<Union> = [""nextStep"", ""order"", ""prevStep"", ""submit""];

// this fails:
const list: LiteralList<Union> = [""nextStep"", ""order"", ""prevStep"", ""submit"", ""submit""];

// this fails:
const list: LiteralList<Union> = [""nextStep"", ""order"", ""prevStep""];
```

I already made this work with this type:
``` ts
type LiteralList<T extends string, U = T> = [T] extends [never]
  ? []
  : U extends T
  ? [U, ...LiteralList<Exclude<T, U>>]
  : [];
```

I'm not sure though if it makes sense since I asked ChatGPT to do it. 

Is it possible to add this type helper to type-fest?


<!-- POLAR PLEDGE BADGE START -->
## Upvote & Fund

- We're using [Polar.sh](https://polar.sh/sindresorhus) so you can upvote and help fund this issue.
- The funding will be given to active contributors.
- Thank you in advance for helping prioritize & fund our backlog.

<a href=""https://polar.sh/sindresorhus/type-fest/issues/904"">
<picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""https://polar.sh/api/github/sindresorhus/type-fest/issues/904/pledge.svg?darkmode=1"">
  <img alt=""Fund with Polar"" src=""https://polar.sh/api/github/sindresorhus/type-fest/issues/904/pledge.svg"">
</picture>
</a>
<!-- POLAR PLEDGE BADGE END -->
"
183,679b3f3dcbe8284d7a72d8b6,issue,"software quality -> fixing -> functional bugs,software quality -> fixing -> supporting debugging",https://github.com/zotify-dev/zotify/issues/151,"Errors: ""bad escape \m at position 3"" and ""[WinError 183] Cannot create a file when that file already exists""
Had the first mentioned error and once fixed, had the second. Used ChatGPT to troubleshoot and fix both so someone may want to review. Sorry I don't know the etiquette or process here but wanted to give my fixes in case anyone else would benefit.

### First error

```powershell
bad escape \m at position 3

Traceback (most recent call last):
  File ""_path to track.py_"", line 190, in download_track
    c = len([file for file in Path(filedir).iterdir() if re.search(f'^{filename}_', str(file))]) + 1
                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.1264.0_x64__qbz5n2kfra8p0\Lib\re\__init__.py"", line 177, in search
    return _compile(pattern, flags).search(string)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.1264.0_x64__qbz5n2kfra8p0\Lib\re\__init__.py"", line 307, in _compile
    p = _compiler.compile(pattern, flags)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.1264.0_x64__qbz5n2kfra8p0\Lib\re\_compiler.py"", line 745, in compile
    p = _parser.parse(p, flags)
        ^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.1264.0_x64__qbz5n2kfra8p0\Lib\re\_parser.py"", line 979, in parse
    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.1264.0_x64__qbz5n2kfra8p0\Lib\re\_parser.py"", line 460, in _parse_sub
    itemsappend(_parse(source, state, verbose, nested + 1,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.1264.0_x64__qbz5n2kfra8p0\Lib\re\_parser.py"", line 544, in _parse
    code = _escape(source, this, state)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.1264.0_x64__qbz5n2kfra8p0\Lib\re\_parser.py"", line 443, in _escape
    raise source.error(""bad escape %s"" % escape, len(escape))
re.error: bad escape \m at position 3
```

### Solution for first error

In line 190 of the file track.py (wherever you installed Zotify to, can use the program Everything to find easily on Windows):

Replace

`c = len([file for file in Path(filedir).iterdir() if re.search(f'^{filename}_', str(file))]) + 1`

with

`c = len([file for file in Path(filedir).iterdir() if re.search(fr'^{re.escape(str(filename))}_', str(file))]) + 1`

### Second error

```powershell
[WinError 183] Cannot create a file when that file already exists: '_path to audio file_' -> '_path to other audio file_'

Traceback (most recent call last):
    File ""_path to track.py_"", line 270, in download_track
        Path(filename_temp).rename(filename)
    File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.1264.0_x64__qbz5n2kfra8p0\Lib\pathlib.py"", line 1363, in rename
    os.rename(self, target)
FileExistsError: [WinError 183] Cannot create a file when that file already exists: '_path to audio file_' -> '_path to other audio file_'
```

### Solution for the second error

In line 269 of same the track.py file:

Replace

Path(filename_temp).rename(filename)

with

```python
if Path(filename).exists():
    # Decide how to handle the existing file
    # For example, delete it or rename filename_temp differently
    Path(filename).unlink()  # Example: delete existing file
    Path(filename_temp).rename(filename)  # Rename filename_temp to filename
else:
    Path(filename_temp).rename(filename)  # Rename filename_temp to filename
```"
184,679b3f3dcbe8284d7a72da24,issue,learning -> how to use a library/framework/tool,https://github.com/pgadmin-org/pgadmin4/issues/7673,"Column Visibility Discrepancy and Query Tool Malfunction in pgAdmin
Bug Description

**1. Column Visibility Discrepancy:**

Initially, I reported an issue where I could not see any columns listed under the Columns node in the Restaurants table.
Support staff member Yogesh presumed this was due to user error and instructed me to ensure I was selecting the correct node.
After providing multiple screenshots showing that I was indeed selecting the correct node, Yogesh finally provided a screenshot from his side, indicating he could see 29 columns in the same table.
This confirmed that the issue was not user error but a deeper problem, as I could not see any columns while Yogesh could see all 29.

**2. Query Tool Malfunction:**

Following the confirmation of the column visibility issue, I encountered a new problem with the Query Tool.
When attempting to open the Query Tool as Yogesh had directed me to do so, the following sequence occurred:

A revolving circle icon briefly appears.
This is followed by a flash.
The screen then closes, displaying the message: ""! No data output. Execute a query to get output.""

Despite following guidance from ChatGPT to enable Query Tool Logging and executing the correct steps, the issue persists.

Steps to Reproduce

Attempt to open the Query Tool.
Observe the revolving circle icon, followed by a flash and the error message.
Check the Columns node in the Restaurants table and note the absence of columns.
Expected Behavior
The Query Tool should open without errors, allowing queries to be executed.
Columns in the Restaurants table should be visible and accessible.
Troubleshooting Steps Taken
Attempted to enable Query Tool Logging as per ChatGPT's instructions but could not locate the Logging option.
Ensured that I was selecting the Restaurants table correctly, but the columns remain invisible.
Screenshot of Query Tool error is attached.

Additional Context
The support staff initially presumed user error for the column visibility issue.
After confirming that I was selecting the correct node, it was verified by Yogesh through a screenshot that he could see 29 columns while I could see none.
Following this, the Query Tool malfunction became apparent, further complicating the issue.
Yogesh has since shifted focus back to user error, disregarding the earlier confirmed problem.
Conclusion
Given these issues and the unproductive nature of current support interactions, I seek detailed guidance from a higher level of support. There appears to be a connection between the column visibility problem and the Query Tool malfunction. Reinstalling pgAdmin is a consideration, but given that my initial setup was correct, I am uncertain and very doubtful that this will resolve the issues.

Your assistance in resolving this matter would be greatly appreciated.

Best regards,

Reuben

The image screenshot files below don't seem to be working, which is yet another example of things not working as they should.

![2024-7-05-Screenshot](https://github.com/pgadmin-org/pgadmin4/assets/141807448/da834398-574c-4a4e-a525-90ca1ad96726)

![2024-7-06-Screenshot-3](https://github.com/pgadmin-org/pgadmin4/assets/141807448/d0343c94-f5dc-4569-91fd-dbc95d2c8b46)"
185,679b3fbfcbe8284d7a72da79,issue,learning -> how to use a library/framework/tool -> generating code examples,https://github.com/simonbengtsson/jsPDF-AutoTable/issues/1062,"I wanted to highlight some part of text in cell text. 
I wanted to highlight some part of text in cell text.tried some code of below link but i guess its not working in current version
[https://github.com/simonbengtsson/jsPDF-AutoTable/issues/744]
tried code from chatGPT getting text overlapped 
![Screenshot from 2024-07-10 13-07-46](https://github.com/simonbengtsson/jsPDF-AutoTable/assets/153071081/561c0624-17e8-4223-b46b-47a141cc4e87)
![Screenshot from 2024-07-10 13-07-46](https://github.com/simonbengtsson/jsPDF-AutoTable/assets/153071081/0d060164-dd37-4151-bc1d-5523b237dde1)

`
pdf.autoTable({
							html: '#mojaniTable',
							startY: 20,
							margin: { left: 20 + pdf.internal.pageSize.width - 40 - 80 },
							pageBreak: 'avoid',
							tableWidth: 80, styles: styles, bodyStyles: { lineWidth: 0.25, lineColor: 'black' },
							columnStyles: { lineWidth: 0.25, lineColor: 'black' },
							useCss: false,
							horizontalPageBreak: false,
							didParseCell: function(data) {
								if (data.cell && data.cell.raw) {
									// Parse the raw HTML content
									let htmlContent = data.cell.raw.innerHTML;

									// Remove unnecessary attributes from <font> tags
									htmlContent = htmlContent.replace(/<font [^>]*color=""red""[^>]*>(.*?)<\/font>/g, (match, p1) => {
										return `**${p1}**`; // Wrap the highlighted text with asterisks or any delimiter
									});
									// Remove any remaining HTML tags
									htmlContent = htmlContent.replace(/<[^>]+>/g, '');

									data.cell.text = [htmlContent];
								}
							},
							didDrawCell: function(data) {
								if (data.cell && data.cell.text[0]) {
									const text = data.cell.text[0];
									const parts = text.split('**'); // Split by delimiter

									if (parts.length > 1) {
										let cursor = { x: data.cell.x + data.cell.padding('left'), y: data.cell.y + 5 };

										for (let i = 0; i < parts.length; i++) {
											if (i % 2 === 1) {
												// Highlighted text
												pdf.setTextColor(255, 0, 0); // Red color
											} else {
												// Normal text
												pdf.setTextColor(0, 0, 0); // Black color
											}
											pdf.text(parts[i], cursor.x, cursor.y);
											cursor.x += pdf.getStringUnitWidth(parts[i]) * pdf.internal.getFontSize() / pdf.internal.scaleFactor;
										}

										data.cell.text = ['']; // Clear original text
									}
								}
							}

						});
`"
186,679b3fbfcbe8284d7a72daae,issue,documentation -> generating api specification,https://github.com/oblitum/Interception/issues/189,"Documentation draft
If I'm not mistaken there's no API documentation for Interception, which would be really helpful to have. I asked ChatGPT to write some documentation and I think it did a decent job. Maybe someone who knows the library well wants check this for correctness and completeness and then make it the official documentation? Thanks!

### Interception Library Documentation

#### Overview
The Interception library provides an interface for capturing and processing keyboard and mouse input at a low level on Windows. It allows applications to intercept, filter, and modify input events before they reach their intended destinations.

### API Reference

#### 1. **Structs**

##### 1.1 **InterceptionKeyStroke**
Represents a keyboard input event.

**Members:**
- `unsigned short code`: The key code of the keyboard event.
- `unsigned short state`: The state of the key (e.g., key down or key up).

##### 1.2 **InterceptionMouseStroke**
Represents a mouse input event.

**Members:**
- `unsigned short state`: The state of the mouse buttons.
- `unsigned short flags`: Flags representing additional state information.
- `short rolling`: The rolling delta for mouse wheel events.
- `int x`: The x-coordinate of the mouse cursor.
- `int y`: The y-coordinate of the mouse cursor.
- `unsigned int information`: Additional information about the mouse event.

##### 1.3 **InterceptionStroke**
A union representing an input event (either keyboard or mouse).

**Members:**
- `InterceptionKeyStroke key`: Represents a keyboard input event.
- `InterceptionMouseStroke mouse`: Represents a mouse input event.

#### 2. **Enums and Constants**

##### 2.1 **Key States**
Constants for `state` in `InterceptionKeyStroke`:

- `INTERCEPTION_KEY_DOWN`
- `INTERCEPTION_KEY_UP`
- `INTERCEPTION_KEY_E0`
- `INTERCEPTION_KEY_E1`
- `INTERCEPTION_KEY_TERMSRV_SET_LED`
- `INTERCEPTION_KEY_TERMSRV_SHADOW`

##### 2.2 **Mouse States**
Constants for `state` in `InterceptionMouseStroke`:

- `INTERCEPTION_MOUSE_LEFT_BUTTON_DOWN`
- `INTERCEPTION_MOUSE_LEFT_BUTTON_UP`
- `INTERCEPTION_MOUSE_RIGHT_BUTTON_DOWN`
- `INTERCEPTION_MOUSE_RIGHT_BUTTON_UP`
- `INTERCEPTION_MOUSE_MIDDLE_BUTTON_DOWN`
- `INTERCEPTION_MOUSE_MIDDLE_BUTTON_UP`
- `INTERCEPTION_MOUSE_BUTTON_4_DOWN`
- `INTERCEPTION_MOUSE_BUTTON_4_UP`
- `INTERCEPTION_MOUSE_BUTTON_5_DOWN`
- `INTERCEPTION_MOUSE_BUTTON_5_UP`
- `INTERCEPTION_MOUSE_WHEEL`
- `INTERCEPTION_MOUSE_HWHEEL`

##### 2.3 **Mouse Flags**
Constants for `flags` in `InterceptionMouseStroke`:

- `INTERCEPTION_MOUSE_MOVE_RELATIVE`
- `INTERCEPTION_MOUSE_MOVE_ABSOLUTE`
- `INTERCEPTION_MOUSE_VIRTUAL_DESKTOP`
- `INTERCEPTION_MOUSE_ATTRIBUTES_CHANGED`

#### 3. **Functions**

##### 3.1 **Context Management**

- **`InterceptionContext interception_create_context(void);`**
  - Creates and initializes a new Interception context.
  - **Returns:** A handle to the newly created context.
  - **Notes:** Must be destroyed using `interception_destroy_context`.

- **`void interception_destroy_context(InterceptionContext context);`**
  - Destroys an Interception context and releases its resources.
  - **Parameters:**
    - `context`: The Interception context to be destroyed.

##### 3.2 **Device Management**

- **`InterceptionDevice interception_wait(InterceptionContext context);`**
  - Waits for the next input event from any device.
  - **Parameters:**
    - `context`: The Interception context.
  - **Returns:** A handle to the device that generated the event.

- **`int interception_receive(InterceptionContext context, InterceptionDevice device, InterceptionStroke *stroke, unsigned int nstroke);`**
  - Receives input events from a specified device.
  - **Parameters:**
    - `context`: The Interception context.
    - `device`: The device handle.
    - `stroke`: Pointer to an array of `InterceptionStroke` structures to receive the events.
    - `nstroke`: The number of strokes to receive.
  - **Returns:** The number of strokes received.

- **`int interception_send(InterceptionContext context, InterceptionDevice device, const InterceptionStroke *stroke, unsigned int nstroke);`**
  - Sends input events to a specified device.
  - **Parameters:**
    - `context`: The Interception context.
    - `device`: The device handle.
    - `stroke`: Pointer to an array of `InterceptionStroke` structures containing the events to send.
    - `nstroke`: The number of strokes to send.
  - **Returns:** The number of strokes sent.

- **`unsigned int interception_get_hardware_id(InterceptionContext context, InterceptionDevice device, void *buffer, unsigned int buffer_size);`**
  - Retrieves the hardware ID of a specified device.
  - **Parameters:**
    - `context`: The Interception context.
    - `device`: The device handle.
    - `buffer`: Pointer to a buffer to receive the hardware ID.
    - `buffer_size`: The size of the buffer.
  - **Returns:** The size of the hardware ID string.

##### 3.3 **Filter Management**

- **`void interception_set_filter(InterceptionContext context, InterceptionPredicate predicate, InterceptionFilter filter);`**
  - Sets a filter for devices matching the given predicate.
  - **Parameters:**
    - `context`: The Interception context.
    - `predicate`: A function pointer to a predicate function to match devices.
    - `filter`: The filter to apply to matching devices.

- **`InterceptionFilter interception_get_filter(InterceptionContext context, InterceptionDevice device);`**
  - Retrieves the current filter for a specified device.
  - **Parameters:**
    - `context`: The Interception context.
    - `device`: The device handle.
  - **Returns:** The current filter for the specified device.

##### 3.4 **Predicate Functions**

- **`int interception_is_keyboard(InterceptionDevice device);`**
  - Checks if the device is a keyboard.
  - **Parameters:**
    - `device`: The device handle.
  - **Returns:** Non-zero if the device is a keyboard, zero otherwise.

- **`int interception_is_mouse(InterceptionDevice device);`**
  - Checks if the device is a mouse.
  - **Parameters:**
    - `device`: The device handle.
  - **Returns:** Non-zero if the device is a mouse, zero otherwise.

### Example Usage

```cpp
#include <iostream>
#include <windows.h>
#include ""interception.h""

void process_key_event(InterceptionKeyStroke& stroke) {
    std::cout << ""Key event: code = "" << stroke.code
              << "", state = "" << stroke.state << std::endl;
}

int main() {
    InterceptionContext context;
    InterceptionDevice device;
    InterceptionStroke stroke;

    context = interception_create_context();

    interception_set_filter(context, interception_is_keyboard, INTERCEPTION_FILTER_KEY_DOWN | INTERCEPTION_FILTER_KEY_UP);

    while (interception_receive(context, device = interception_wait(context), &stroke, 1) > 0) {
        if (interception_is_keyboard(device)) {
            InterceptionKeyStroke& keyStroke = *(InterceptionKeyStroke*)&stroke;
            
            // Process key event
            process_key_event(keyStroke);

            // Continue sending the stroke to the OS
            interception_send(context, device, &stroke, 1);
        }
    }

    interception_destroy_context(context);

    return 0;
}
```

### Notes:
- Ensure that the device ID used for filtering matches the specific keyboard or mouse you want to intercept.
- The Interception library operates at a low level and may require administrative privileges.
- Some special keys may require additional handling based on their unique key codes or state information."
187,679b4a5dcbe8284d7a72dfaf,issue,software quality -> fixing -> functional bugs,https://github.com/epwalsh/obsidian.nvim/issues/663,"Backlinks Not Recognized When Using Wikilinks with DisplayText/Alias and Without File Extension
### ðŸ› Describe the bug

#### Description

The plugin fails to recognize backlinks if the link uses different display text or an alias without including the file extension. This behavior differs from Obsidian, which correctly identifies backlinks in such cases.

#### Steps to Reproduce

1. Create a note, e.g., `NoteA.md`.
2. Create another note, e.g., `NoteB.md`, and link to `NoteA` using `[[NoteA|Display Text]]` or `[[NoteA#section|Display Text]]` (without the `.md` extension).
3. The backlink to `NoteA` is not recognized by the plugin.

#### Expected Behavior

The plugin should recognize backlinks even when the file extension is omitted and different display text or aliases are used, consistent with Obsidian's behavior.

#### Actual Behavior

The plugin only recognizes backlinks with display text or aliases when the file extension is included in the link.

| Link Type                     | Link to file Works | Backlinking Works | Notes                                   |
|-------------------------------|--------------------|-------------------|-----------------------------------------|
| `[[dir/filename.md]]`         | Yes                | Yes               | Full path with extension                |
| `[[filename.md]]`             | Yes                | Yes               | Filename with extension                 |
| `[[filename]]`                | Yes                | Yes               | Filename without extension              |
| `[[dir/filename.md\|alias]]`  | Yes                | Yes               | Full path with extension and alias      |
| `[[filename.md\|alias]]`      | Yes                | Yes               | Filename with extension and alias       |
| `[[filename\|alias]]`         | Yes                | No                | Filename without extension and alias    |

I also noticed that if a wikilink is used with just the filename without extension and using an alias, and the filename used in the link matches the id in the frontmatter of the linked note, the backlink works. but if using a random id then it doesnt work.

#### Proposed Solution

i asked chatgpt and it came up with following fix which actully works:

Update the backlink search functionality to include variations of the file name without the file extension in the search terms when using different display text or aliases. This would involve modifying the search terms generation to handle links both with and without the file extension.

```diff
 lua/obsidian/client.lua | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/lua/obsidian/client.lua b/lua/obsidian/client.lua
index ff6e500..8bbbd78 100644
--- a/lua/obsidian/client.lua
+++ b/lua/obsidian/client.lua
@@ -1301,7 +1301,8 @@ Client.find_backlinks_async = function(self, note, callback, opts)
 
   -- Prepare search terms.
   local search_terms = {}
-  for raw_ref in iter { tostring(note.id), note:fname(), self:vault_relative_path(note.path) } do
+  local filename_without_extension = note:fname():gsub(""%.%w+$"", """")
+  for raw_ref in iter { tostring(note.id), note:fname(), filename_without_extension, self:vault_relative_path(note.path) } do
     for ref in
       iter(util.tbl_unique {
         raw_ref,
```


### Config

```
  {
    ""epwalsh/obsidian.nvim"",
    version = ""*"",  -- recommended, use latest release instead of latest commit
    lazy = true,
    ft = ""markdown"",
    -- Replace the above line with this if you only want to load obsidian.nvim for markdown files in your vault:
    -- event = {
    --   -- If you want to use the home shortcut '~' here you need to call 'vim.fn.expand'.
    --   -- E.g. ""BufReadPre "" .. vim.fn.expand ""~"" .. ""/my-vault/**.md""
    --   ""BufReadPre path/to/my-vault/**.md"",
    --   ""BufNewFile path/to/my-vault/**.md"",
    -- },
    dependencies = {
      -- Required.
      ""nvim-lua/plenary.nvim"",

      -- see below for full list of optional dependencies ðŸ‘‡
    },
    opts = {
        -- A list of workspace names, paths, and configuration overrides.
        -- If you use the Obsidian app, the 'path' of a workspace should generally be
        -- your vault root (where the `.obsidian` folder is located).
        -- When obsidian.nvim is loaded by your plugin manager, it will automatically set
        -- the workspace to the first workspace in the list whose `path` is a parent of the
        -- current markdown file being edited.
        workspaces = {
          {
            name = ""notes"",
            path = ""~/notes"",
          },
          {
            name = ""apple-notes"",
            path = ""~/apple-notes"",
          },
          {
            name = ""test-vault"",
            path = ""~/obsidian-test-vault"",
          },
        },

        -- Alternatively - and for backwards compatibility - you can set 'dir' to a single path instead of
        -- 'workspaces'. For example:
        -- dir = ""~/vaults/work"",

        -- Optional, if you keep notes in a specific subdirectory of your vault.
        notes_subdir = ""0_uncathegorised_notes"",

        -- Optional, set the log level for obsidian.nvim. This is an integer corresponding to one of the log
        -- levels defined by ""vim.log.levels.*"".
        log_level = vim.log.levels.INFO,

        daily_notes = {
          -- Optional, if you keep daily notes in a separate directory.
          folder = ""notes-dailies"",
          -- Optional, if you want to change the date format for the ID of daily notes.
          date_format = ""%Y-%m-%d"",
          -- Optional, if you want to change the date format of the default alias of daily notes.
          alias_format = ""%B %-d, %Y"",
          -- Optional, default tags to add to each new daily note created.
          default_tags = { ""daily-notes"" },
          -- Optional, if you want to automatically insert a template from your template directory like 'daily.md'
          template = nil
        },

        -- Optional, completion of wiki links, local markdown links, and tags using nvim-cmp.
        completion = {
          -- Set to false to disable completion.
          nvim_cmp = true,
          -- Trigger completion at 2 chars.
          min_chars = 0,
        },

        -- Optional, configure key mappings. These are the defaults. If you don't want to set any keymappings this
        -- way then set 'mappings = {}'.
        mappings = {
          -- Overrides the 'gf' mapping to work on markdown/wiki links within your vault.
          [""gf""] = {
            action = function()
              return require(""obsidian"").util.gf_passthrough()
            end,
            opts = { noremap = false, expr = true, buffer = true },
          },
          -- Toggle check-boxes.
          [""<leader>oo""] = {
            action = function()
              return require(""obsidian"").util.toggle_checkbox()
            end,
            opts = { buffer = true },
          },
          -- -- Smart action depending on context, either follow link or toggle checkbox.
          -- [""<cr>""] = {
          --   action = function()
          --     return require(""obsidian"").util.smart_action()
          --   end,
          --   opts = { buffer = true, expr = true },
          -- }
        },

        -- Where to put new notes. Valid options are
        --  * ""current_dir"" - put new notes in same directory as the current buffer.
        --  * ""notes_subdir"" - put new notes in the default notes subdirectory.
        new_notes_location = ""notes_subdir"",



        -- -- Optional, customize how note IDs are generated given an optional title.
        -- ---@param title string|?
        -- ---@return string
        -- note_id_func = function(title)
        --   -- Create note IDs in a Zettelkasten format with a timestamp and a suffix.
        --   -- In this case a note with the title 'My new note' will be given an ID that looks
        --   -- like '1657296016-my-new-note', and therefore the file name '1657296016-my-new-note.md'
        --   local suffix = """"
        --   if title ~= nil then
        --     -- If title is given, transform it into valid file name.
        --     suffix = title:gsub("" "", ""-""):gsub(""[^A-Za-z0-9-]"", """"):lower()
        --   else
        --     -- If title is nil, just add 4 random uppercase letters to the suffix.
        --     for _ = 1, 4 do
        --       suffix = suffix .. string.char(math.random(65, 90))
        --     end
        --   end
        --   return tostring(os.time()) .. ""-"" .. suffix
        -- end,


        --
        -- Optional, customize how note file names are generated given the ID, target directory, and title.
        ---@param spec { id: string, dir: obsidian.Path, title: string|? }
        ---@return string|obsidian.Path The full path to the new note.
        note_path_func = function(spec)
          -- -- This is equivalent to the default behavior.
          -- local path = spec.dir / tostring(spec.id)
          -- return path:with_suffix("".md"")
          local ltitle = spec.title
          return ltitle .. "".md""
        end,

        -- -- TODO: change how wiki links are formatted to make it compatible with obsidian
        -- -- Optional, customize how wiki links are formatted. You can set this to one of:
        -- --  * ""use_alias_only"", e.g. '[[Foo Bar]]'
        -- --  * ""prepend_note_id"", e.g. '[[foo-bar|Foo Bar]]'
        -- --  * ""prepend_note_path"", e.g. '[[foo-bar.md|Foo Bar]]'
        -- --  * ""use_path_only"", e.g. '[[foo-bar.md]]'
        -- -- Or you can set it to a function that takes a table of options and returns a string, like this:
        -- wiki_link_func = function(opts)
        --   return require(""obsidian.util"").wiki_link_id_prefix(opts)
        -- end,
        wiki_link_func = ""use_path_only"",


-- function(opts)
--   ---@type string
--   local header_or_block = """"
--   if opts.anchor then
--     header_or_block = opts.anchor.anchor
--   elseif opts.block then
--     header_or_block = string.format(""#%s"", opts.block.id)
--   end
--   return string.format(""[[%s%s]]"", opts.path, header_or_block)
-- end


        -- NOTE: same for markdown
        -- -- Optional, customize how markdown links are formatted.
        -- markdown_link_func = function(opts)
        --   return require(""obsidian.util"").markdown_link(opts)
        -- end,

        -- Either 'wiki' or 'markdown'.
        preferred_link_style = ""wiki"",






        -- Optional, boolean or a function that takes a filename and returns a boolean.
        -- `true` indicates that you don't want obsidian.nvim to manage frontmatter.
        disable_frontmatter = false,



        -- Optional, for templates (see below).
        templates = {
          folder = ""1_templates"",
          date_format = ""%Y-%m-%d"",
          time_format = ""%H:%M"",
          -- A map for custom variables, the key should be the variable and the value a function
          substitutions = {
              yesterday = function()
                return os.date(""%Y-%m-%d"", os.time() - 86400)
              end,
              -- the default id substitution is broken in templates
              id = function()
                -- Create note IDs in a Zettelkasten format with a timestamp and a suffix.
                -- In this case a note with the title 'My new note' will be given an ID that looks
                -- like '1657296016-my-new-note', and therefore the file name '1657296016-my-new-note.md'
                local suffix = """"
                if title ~= nil then
                  -- If title is given, transform it into valid file name.
                  suffix = title:gsub("" "", ""-""):gsub(""[^A-Za-z0-9-]"", """"):lower()
                else
                  -- If title is nil, just add 4 random uppercase letters to the suffix.
                  for _ = 1, 4 do
                    suffix = suffix .. string.char(math.random(65, 90))
                  end
                end
                return tostring(os.time()) .. ""-"" .. suffix
              end,
          }
        },



        -- Optional, set to true if you use the Obsidian Advanced URI plugin.
        -- https://github.com/Vinzent03/obsidian-advanced-uri
        use_advanced_uri = false,


        -- Optional, set to true to force ':ObsidianOpen' to bring the app to the foreground.
        open_app_foreground = false,




        -- Set the maximum number of lines to read from notes on disk when performing certain searches.
        search_max_lines = 2000,





        -- Optional, configure additional syntax highlighting / extmarks.
        -- This requires you have `conceallevel` set to 1 or 2. See `:help conceallevel` for more details.
        ui = {
          enable = true,  -- set to false to disable all additional syntax features
          update_debounce = 200,  -- update delay after a text change (in milliseconds)
          max_file_length = 5000,  -- disable UI features for files with more than this many lines
          -- Define how various check-boxes are displayed
          checkboxes = {},

          -- Use bullet marks for non-checkbox lists.
          bullets = {}, -- NOTE: done by markdown.nvim

          external_link_icon = { char = ""ï‚Ž"", hl_group = ""ObsidianExtLinkIcon"" }, -- NOTE: need to set conceallevel=2 in markdown.nvim to be visible
          -- Replace the above with this if you don't have a patched font:
          -- external_link_icon = { char = """", hl_group = ""ObsidianExtLinkIcon"" },

          reference_text = { hl_group = ""ObsidianRefText"" },
          highlight_text = { hl_group = ""ObsidianHighlightText"" },

          tags = { hl_group = ""ObsidianTag"" },
          block_ids = { hl_group = ""ObsidianBlockID"" },

          hl_groups = {
            -- The options are passed directly to `vim.api.nvim_set_hl()`. See `:help nvim_set_hl`.
            ObsidianTodo = { bold = true, fg = ""#f78c6c"" },
            ObsidianDone = { bold = true, fg = ""#89ddff"" },
            ObsidianRightArrow = { bold = true, fg = ""#f78c6c"" },
            ObsidianTilde = { bold = true, fg = ""#ff5370"" },
            ObsidianImportant = { bold = true, fg = ""#d73128"" },
            ObsidianBullet = { bold = true, fg = ""#89ddff"" },
            ObsidianRefText = { underline = true, fg = ""#c792ea"" },
            ObsidianExtLinkIcon = { fg = ""#c792ea"" },
            ObsidianTag = { italic = true, fg = ""#89ddff"" },
            ObsidianBlockID = { italic = true, fg = ""#89ddff"" },
            ObsidianHighlightText = { bg = ""#75662e"" },
          },
        },
    }
  }

```


### Environment

```
NVIM v0.10.0
Build type: Release
LuaJIT 2.1.1716656478
Run ""nvim -V1 -v"" for more info

Error detected while processing /Users/name/.local/share/lunarvim/lvim/init.lua:
&termguicolors must be set
Obsidian.nvim v3.9.0 (2a401724579884d22bb584d06beba67aecb6a0ac)
Status:
  â€¢ buffer directory: nil
  â€¢ working directory: /Users/name
Dependencies:
  âœ“ plenary.nvim: a3e3bc82a3f95c5ed0d7201546d5d2c19b20d683
  âœ“ nvim-cmp: 5260e5e8ecadaf13e6b82cf867a909f54e15fd07
Integrations:
  âœ“ picker: TelescopePicker()
  âœ“ completion: enabled (nvim-cmp) âœ— refs, âœ— tags, âœ— new
    all sources:
      â€¢ copilot
      â€¢ nvim_lsp
      â€¢ path
      â€¢ luasnip
      â€¢ cmp_tabnine
      â€¢ nvim_lua
      â€¢ buffer
      â€¢ calc
      â€¢ emoji
      â€¢ treesitter
      â€¢ crates
      â€¢ tmux
Tools:
  âœ“ rg: ripgrep 14.1.0
Environment:
  â€¢ operating system: Darwin
Config:
  â€¢ notes_subdir: 0_uncathegorised_notes%         
```
"
188,679b4a5dcbe8284d7a72e0bf,issue,learning -> how to use a library/framework/tool,https://github.com/koekeishiya/yabai/issues/2365,"How can I set up window snapping to left, right, top, bottom of the screen? Like in Rectangle. ctrl+alt+(arrow)
I asked chatgpt but it doesnt work. "
189,679b4ad8cbe8284d7a72e16d,issue,software quality -> testing -> generating test data,https://github.com/az-digital/az_quickstart/issues/3576,"Add demo content to test media embed displays
## Motivation
We need more comprehensive demo content with embedded media so we can test the displays of in-line media.

### Is your feature request related to a problem? Please describe.
Yes, this issue is foundational work for #3439 

### Describe the solution you'd like
Add or adjust existing demo content to include more in-line media items (i.e. images, remote video).

## Additional context
I used ChatGPT to generate a press release using the prompt: â€œgenerate a fake press release about some medical breakthough at the University of Arizona"" The following is the fake press release it generated:

**FOR IMMEDIATE RELEASE**

**University of Arizona Unveils Pioneering Treatment for Type 1 Diabetes: A Game-Changer for Patients**

*Tucson, AZ â€“ July 31, 2024* â€“ The University of Arizona has announced a groundbreaking medical breakthrough in the treatment of Type 1 diabetes, unveiling a revolutionary new therapy that holds the potential to dramatically alter the management of this chronic condition. The innovative treatment, known as Insulix-21, has demonstrated remarkable success in early clinical trials, offering new hope to millions of individuals living with Type 1 diabetes.

Developed by the University of Arizona's Center for Endocrine Research, Insulix-21 is a novel therapy that combines cutting-edge stem cell technology with advanced immunomodulation techniques. This dual-action approach not only aims to restore the body's ability to produce insulin but also prevents the autoimmune response that attacks insulin-producing cells in the pancreas.

Dr. Maria Alvarez, the principal investigator and Professor of Endocrinology at the University of Arizona, elaborated on the breakthrough: ""Insulix-21 represents a significant advancement in Type 1 diabetes treatment. By leveraging stem cells to regenerate pancreatic beta cells and incorporating immunomodulatory agents to protect these cells from immune system attacks, we are addressing the root causes of the disease.""

In a recent clinical trial involving 120 participants, Insulix-21 demonstrated a 65% reduction in insulin dependence and a 50% improvement in glycemic control over a 12-month period. Patients also reported enhanced quality of life and reduced incidence of diabetes-related complications.

Dr. Alvarez expressed her excitement about the potential impact of Insulix-21: ""This breakthrough has the potential to not only manage Type 1 diabetes more effectively but also offer a path towards long-term remission for many patients. The early results are incredibly promising, and we are eager to continue our research and bring this transformative therapy to the wider community.""

The University of Arizona is now preparing for larger-scale trials and is in discussions with leading pharmaceutical companies to accelerate the development and availability of Insulix-21. Additionally, the university has established a comprehensive support program to assist patients and families in accessing this cutting-edge therapy and managing their diabetes effectively.

This achievement highlights the University of Arizona's dedication to advancing medical research and improving patient outcomes. The Insulix-21 team is optimistic about the potential of this new treatment to make a significant difference in the lives of individuals affected by Type 1 diabetes.

For further information about Insulix-21 and ongoing research at the University of Arizona, please contact:

**Press Contact:**

Rachel Thompson  
Director of Communications  
University of Arizona  
Phone: (520) 621-1986  
Email: rachel.thompson@arizona.edu  
Website: [www.arizona.edu/insulix21](http://www.arizona.edu/insulix21)

---

**About the University of Arizona:**

The University of Arizona is a leading institution in research, education, and community service. Committed to addressing global challenges through innovation and discovery, the university strives to enhance the well-being of individuals and communities worldwide.

**###**
"
190,679b4ad8cbe8284d7a72e18d,issue,development environment -> setting/implementing a new configuration file,https://github.com/premake/premake-core/issues/2231,"Issues Building Fortran Projects within workspace
**What are you trying to do?**
Trying to setup my premake 5.lua script to build an intel fortran project.

**What problem are you having?**
I'm getting the following Error: invalid value 'intel' for toolset

I was originally following along from [this](https://github.com/premake/premake-core/issues/1309) issue which solved the language problem with setting the language to Fortran. But now I'm not sure what to do about the toolset being set to intel. I can't find the UUID to do the same thing that was done with the language

**What have you tried so far?**
I've tried looking elsewhere, at public repos and asking chatGPT for assistance with my config. I have also tried manually searching through an already existing sln for this info that had the type of project that I'm trying to get my premake script to create

**What version of Premake are you using?**
premake-5.0.0-beta2

**Anything else we should know?**
My script is building a solution with with that will have a C++ SharedLib and a Fortran StaticLib. I also have the following in a .bat file that I'm not entirely sure how to make it work with the prior solution yet

```
premake5 --os=windows --file=BuildProjects.lua vs2022
premake5 --os=macosx --file=BuildProjects.lua gmake
premake5 --os=linux --file=BuildProjects.lua gmake
pause
```
"
191,679b4b52cbe8284d7a72e3f7,issue,generating/manipulating data,https://github.com/need4swede/Portall/issues/26,"Feature request: scheduled nmap scans
I have a couple of vms I use to run my docker containers. As I spin up/shut down containers, my ports may change. 
I propose the ability to schedule nmap scans and update known hosts accordingly. 

So far, I had good success with using chatgpt to convert nmap output to json input and using the import feature, but obviously this is a manual process. "
192,679b4cc8cbe8284d7a72e99b,issue,software quality -> fixing -> supporting debugging -> writing code to reproduce a bug,https://github.com/AdguardTeam/AdguardBrowserExtension/issues/2912,"The exception `domain=~` filter rule is not working properly.
### Please answer the following questions for yourself before submitting an issue

- [X] Filters were updated before reproducing an issue
- [X] I checked the [knowledge base](https://adguard.com/kb/) and found no answer
- [X] I checked to make sure that this issue has not already been filed

### AdGuard Extension version

v4.4.18

### Browser version

Chrome 128

### OS version

Windows 10

### Ad Blocking

AdGuard Base filter

### Privacy

AdGuard Tracking Protection filter, AdGuard URL Tracking filter

### Social

AdGuard Social Media filter

### Annoyances

AdGuard Annoyances filter

### Security

_No response_

### Other

_No response_

### Language-specific

AdGuard Turkish filter

### What Tracking protection options do you have enabled?

_No response_

### Issue Details

Steps to reproduce:
1. Add this filter rule to the User rule: 

```adblock
||chatango.com^$important,third-party,domain=~example.com
```
2. Go to example.com add this code to devtool console:

```
var chatangoWidget = document.createElement('iframe');
chatangoWidget.src = 'https://room-test.chatango.com/';
chatangoWidget.width = '300';
chatangoWidget.height = '400';
chatangoWidget.style.border = 'none';
document.body.appendChild(chatangoWidget);
```

(This code is from ChatGPT ðŸ˜„)

### Expected Behavior

Chatango is excluded

### Actual Behavior

Chatango is not excluded

### Video

<details><summary>Video</summary>

https://github.com/user-attachments/assets/9f75d2b2-998a-4b21-8a83-7b8cf62322e4

</details>


### Additional Information

_No response_"
193,679b4cc8cbe8284d7a72e9e7,issue,software quality -> fixing -> functional bugs,https://github.com/confluentinc/librdkafka/issues/4830,"librdkafka 2.2.0 uses system to run kinit and hence can receive SIGCHLD
Read the FAQ first: https://github.com/confluentinc/librdkafka/wiki/FAQ

Do NOT create issues for questions, use the discussion forum: https://github.com/confluentinc/librdkafka/discussions



Description
===========
When an application uses librdkafka and another library that uses SIGCHLD for its own purposes, SIGCHLD will not be ignored and cannot, must not be ignored. When the application uses kerberos with kafka then librdkafka will run the kinit command. It uses the system call to do this, see the function rd_kafka_sasl_cyrus_kinit_refresh. This means the calling thread will receive SIGCHLD. The code is assuming that the caller will be able to disable that signal. This assumption is not stated anywhere in the documentation. It is quite common to disable that signal. However, not every application can do so. Those that can't, ones that use SIGCHLD for reasons of their own, will get their handler invoked when the kinit command completes. This is certainly not the intention of librdkafka.

How to reproduce
================
I cannot reproduce any of the details of the application I am working on, since it is proprietary. All I can say is that I am using librdkafka 2.2.0 on RHEL8 and the app is configured to use kerberos authentication with the kinit command configured as the appropriate kafka parameter. The application is also using a proprietary library that uses SIGCHLD for its own purposes. The thread that uses librdkafka is started by that proprietary library so when SIGCHLD is received that library reports that SIGCHLD has been received. The app is a long-running app so its logfile contains many of these messages as the day goes on.

Checklist
=========
**IMPORTANT**: We will close issues where the checklist has not been completed.
Please provide the following information:

 - [x] librdkafka version (release number or git tag): `<REPLACE with e.g., v0.10.5 or a git sha. NOT ""latest"" or ""current"">`
 2.2.0 
 - [ ] Apache Kafka version: `<REPLACE with e.g., 0.10.2.3>`
 The one that comes with confluent 7. I think it is kafka 3.0.
 - [ ] librdkafka client configuration: `<REPLACE with e.g., message.timeout.ms=123, auto.reset.offset=earliest, ..>`
 kerberos authentication
 - [ ] Operating system: `<REPLACE with e.g., Centos 5 (x64)>`
 RHEL8

Suggested fix
============
Instead of calling system in rd_kafka_sasl_cyrus_kinit_refresh one could call a new function that librdkafka will have to implement. It would use the double-fork-technique such that kinit is run by a grandchild rather than a child. The grandchild can indicate completion to the parent using a standard unix/linux pipe. I asked chatGPT about this problem and it gave sample code for both aspects of the solution. They are reproduced in a simple example program below:

int main() {
    int pipefd[2];
    pipe(pipefd);

    pid_t pid = fork();
    if (pid == 0) {
        // First child process
        pid_t grandchild_pid = fork();
        if (grandchild_pid > 0) {
            // First child exits immediately
            exit(0);
        } else if (grandchild_pid == 0) {
            // Grandchild process
            close(pipefd[0]);  // Close the read end of the pipe
            // ... Perform some task here ...
            sleep(2); // Simulate some work

            // Notify parent of completion
            write(pipefd[1], ""done"", 4);
            close(pipefd[1]);  // Close the write end after writing
            exit(0);
        }
    } else if (pid > 0) {
        // Parent process
        waitpid(pid, NULL, 0);  // Wait for the first child

        // Now wait for the grandchild to complete
        char buffer[5] = {0};
        close(pipefd[1]);  // Close the write end of the pipe
        read(pipefd[0], buffer, 4);
        close(pipefd[0]);

        if (strncmp(buffer, ""done"", 4) == 0) {
            printf(""Grandchild has completed its task.\n"");
        }
    }

    return 0;
}
"
194,679b5897cbe8284d7a72f3f4,issue,learning -> how to use a library/framework/tool,https://github.com/nikitabobko/AeroSpace/issues/520,"Focused workspace not moving to assigned monitor
Hello guys, first of all thanks for this tool, this was literally what Iâ€™ve been looking for, thanks a lot.

I just found a behaviour, I might probably be setting up wrong on my config or something. Basically I have a workspace assigned to move to a specific monitor, with workspace-to-monitor-force-assignment and what happens is that, whenever I disconnect my monitor and I connect it again with the workspace I have assigned to force move, it will not move and will stay on the main monitor, even though I have it forced moved. If I have any other workspace focus on the moment I will connect the monitor again, then it works perfectly fine.

Am I missing some configuration or something?

thanks in advanceâ€¦

<!--
Consider including in bug reports:
- `aerospace debug-windows` output if applicable
- Screenshots of problematic windows if applicable
- Videos of problematic windows if applicable
- Your config
-->

## Checklist

<!-- Please fill in the checklist. You must have strong reasons not to do so -->

- [X] I've searched for duplicates. My search queries were (please don't forget to search in closed issues too):
  - workspace force assign 
  - workspace-to
  - Monitor move
  - workspace move
- [X] I've read the documentation https://nikitabobko.github.io/AeroSpace/guide & https://nikitabobko.github.io/AeroSpace/commands
- [X] I've searched in the documentation. My search queries were:
  The same than the duplicates, even used ChatGPT 

```
aerospace CLI client version: 0.14.2-Beta 0cb8dbdfc5ee73b8cbc200f175f467ebead55201
AeroSpace.app server version: 0.14.2-Beta Ocb8dbdfc5ee73b8cbc200f175f467ebead55201
```
"
195,679b5897cbe8284d7a72f446,issue,software quality -> fixing -> supporting debugging,https://github.com/AnasAito/SkillNER/issues/78,"issue with !python -m spacy download en_core_web_lg.
I downloaded skillNer using !pip install skillNer, as a second step, i am downloading spacy using !python -m spacy download en_core_web_lg in jupyter notebook.

but I am getting error as below.

Traceback (most recent call last):
  File ""<frozen runpy>"", line 189, in _run_module_as_main
  File ""<frozen runpy>"", line 148, in _get_module_details
  File ""<frozen runpy>"", line 112, in _get_module_details
  File ""C:\Users\sajja\anaconda3\Lib\site-packages\spacy\__init__.py"", line 13, in <module>
    from . import pipeline  # noqa: F401
    ^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\sajja\anaconda3\Lib\site-packages\spacy\pipeline\__init__.py"", line 1, in <module>
    from .attributeruler import AttributeRuler
  File ""C:\Users\sajja\anaconda3\Lib\site-packages\spacy\pipeline\attributeruler.py"", line 8, in <module>
    from ..language import Language
  File ""C:\Users\sajja\anaconda3\Lib\site-packages\spacy\language.py"", line 43, in <module>
    from .pipe_analysis import analyze_pipes, print_pipe_analysis, validate_attrs
  File ""C:\Users\sajja\anaconda3\Lib\site-packages\spacy\pipe_analysis.py"", line 6, in <module>
    from .tokens import Doc, Span, Token
  File ""C:\Users\sajja\anaconda3\Lib\site-packages\spacy\tokens\__init__.py"", line 1, in <module>
    from ._serialize import DocBin
  File ""C:\Users\sajja\anaconda3\Lib\site-packages\spacy\tokens\_serialize.py"", line 14, in <module>
    from ..vocab import Vocab
  File ""spacy\vocab.pyx"", line 1, in init spacy.vocab
  File ""spacy\tokens\doc.pyx"", line 49, in init spacy.tokens.doc
  File ""C:\Users\sajja\anaconda3\Lib\site-packages\spacy\schemas.py"", line 195, in <module>
    class TokenPatternString(BaseModel):
  File ""C:\Users\sajja\anaconda3\Lib\site-packages\pydantic\main.py"", line 286, in __new__
    cls.__try_update_forward_refs__()
  File ""C:\Users\sajja\anaconda3\Lib\site-packages\pydantic\main.py"", line 808, in __try_update_forward_refs__
    update_model_forward_refs(cls, cls.__fields__.values(), cls.__config__.json_encoders, localns, (NameError,))
  File ""C:\Users\sajja\anaconda3\Lib\site-packages\pydantic\typing.py"", line 554, in update_model_forward_refs
    update_field_forward_refs(f, globalns=globalns, localns=localns)
  File ""C:\Users\sajja\anaconda3\Lib\site-packages\pydantic\typing.py"", line 529, in update_field_forward_refs
    update_field_forward_refs(sub_f, globalns=globalns, localns=localns)
  File ""C:\Users\sajja\anaconda3\Lib\site-packages\pydantic\typing.py"", line 520, in update_field_forward_refs
    field.type_ = evaluate_forwardref(field.type_, globalns, localns or None)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\sajja\anaconda3\Lib\site-packages\pydantic\typing.py"", line 66, in evaluate_forwardref
    return cast(Any, type_)._evaluate(globalns, localns, set())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'

how to rectify it? when I asked chatGPT it said some version compatibility issues. could you please provide the version numbers for the packages used?"
196,679b592ccbe8284d7a72f5aa,issue,software quality -> fixing -> supporting debugging,https://github.com/thebrumby/HotWalletClaimer/issues/240,"Help Me: need detailed guide for linux. The present one has too many errors with help of chatgpt too almost spend a day but couln't run. Is it issue to the source or i am doing something wrog?
**Introduction:**
Before raising a help request, please ensure you've followed the necessary setup and troubleshooting steps. This will help us assist you more effectively and efficiently.

**Which self-help guides have you tried to follow? (required):**
Please list the guides or resources you've consulted so far. 

**Describe Your Issue (required):**
Please provide a clear and concise description of the help you need.

**Have you checked the [FAQ](https://github.com/thebrumby/HotWalletClaimer/wiki/FAQ) (required):**
Before submitting, please confirm that you've reviewed the FAQ linked above.

**Steps Taken So Far (optional):**
Describe what you have tried so far to resolve your issue.

**Additional Information:**
Include any other information that might help us assist you, such as screenshots, logs, or system details.
"
197,679b592ccbe8284d7a72f64f,issue,learning -> how to use a library/framework/tool,https://github.com/389ds/389-ds-base/issues/6344,"Does 389ds support sh\python plugins?
**Is your feature request related to a problem? Please describe.**
I need some custom password sync plugin for very special purpose. In short, I need username and cleartext password exported when user changes its password or when admin resets password.

**Describe the solution you'd like**
I asked ChatGPT about 389ds support of some kind of scripts as plugins and it gave me tons of variants about how to register python file as plugin or use ipa_pwd_extop as runner for external bash script. Nothing work.

**Additional context**
I just want to ask professionals - does 389ds support any kind of external `script` plugins? Please, provide me and example of creating one. Maybe there is a plugin that can run passthrough some operations to external script?
"
198,679b592ccbe8284d7a72f745,issue,software quality -> fixing -> supporting debugging,https://github.com/jamezmca/broodl/issues/3,"Got an error:  Error (auth/invalid-api-key).
i got this error while wrapping my layout using AuthProvider
firebase.js (21:27) @ app

  19 | const app = initializeApp(firebaseConfig);
  20 |
> 21 | export const auth=getAuth(app)
     |                           ^
  22 | export const db=getFirestore(app)
  
  Now my appl is not working.
  I copied the API key correctly and also made the .env file properly
  I used ChatGPT and it suggested using .env.local 
I am so confused."
199,679b592ccbe8284d7a72f749,issue,learning -> how to use a library/framework/tool,https://github.com/metavoiceio/metavoice-src/issues/196,"Colab Errors
Hi all!
I have been trying to work with this model since long. And very frustrated, as it is showing the same issue every time. I asked ChatGPT and the ClaudeAI, but they too are unable to solve. I literally tried 20-30 times, but the same issues.  As I'm new at coding, unable to solve. So thought to take help from you professional coder bros. Hope somebody will help soon.

So, I had run the colab and written down the my issues I'm facing, as BIG TEXT, 
for example,
>> CELL-1. NO ISSUES, and for each cell I wrote my problems.
Here is the link: https://colab.research.google.com/drive/12qKePkpfm7ubzcJtvS8Zycs9TITEjuof?usp=drive_link

So please somebody help me out, you can comment there in the colab. Or please reply here with cell number 
 for example as:
>> CELL-1. Here is what you can do..."
200,679b59c7cbe8284d7a72f80d,issue,process -> automating commit/issue/PR creation -> creating commit/issue/PR description -> motivating the need for a proposed change,https://github.com/hasanharman/form-builder/issues/2,"Security Issues on Source Code
Hi, while parsing the code I found that Google Analytics (GA) Ids are open even with Google Tag Manager ID (gtmId). It would be better if this is exported in a separate .env. As you have other environments.

ex: NEXT_PUBLIC_POSTHOG_KEY, NEXT_OPEN_PANEL_CLIENT_ID, etc.

It would be better if these Keys were not NEXT_PUBLIC, it violates the security part...

--- 

From ChatGPT when i'm asking why we won't use NEXT_PUBLIC for sensitive environment's

Variables prefixed with NEXT_PUBLIC are exposed to the client-side, making them accessible in the browserâ€™s developer tools. This can be a security risk if you are handling sensitive data such as API keys, tokens, or database credentials. Any sensitive or private information should remain server-side and not be exposed to the client.

---

More on this topic: https://stackoverflow.com/a/70766460"
201,679b59c7cbe8284d7a72f94a,issue,process -> automating commit/issue/pr creation -> translating commit/issue/pr description,https://github.com/CorgiTaco/Enhanced-Celestials/issues/220,"[Bug]: 
### Minecraft Version

1.19.2

### Enhanced Celestials Version

4.0.0.1

### Mod Loader

Forge

### Mod Loader Version

45.3.0

### Bug Severity

Major

### What happened?

Error executing the command /corgilib worldRegistryExport in Minecraft 1.19.4

I am experiencing an issue when trying to use the command /corgilib worldRegistryExport on a Minecraft 1.19.4 server. Despite having CorgiLib and Enhanced Celestials installed, when I execute the command, I receive an error message indicating:

[19:04:51] [Server thread/INFO]: Unknown or incomplete command, see below for error
[19:04:51] [Server thread/INFO]: corgilib worldRegistryExport <--[HERE]

I am entering the command because when I try to access the server(The server is in Aternos) I created, the required datapacks, enhancedcelestials/event and enhancedcelestials/dimension_settings, are missing. I investigated and found that these could be created with the above command, but I am encountering the error mentioned earlier.
(I had to use ChatGPT For the translation)

### latest.log

https://mclo.gs/6W43zGr"
202,679b59c7cbe8284d7a72f97b,issue,software quality -> fixing,https://github.com/comfyanonymous/ComfyUI/issues/5129,"TypeError: Cannot handle this data type: (1, 1, 5), |u1
### Your question

Hi all, I'm currently trying to get ComfyUI started for a project, but I'm quite a newbie at programming and bug fixing. I am facing this error report as shown below, it is my attempt in getting TripoSR (2D image to 3D object) running on ComfyUI. I have tried asking ChatGPT for guidance on this but the solutions were helping. Looking forward to receiving feedback from the community on ways this issue can be tackled. Thank you for your time and patience!


# ComfyUI Error Report
## Error Details
- **Node Type:** TripoSRSampler
- **Exception Type:** TypeError
- **Exception Message:** Cannot handle this data type: (1, 1, 5), |u1
## Stack Trace
```
  File ""D:\ComfyUI_Portable\ComfyUI_windows_portable_nvidia\ComfyUI_windows_portable\ComfyUI\execution.py"", line 317, in execute
    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File ""D:\ComfyUI_Portable\ComfyUI_windows_portable_nvidia\ComfyUI_windows_portable\ComfyUI\execution.py"", line 192, in get_output_data
    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File ""D:\ComfyUI_Portable\ComfyUI_windows_portable_nvidia\ComfyUI_windows_portable\ComfyUI\execution.py"", line 169, in _map_node_over_list
    process_inputs(input_dict, i)

  File ""D:\ComfyUI_Portable\ComfyUI_windows_portable_nvidia\ComfyUI_windows_portable\ComfyUI\execution.py"", line 158, in process_inputs
    results.append(getattr(obj, func)(**inputs))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File ""D:\ComfyUI_Portable\ComfyUI_windows_portable_nvidia\ComfyUI_windows_portable\ComfyUI\custom_nodes\ComfyUI-Flowty-TripoSR\__init__.py"", line 92, in sample
    image = Image.fromarray(np.clip(255. * image, 0, 255).astype(np.uint8))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File ""D:\ComfyUI_Portable\ComfyUI_windows_portable_nvidia\ComfyUI_windows_portable\python_embeded\Lib\site-packages\PIL\Image.py"", line 3098, in fromarray
    raise TypeError(msg) from e


### Logs

_No response_

### Other

_No response_"
203,679b59c7cbe8284d7a72f9e8,issue,software quality -> fixing -> functional bugs,https://github.com/martabal/qbittorrent-exporter/issues/77,"cannot unmarshal integers
Hello,

I'm running qbittorrent-exporter on rpi with os as you can see the details below.

> rpi@rpi:~/logs $ lsb_release -a
> No LSB modules are available.
> Distributor ID: Debian
> Description:    Debian GNU/Linux 12 (bookworm)
> Release:        12
> Codename:       bookworm
> rpi@rpi:~/logs $ uname -a
> Linux rpi 6.6.51+rpt-rpi-2712 #1 SMP PREEMPT Debian 1:6.6.51-1+rpt2 (2024-10-01) aarch64 GNU/Linux


while running exporter I had integer errors such as 

```
[2024-10-03 13:06:23] DEBUG json: cannot unmarshal number 171526531416 into Go struct field .server_state.dl_info_data of type int
[2024-10-03 13:06:23] ERROR Can not unmarshal JSON for maindata
```

```
[2024-10-03 13:06:08] DEBUG json: cannot unmarshal number 14989532172 into Go struct field .downloaded of type int
[2024-10-03 13:06:08] ERROR Can not unmarshal JSON for info
```


I think the reason was my torrents download size was too big for int type.

than I edited qbittorent.go files and changed those variables from int to int64,

```
Downloaded        int64     `json:""downloaded""`
DownloadedSession int64     `json:""downloaded_session""`
Size              int64    `json:""size""`
Uploaded          int64     `json:""uploaded""`
UploadedSession   int64     `json:""uploaded_session""`
AlltimeDl         int64 `json:""alltime_dl""`
AlltimeUl         int64 `json:""alltime_ul""`
DlInfoData        int64    `json:""dl_info_data""`
UpInfoData        int64    `json:""up_info_data""`
```

i also edited prometheus.go and changed the conversions for related variables.

```
		infoLabels := prometheus.Labels{
			""name"":               torrent.Name,
			""category"":           torrent.Category,
			""state"":              torrent.State,
			""size"":               strconv.FormatInt(torrent.Size, 10),
			""progress"":           strconv.Itoa(int(torrent.Progress)),
			""seeders"":            strconv.Itoa(torrent.NumSeeds),
			""leechers"":           strconv.Itoa(torrent.NumLeechs),
			""dl_speed"":           strconv.Itoa(torrent.Dlspeed),
			""up_speed"":           strconv.Itoa(torrent.Upspeed),
 			""amount_left"":        strconv.FormatInt(torrent.AmountLeft, 10), // DÃ¶nÃ¼ÅŸtÃ¼rÃ¼len kÄ±sÄ±m
			""time_active"":        strconv.Itoa(torrent.TimeActive),
			""eta"":                strconv.Itoa(torrent.Eta),
		    ""uploaded"":           strconv.FormatInt(torrent.Uploaded, 10), // DÃ¶nÃ¼ÅŸtÃ¼rÃ¼len kÄ±sÄ±m
		    ""uploaded_session"":   strconv.FormatInt(torrent.UploadedSession, 10), // DÃ¶nÃ¼ÅŸtÃ¼rÃ¼len kÄ±sÄ±m
		    ""downloaded"":         strconv.FormatInt(torrent.Downloaded, 10), // DÃ¶nÃ¼ÅŸtÃ¼rÃ¼len kÄ±sÄ±m
		    ""downloaded_session"": strconv.FormatInt(torrent.DownloadedSession, 10), // DÃ¶nÃ¼ÅŸtÃ¼rÃ¼len kÄ±sÄ±m
			""max_ratio"":          strconv.FormatFloat(torrent.MaxRatio, 'f', 3, 64),
			""ratio"":              strconv.FormatFloat(torrent.Ratio, 'f', 3, 64),
			""tracker"":            torrent.Tracker,
		}
```


after these changes and I could run exporter.

I'm not a professional developer so I could not create a pull request for you. I jus did all changes with the help of chatgpt :)
 Ä± just want to inform the issue. maybe you can make the necessary changes in source code. thanks."
204,679b5b14cbe8284d7a72fe97,issue,development environment -> continuous integration -> generating/updating docker container,https://github.com/meshtastic/network-management-client/issues/472,"[Feature Request]: Docker support
I managed to put together a Dockerfile that does build but stops on some Tauri error. I haven't run across Tauti before and have a tough time proceeding further. I did some troubleshooting with the help of ChatGPT but got stuck on some Tauri configuration files .


The Dockerfile I've created

```
FROM node:18-slim

# Install system dependencies required by Tauri and Rust
RUN apt-get update && \
    apt-get install -y curl build-essential libssl-dev libwebkit2gtk-4.0-dev libgtk-3-dev \
    libayatana-appindicator3-dev librsvg2-dev && \
    rm -rf /var/lib/apt/lists/*

# Install Rust and Cargo
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
ENV PATH=""/root/.cargo/bin:${PATH}""

# Install the Tauri CLI using Cargo
RUN cargo install tauri-cli && \
    echo ""Tauri installed, checking version..."" && \
    ls /root/.cargo/bin && \
    /root/.cargo/bin/cargo-tauri --version

# Install PNPM globally
RUN npm install -g pnpm

# Set the working directory in the container
WORKDIR /usr/src/app

# Copy the package.json and pnpm-lock.yaml to the container
COPY package.json pnpm-lock.yaml ./

# Install dependencies using PNPM
RUN pnpm install

# Copy the rest of the application code to the container
COPY . .

# Build the frontend using Vite
RUN pnpm run ui:build

# Expose the port on which the application will run
EXPOSE 5173

# Set the environment variable for running in development mode
ENV NODE_ENV=development

# Command to run the Rust development server using PNPM
CMD [""cargo-tauri"", ""dev"", ""--"", ""--no-default-features""]
```

And these are the error messages when I run the Docker

```
docker run  meshtastic-network-client
    Error `tauri.conf.json` error: ""identifier"" is a required property
    Error `tauri.conf.json` error on `build`: Additional properties are not allowed ('devPath', 'distDir' were unexpected)
    Error `tauri.conf.json` error: Additional properties are not allowed ('package', 'tauri' were unexpected
```)"
205,679b5ba1cbe8284d7a73005e,issue,generating/manipulating data -> ui translations,https://github.com/writingtool-org/writingtool/issues/10,"WritingTool: translation of description â€” 2024-10-17
Heya, @FredKruse 

Here is the translation of the LibreOffice extension.

I used ChatGPT for it, so I can only assure that Portuguese Brazilian and Portuguese European are correct:

Portuguese European:
```
WritingTool, o seu assistente de escrita.
Baseado no LanguageTool (verificador de gramÃ¡tica, estilo e ortografia para mais de 25 idiomas).
```

Portuguese Brazilian:
```
WritingTool, seu assistente de escrita.
Baseado no LanguageTool (verificador de gramÃ¡tica, estilo e ortografia para mais de 25 idiomas).
```

Spanish European:
```
WritingTool, tu asistente de escritura.
Basado en LanguageTool (corrector de gramÃ¡tica, estilo y ortografÃ­a para mÃ¡s de 25 idiomas).
```

French European:
```
WritingTool, votre assistant d'Ã©criture.
BasÃ© sur LanguageTool (correcteur de grammaire, style et orthographe pour plus de 25 langues).
```


"
206,679b5ba1cbe8284d7a7300e2,issue,learning->brainstorming,https://github.com/dome272/Diffusion-Models-pytorch/issues/45,"Is the pos_encoding function in modules.py correct?
The description of the video links to [this](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/) article for a description of the positional encoding for timesteps. However, i've noticed a difference between how the function is implemented in this repo, and the definition in the article. In the article, the sin and cos terms are interleaved such that indexes corresponding to 2i are sin, and indexes corresponding to 2i + 1 are cos. In the code, afaict instead the cos terms are just appended to the sin terms rather than interleaved.

I figure maybe this doesn't matter and that the neural network / training process is indifferent to which position in the vector different terms are in? But i asked ChatGPT and it seemed to think it mattered a lot, so I thought it was at least worth an ask."
207,679b5ba1cbe8284d7a7301b0,issue,software quality -> fixing -> supporting debugging -> writing code to reproduce a bug,https://github.com/ProjectEvergreen/wcc/issues/170,"Issue Setting Element Properties
## Type of Change
Enhancement/Bug

## Summary
First of all, I just want to say what you're doing here is fantastic... this is awesome! I've got wcc working with SSR for completely native web components, using a custom renderer or lit-ssr, both with working client-side hydration.

My main issue is when setting properties using a custom renderer or lit-html's render (not LitElement) with template literals, element properties in rendered templates aren't passed down to child elements. This happens with both the light dom and shadow dom.

## Why is it happening?

I'm sure I'm missing a lot here, but I think this is happening because wcc is using .innerHTML instead of .appendChild/.childNodes when adding/retrieving child nodes which causes all element properties to be discarded. I notice the dom-shim for appendChild also uses innerHTML. Is there a way to shim appendChild that retains the element's properties? This is just a guess, but I believe that by using appendChild/childNodes with a true appendChild shim instead of innerHTML, you can retain all of the properties set by renderers like lit-ssr or custom renderers like the one below. It looks like with parse 5, you can just use childNodes.push to append children. Additionally, I put some chatGPT generated shims for appendChild, removeChild, etc. at the bottom of this request in case it's any help.

**This would be huge for web components because you could SSR or SSG entire intricate render templates, passing down complex objects, maps, sets, etc. to child components, rendering the initial state on the server, without setting dozens of attributes. This would allow for creating really interesting, reusable light dom and shadow dom web component systems that allow for a complex initial render that can be cached for performance but that become deeply reactive when the client side renderer hydrates the templates. It also makes the idea of complex, _isomorphic_, vanilla web components a reality which may seem impossible otherwise.**

Studying the wcc codebase, I'm sure this goes deeper than I understand but man, this would be really powerful. I'm hoping you can fill in the gaps and see a way to make this happen!

## Details
Here is a custom ssr renderer I'm using (with everything not related to this issue removed) but this can be tested with lit-ssr's render as well:

```javascript
import { parse } from 'node-html-parser';

const isFalsy = (str) => ['false', 'null', 'undefined', '0', '-0', 'NaN', '0n', '-0n'].includes(str);

export const render = (content, container, deps) => {
  const parsedContent = parse(content());
  const elements = parsedContent.getElementsByTagName('*');

  elements.forEach((element) => {
    const attributes = element.attributes;

    Object.entries(attributes).forEach(([attribute, value]) => {
      if (attribute.startsWith('.')) {
        const propName = attribute.substring(1);
        element.removeAttribute(attribute);
        element[propName] = deps?.[value] ?? value;
      }
    });
  });

  // This is using the wcc appendChild shim which I'm sure is part of the 
  // problem here since it just uses innerHTML
  if (container.shadowRoot) {
    const template = document.createElement('template');
    template.appendChild(parsedContent);
    container.shadowRoot.appendChild(template.content.cloneNode(true));
  } else {
    container.appendChild(parsedContent);
  }
};
```

Example components:

```javascript
customElements.define(
  'parent-element',
  class extends HTMLElement {
    connectedCallback() {
      const item = { message: 'Hello World' };
      this.test = 'This works';
      render(() => `<child-element .message=""${item.message}""></child-element><p>${this.test}</p>`, this);
    }
  }
);

customElements.define(
  'child-element',
  class extends HTMLElement {
    connectedCallback() {
      render(() => `<p>${this.message}</p>`, this);
    }
  }
);

```

Here's how this renderer works passing down an object. Again, this doesn't work server side with wcc, but works once hydration kicks in:

```javascript
customElements.define(
  'parent-element',
  class extends HTMLElement {
    connectedCallback() {
      const item = { message: 'Hello World' };
      this.test = 'This works';
      render(() => `<child-element .item=""item""></child-element><p>${this.test}</p>`, this, { item });
    }
  }
);

customElements.define(
  'child-element',
  class extends HTMLElement {
    connectedCallback() {
      render(() => `<p>${this.item?.message}</p>`, this);
    }
  }
);
```

## What do I expect?

I would expect the paragraph tag to display ""Hello World"" on the server side render in both scenarios. The console.log in the render function shows that the element's message property is successfully being set server side.


## What actually happens?

The paragraph element in child-element displays ""undefined"" 

With the above components, ""This works"" does render server side with wcc which shows that element properties are renderable, they're just being lost somewhere when being passed to child elements.

Once the client renderer kicks in and the component is hydrated, things display as you would expect (""Hello World"").

What are your thoughts? Thank you in advance!

## Some chatGPT shims
Btw, here are some quick chatGPT shims for parentNode, childNodes, appendChild, and removeChild if it's any help:

```javascript
// parentNode shim
if (!Element.prototype.parentNode) {
  Object.defineProperty(Element.prototype, 'parentNode', {
    get: function () {
      return this._parentNode || null;
    },
    set: function (newParent) {
      this._parentNode = newParent;
    },
    configurable: true
  });
}

// childNodes shim
if (!Element.prototype.childNodes) {
  Object.defineProperty(Element.prototype, 'childNodes', {
    get: function () {
      this._childNodes = this._childNodes || [];
      return this._childNodes;
    },
    configurable: true
  });
}

// appendChild shim
if (!Element.prototype.appendChild) {
  Element.prototype.appendChild = function (child) {
    if (child.parentNode) {
      child.parentNode.removeChild(child); // Remove from current parent
    }

    // Ensure childNodes exists
    this.childNodes = this.childNodes || [];

    // Add child to childNodes array
    this.childNodes.push(child);

    // Update the child's parentNode
    child.parentNode = this;

    return child;
  };
}

// removeChild shim
if (!Element.prototype.removeChild) {
  Element.prototype.removeChild = function (child) {
    if (!this.childNodes || !this.childNodes.length) return null;

    const index = this.childNodes.indexOf(child);
    if (index === -1) return null;

    // Remove the child from the array
    this.childNodes.splice(index, 1);

    // Clear the child's parentNode reference
    child.parentNode = null;

    return child;
  };
}
```
"
208,679b5c36cbe8284d7a730307,issue,software quality -> fixing -> functional bugs,https://github.com/godotjs/GodotJS/issues/14,"compilation troublesome when using MinGW
Hello,

For technical reasons, I need to build using MinGW instead of MSVC. But compiling with MinGW instead of MSVC is not going ""smoothly"". Not being sure I could build the v8 libs against the target, I opted to use QuickJS which did not generate anything other than warnings once those couple of issues where ""patched"".

for example, in `weaver-editor/jsb_editor_macros.h`, MinGW doesn't like the `'` char in the #error statement
```
#error CAN NOT COMPILE WITHOUT TOOLS_ENABLED, PLEASE CHECK IT'S NOT UNEXPECTEDLY INCLUDED.
```

or in `internal/jsb_macros.h`, you might want to replace `#   define jsb_ensure(Condition) (void) (Condition)` with `#   define jsb_ensure(Condition) ((Condition) ? true : false)` (suggestion from ChatGPT when giving it the error message).

```
In file included from modules\GodotJS\impl\quickjs\jsb_quickjs_broker.cpp:2:
modules\GodotJS\impl\quickjs/jsb_quickjs_isolate.h:241:17: error: value of type 'void' is not contextually convertible to 'bool'
  241 |             if (jsb_ensure(it) && --it->value.watcher_ == 0)
      |                 ^~~~~~~~~~~~~~
modules\GodotJS\impl\quickjs/../../internal/jsb_macros.h:65:34: note: expanded from macro 'jsb_ensure'
   65 | #   define jsb_ensure(Condition) (void) (Condition)
      |                                  ^~~~~~~~~~~~~~~~~~
In file included from modules\GodotJS\impl\quickjs\jsb_quickjs_broker.cpp:2:
modules\GodotJS\impl\quickjs/jsb_quickjs_isolate.h:241:32: error: invalid operands to binary expression ('void' and 'bool')
  241 |             if (jsb_ensure(it) && --it->value.watcher_ == 0)
      |                 ~~~~~~~~~~~~
```

in `internal/jsb_sarrays.h`, I had to add:
```cpp
#include <cstddef>
-            TScopedPointer(nullptr_t) : container_(nullptr), ptr_(nullptr) { }
+            TScopedPointer(std::nullptr_t) : container_(nullptr), ptr_(nullptr) { }
-            TScopedPointer& operator=(nullptr_t)
+            TScopedPointer& operator=(std::nullptr_t)
```"
209,679b5c36cbe8284d7a730439,issue,feature implementation/enhancement -> feature enhancement,https://github.com/voidtrance/voron-klipper-extensions/issues/15,"[FEATURE]Multiple settling samples
**Is your feature request related to a problem? Please describe.**
As described [here](https://github.com/voidtrance/voron-klipper-extensions/discussions/6) my printer configuration (bed, gantry, whatever) sometimes needs more than 1 settling sample 

**Describe the solution you'd like**
it would be great to be able to pass a parameter for the number of settling samples to the script.

**Describe alternatives you've considered**
I've considered just modify the script myself so on my printer it does 2 settling samples instead of 1.

**Additional context**
Since I can't write python code, I've asked ChatGPT for the changes I would have to do. It said to add this line:

`self.settling_sample_count = config.getint('settling_sample_count', 1) if self.settling_sample else 0`

to SettlingProbeSessionHelper.__init__

and

`for i in range(self.settling_sample_count):
    gcmd.respond_info(f""Performing settling sample {i + 1} of {self.settling_sample_count}..."")
    pos = self._probe(speed)
    toolhead.manual_move(probexy + [pos[2] + sample_retract_dist], lift_speed)`

to _run_settling_probe.

What do you think?


"
210,679b666ecbe8284d7a7305b0,issue,process -> automating commit/issue/PR creation -> creating commit/issue/PR description,https://github.com/microsoft/vscode-js-debug/issues/2118,"module.register() will hang when debugging
My English is not very good, so Iâ€™m using ChatGPT for translation. I apologize for any formatting errors or other issues.

**Describe the bug**
Debugging a JavaScript file which leverages [module.register](https://nodejs.org/api/module.html#moduleregisterspecifier-parenturl-options) does end up in a infinite hang.
This issue is the same as[issue2009](https://github.com/microsoft/vscode-js-debug/issues/2009), which was closed due to lack of a reproduction method. I have found a stable way to reproduce the issue.

**To Reproduce**
```js
//å…¥å£.js
import { register } from 'module'

console.log('print something');
register('anything') // hang up if has break point in this file
console.log('set break point here');
```
1. In VS Code, set a breakpoint on line 6 of`å…¥å£.js`
2. In the VS Code terminal (I'm using cmd), run `node --inspect ./å…¥å£.js`

**Log File**
N/A

**VS Code Version:** 1.94.2

**Additional context**


The key to reproducing this issue is **having Chinese characters in the file path of the file containing the breakpoint**,However, it seems that the length of the Chinese characters matters. For example,if `å…¥å£.js` is replaced with`å…¥.js`or`å£.js`, it outputs nothing and simply exits.

If the filename is in another language, such as `ãƒ†ã‚¹ãƒˆ.js` (Japanese), it also outputs nothing.

I tested this on two versions of Node (`v22.11.0` and `v18.20.4`), and the issue exists on both.
"
211,679b666ecbe8284d7a7306e2,issue,software quality -> fixing -> functional bugs,https://github.com/ProjectSidewalk/SidewalkWebpage/issues/3714,"Unified login should share authentication cookie between servers
##### Brief description of problem/feature
In #3712 we added unified login, so one account now works to log in to each city. But you still have to actually log in in each city, because they each create a session cookie that isn't shared (because the domain names are different). 

##### Potential solution(s)
I asked ChatGPT about this, and it sounds like option 2 is what we're looking for:
![Screenshot from 2024-10-31 13-13-54](https://github.com/user-attachments/assets/20f9d3c5-b006-43dd-8dff-3060d2310539)
"
212,679b666ecbe8284d7a73073e,issue,"feature implementation/enhancement -> feature enhancement,feature implementation/enhancement -> prototyping",https://github.com/zyedidia/micro/issues/3529,"Copilot for micro [wip]
Hi. 
I started 2 days ago to check micro (never hear about it before) because i wanted to prepare my android phone to perform some small development.
I came across micro with plugin filemanager which is the unique (i think) text editor with a file picker and also split views. 
Immediately felt the need to have copilot as in my daily setup.

I created this plugin which is an initial idea.

https://gist.github.com/iGio90/f6f7c60bad824ae469cf65f9280ee493

the logic is: 
1) when a new character is added and its placed at the end of the line, we build a context which consist of all function signatures from opened files + the whole function body of the current function we are in
2) we push the context to copilot and get the response
3) the completion suggestion is added in the so called info bar
4) there is a key binding to add the result straight


This code has been built by 99% from chatgpt as i never wrote a line of lua in my entire life. (We can discuss in other channels about the choice of using this language xD)
Anyway, im not really happy about it. I asked chatgpt to make it async, it gave me some output which doesn't look to work, but its an initial solution.

Im sharing the code on a gist in the hope that someone with better experience with lua can jump in and help with the development.

How to use it, since currently the whole login part is missing.
1) pull this repo https://github.com/B00TK1D/copilot-api
2) run the ``api.py`` and follow instruction to login
3) after login, a file named ``.copilot_token`` is created in the same dir
4) create a folder ``~/.config/micro/plug/copilot`` and move inside the .copilot_token and the content of my gist saved to copilot.lua
5) download https://github.com/rxi/json.lua/blob/master/json.lua and save it as well to ``~/.config/micro/plug/copilot``

the plugin will perform the api call through curl (chatgpt told me there is no proper way with lua to perform http request so it just used shell + curl) on each rune added at the end of line, it will display the hint and give you ability to apply it


"
213,679b666ecbe8284d7a730789,issue,feature implementation/enhancement -> prototyping -> implementing proof of concept,https://github.com/cornellh4i/ithaca-recovery/issues/79,"Daily Calendar View (fullstack)
Hi Brandon and Tanvi!

**branch name:** _same branch as your previous ticket_

For this week, we want you to integrate your daily calendar view that you worked on last week with the backend. There are a lot of moving parts in this sprint, so please read the entire ticket carefully before starting!

**Task:**

- To start, we want to add some additional props inside `frontend/app/components/atoms/BoxText/index.tsx`. Add a prop named `meetingId` of type `string` and a prop named 'onClick' of type `(meetingId: string) => void`. Under the return for the BoxText within the parent `div`, add an attribute `onClick={() => onClick(meetingId)}`. These changes will eventually become very important once we make the blocks clickable, as we'll need to be able to display all the expanded details for each meeting. _Note that there may be other parts of your code that still assume an old implementation of the BoxText component, so you may want to fix those errors as they come up._ 

Now, it's time to connect to the backend. Implement your work inside of your  `/organisms/DailyView/index.tsx` component.
- By default, the view should display all of the meetings for the **_current day_**, as determined by the user's machine. You should make use of the React `useState` hook to manage the selected day. It will also be a good idea to use `useState` to store the meetings for the given day. 
- Implement a function `fetchMeetingsByDay` which will call the API endpoint for getting all the meetings by the selected day. An example of this API call is included inside of `frontend/app/test/page.tsx`.
- You'll want to implement a function such as `handleDateChange` to manage when the date changes. Upon this happening, you should call `fetchMeetingsByDay` and update the state for the meetings and the day. You also want to make use of the React `useEffect` hook to call `handleDateChange` upon the initial render (so that the Calendar does not appear empty by default).
- Eventually, the user should be able to cycle forward and backward through the dates freely. However, notice that this will lead to many redundant API calls if they are viewing the same day multiple times. Therefore, we want to optimize our system by utilizing **_caching_**. You should initialize some sort of cache like `const meetingCache = new Map();`. Anytime `fetchMeetingsByDay` is called, it should first look into the cache to check if the meetings for the given date have previously been loaded and stored in the cache. If so, it should immediately return those. Otherwise, it should make the API call as necessary and store the meetings in the cache before returning.
- For now, do not worry about how to actually cycle through the dates. That will be for a future ticket.
- I know that was a lot of info, so to help get you started, I've put some generic starter code generated by ChatGPT at the bottom of this ticket. You can use this as a starting point or as clarification in case anything I described above was confusing (but be cautious about copying things directly, particularly for the API call). _Do NOT worry_ about `handlePreviousDay` or `handleNextDay` for now; that functionality will be for a future ticket:

**Testing:**

- Test your component by adding it to the relevant div within `/app/components/templates/HomePageLayout.tsx` and checking that it displays all the meetings properly for the current day.
- You may need to add some fake meetings to the database to test with. MongoDB Compass will come in handy for this.
- Take a screenshot of the working component and attach it to your PR.

**Starter code:**
```
import { useState, useEffect } from 'react';

const meetingCache = new Map();

const fetchMeetingsByDay = async (date) => {
  if (meetingCache.has(date)) {
    // Use cached data if available
    return meetingCache.get(date);
  }

  // Fetch data from API if not in cache
  const response = await fetch(`/api/retrieve/meeting/day?date=${date}`);
  const data = await response.json();

  // Store data in cache
  meetingCache.set(date, data);

  return data;
};

const Calendar = () => {
  const [meetings, setMeetings] = useState([]);
  const [currentDate, setCurrentDate] = useState(getTodayDate());

  // Function to get today's date in YYYY-MM-DD format
  function getTodayDate() {
    const today = new Date();
    return today.toISOString().split('T')[0];
  }

  // Load meetings for the current date on initial render
  useEffect(() => {
    handleDateChange(currentDate);
  }, []);

  // Fetch and update meetings when the date changes
  const handleDateChange = async (date) => {
    const data = await fetchMeetingsByDay(date);
    setMeetings(data);
    setCurrentDate(date);
  };

  // Navigate to the previous day
  const handlePreviousDay = () => {
    const prevDate = new Date(currentDate);
    prevDate.setDate(prevDate.getDate() - 1);
    handleDateChange(prevDate.toISOString().split('T')[0]);
  };

  // Navigate to the next day
  const handleNextDay = () => {
    const nextDate = new Date(currentDate);
    nextDate.setDate(nextDate.getDate() + 1);
    handleDateChange(nextDate.toISOString().split('T')[0]);
  };

  return (
    <div>
      <div>
        <button onClick={handlePreviousDay}>Previous Day</button>
        <span>{currentDate}</span>
        <button onClick={handleNextDay}>Next Day</button>
      </div>

      <MeetingList meetings={meetings} />
    </div>
  );
};
```"
214,679b66f0cbe8284d7a730807,issue,learning -> program comprehension -> formatting logs for better readability,https://github.com/freelawproject/courtlistener/issues/4646,"RECAP Sweep Cronjob failing with missing index?
Just saw this in k8s logs:

```
Traceback (most recent call last):
  File ""/opt/courtlistener/manage.py"", line 15, in <module>
    main()
  File ""/opt/courtlistener/manage.py"", line 11, in main
    execute_from_command_line(sys.argv)
  File ""/usr/local/lib/python3.13/site-packages/django/core/management/__init__.py"", line 442, in execute_from_command_line
    utility.execute()
  File ""/usr/local/lib/python3.13/site-packages/django/core/management/__init__.py"", line 436, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/usr/local/lib/python3.13/site-packages/django/core/management/base.py"", line 413, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/usr/local/lib/python3.13/site-packages/django/core/management/base.py"", line 459, in execute
    output = self.handle(*args, **options)
  File ""/opt/courtlistener/cl/alerts/management/commands/cl_send_recap_alerts.py"", line 730, in handle
    query_and_send_alerts(r, Alert.REAL_TIME, query_date)
  File ""/opt/courtlistener/cl/alerts/management/commands/cl_send_recap_alerts.py"", line 577, in query_and_send_alerts
    results, parent_results, child_results = query_alerts(search_params)
  File ""/opt/courtlistener/cl/alerts/management/commands/cl_send_recap_alerts.py"", line 445, in query_alerts
    return do_es_sweep_alert_query(search_query, child_search_query, search_params)
  File ""/opt/courtlistener/cl/lib/elasticsearch_utils.py"", line 3235, in do_es_sweep_alert_query
    responses = multi_search.execute()
  File ""/usr/local/lib/python3.13/site-packages/elasticsearch_dsl/search.py"", line 831, in execute
    raise ApiError(""N/A"", meta=responses.meta, body=r)

elasticsearch.ApiError: ApiError(200, 'N/A', 'no such index [recap_document_sweep]', recap_document_sweep, index_or_alias)     
```

(I used chatgpt to reformat that, k9s is making it hard, so it may have hallucinations, but I think it looks OK.)

Looks like it wants an index called recap_document_sweep, which we lack. Need to investigate."
215,679b66f0cbe8284d7a730862,issue,learning -> program comprehension,https://github.com/matteo-convertino/otpmanager-app/issues/57,"No password initialized when using fingerprint on android 
I'll try to explain my problem the best I can.

I installed otpmanager to my nextcloud aio running via docker. Never opened the app on my browser. But I then installed the Android app as I wanted to check it out. I could login with my fingerprint. So no worries there. But when I wanted to login to the app on my browser, it asks me for a password that I have never set.
I asked chatgpt and it said ""your description matches the logic used in the provided code"".

I would really love to use this app. But it only has advantage over authenticator if I also can use it on my pc/laptop/browser.
Is there anything to get a fix for this? "
216,679b66f0cbe8284d7a73087b,issue,generating/manipulating data,https://github.com/languagetool-org/languagetool/issues/10994,"[pt] Fix in Premium â€” 2024-11-06
Heya, @jaumeortola , @susanaboatto and @p-goulart ,

I know you are very busy, but this is an issue that involves the Premium version of LanguageTool.

This disambiguator rule breaks the Premium version:

```
    <rule> <!-- Used ChatGPT 4o to verify the 3217 results -->
      <pattern>
        <token postag=""V.+"" postag_regexp=""yes""><exception postag_regexp='yes' postag='CS|RG|NC.+|AQ.+|CC|SPS.+|[DP].+'/></token>
        <token regexp='yes' inflected='yes'>um|muito|pouco|vÃ¡rios|diversos|tal</token> <!-- Special PI.* that don't break rules -->
        <marker>
          <and>
            <token postag=""VMIP3S0|VMM02S0|VMSP2S0|VMIP2S0|VMN02S0|VMSF2S0|VMIP1S0|VMP00SM"" postag_regexp=""yes""/>
            <token postag=""NC.+"" postag_regexp=""yes""/>
          </and>
        </marker>
      </pattern>
      <disambig action=""remove"" postag=""V.*""/>
    </rule>
```

Examples of its usage:
```
        <example correction=''>Quero muitos <marker>marines</marker>.</example>
        <example correction=''>Quero vÃ¡rios <marker>marines</marker>.</example>
        <example correction=''>Terminar tal <marker>curso</marker>.</example>
        <example correction=''>Terminar um <marker>doutorado</marker>.</example>
        <example correction=''>Vendeu pouco <marker>mercado</marker>.</example>
        <example correction=''>Vendeu muito <marker>mercado</marker>.</example>
```

Could one of you make changes in Premium so that I may implement it?

Thanks!

â¤ï¸ â¤ï¸ â¤ï¸ 
"
217,679b66f0cbe8284d7a730949,issue,software quality -> fixing -> supporting debugging -> writing code to reproduce a bug,https://github.com/arkivanov/Decompose/issues/807,"Multiple retainedComponent (RootComponent) ComponentContext for android app will crash
Here my **RootComponent** class 

```
class RootComponent(componentContext: ComponentContext) : ComponentContext by componentContext {
    private val navigation = StackNavigation<Configuration>()

    val childStack = childStack(
        key = ""mainRoot"",
        source = navigation,
        serializer = Configuration.serializer(),
        initialConfiguration = Configuration.SplashActivity,
        handleBackButton = true,
        childFactory = ::createRootChild
    )

    private fun createRootChild(
        configuration: Configuration, context: ComponentContext
    ): Child {
        return when (configuration) {
            is Configuration.SplashActivity -> Child.SplashActivity(
                SplashComponent(contextComponent = context, onTimerComplete = {
                    navigation.pushNew(Configuration.LoginActivity)
                })
            )

            is Configuration.LoginActivity -> Child.LoginActivity(
                LoginComponent(componentContext = context, onLogin = {
                    navigation.pushNew(Configuration.DashboardActivity)
                })
            )

            is Configuration.DashboardActivity -> Child.DashboardActivity(DashboardComponent(context))
        }
    }

    sealed class Child {
        data class SplashActivity(val component: SplashComponent) : Child()
        data class LoginActivity(val component: LoginComponent) : Child()
        data class DashboardActivity(val component: DashboardComponent) : Child()
    }

    @Serializable
    sealed class Configuration {
        @Serializable
        data object SplashActivity : Configuration()

        @Serializable
        data object LoginActivity : Configuration()

        @Serializable
        data object DashboardActivity : Configuration()
    }
}
```


Here my **DashBoardRootComponent** class 

```
class DashBoardRootComponent(componentContext: ComponentContext) :
    ComponentContext by componentContext {
    private val navigation = StackNavigation<Configuration>()

    val childStack = childStack(
        key = ""dashBoardRoot"",
        source = navigation,
        serializer = Configuration.serializer(),
        initialConfiguration = Configuration.PlayListActivity,
        handleBackButton = true,
        childFactory = ::createDashBoardChild
    )

    private fun createDashBoardChild(
        configuration: Configuration, context: ComponentContext
    ): Child {
        return when (configuration) {
            is Configuration.PlayListActivity -> Child.PlayListActivity(
                PlayListComponent(componentContext = context, onPlayListClick = {
                    navigation.pushNew(Configuration.TrackListActivity)
                })
            )

            is Configuration.TrackListActivity -> Child.TrackListActivity(TrackListComponent(componentContext = context,
                onTrackListClick = {
                },
                onBackPress = {
                    navigation.pop()
                }))
        }
    }

    sealed class Child {
        data class PlayListActivity(val component: PlayListComponent) : Child()
        data class TrackListActivity(val component: TrackListComponent) : Child()
//        data class DashboardActivity(val component: DashboardComponent) : Child()
    }

    @kotlinx.serialization.Serializable
    sealed class Configuration {
        @kotlinx.serialization.Serializable
        data object PlayListActivity : Configuration()

        @kotlinx.serialization.Serializable
        data object TrackListActivity : Configuration()
    }

}
```

My App Component

```
@Composable
@Preview
fun App(root: RootComponent, dashBoardRoot: DashBoardRootComponent) {
    MaterialTheme {
        val childStack by root.childStack.subscribeAsState()
        Children(
            stack = childStack,
            animation = stackAnimation(slide()),
            content = { child ->
                when (val instance = child.instance) {
                    is RootComponent.Child.SplashActivity -> Splash(instance.component)
                    is RootComponent.Child.LoginActivity -> Login(instance.component)
                    is RootComponent.Child.DashboardActivity -> Dashboard(
                        instance.component,
                        dashBoardRootComponent = dashBoardRoot
                    )
                }
            }
        )
    }
}
```

For **Android MainActivity 1st Try** 

```
class MainActivity : ComponentActivity() {
    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)

        val rootF = retainedComponent {
            RootComponent(it)
        }
        val dashF = retainedComponent {
            DashBoardRootComponent(it)
        }
        setContent {
            App(rootF, dashF)
        }
    }
}
```

Logcat  : 
```
FATAL EXCEPTION: main
Process: com.neonmusic.app, PID: 3633
java.lang.RuntimeException: Unable to start activity ComponentInfo{com.neonmusic.app/com.neonmusic.app.MainActivity}: java.lang.IllegalArgumentException: SavedStateProvider with the given key is already registered
	at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3635)
	at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3792)
	at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:103)
	at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)
	at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)
	at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2210)
	at android.os.Handler.dispatchMessage(Handler.java:106)
	at android.os.Looper.loopOnce(Looper.java:201)
	at android.os.Looper.loop(Looper.java:288)
	at android.app.ActivityThread.main(ActivityThread.java:7839)
	at java.lang.reflect.Method.invoke(Native Method)
	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:548)
	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1003)
Caused by: java.lang.IllegalArgumentException: SavedStateProvider with the given key is already registered
	at androidx.savedstate.SavedStateRegistry.registerSavedStateProvider(SavedStateRegistry.kt:110)
	at com.arkivanov.essenty.statekeeper.AndroidExtKt.StateKeeper(AndroidExt.kt:31)
	at com.arkivanov.essenty.statekeeper.AndroidExtKt.stateKeeper(AndroidExt.kt:54)
	at com.arkivanov.decompose.RetainedComponentKt.retainedComponent(RetainedComponent.kt:101)
	at com.arkivanov.decompose.RetainedComponentKt.retainedComponent(RetainedComponent.kt:52)
	at com.arkivanov.decompose.RetainedComponentKt.retainedComponent$default(RetainedComponent.kt:45)
	at com.neonmusic.app.MainActivity.onCreate(MainActivity.kt:20)
	at android.app.Activity.performCreate(Activity.java:8051)
	at android.app.Activity.performCreate(Activity.java:8031)
	at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1329)
	at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3608)
	at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3792)Â 
	at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:103)Â 
	at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)Â 
	at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)Â 
	at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2210)Â 
	at android.os.Handler.dispatchMessage(Handler.java:106)Â 
	at android.os.Looper.loopOnce(Looper.java:201)Â 
	at android.os.Looper.loop(Looper.java:288)Â 
	at android.app.ActivityThread.main(ActivityThread.java:7839)Â 
	at java.lang.reflect.Method.invoke(Native Method)Â 
	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:548)Â 
	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1003)Â Â 
```


For **Android MainActivity 2nd Try** from chatGpt reference

```
class MainActivity : ComponentActivity() {
    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)

        val root = retainedComponentWithKey(key = ""RootComponent_mainRoot"") {
            RootComponent(it)
        }
        val dashBoardRoot = retainedComponentWithKey(key = ""DashBoardRootComponent_dashBoardRoot"") {
            DashBoardRootComponent(it)
        }

        setContent {
            App(root.first, dashBoardRoot.first)
        }
    }

    private fun <T : Any> retainedComponentWithKey(
        key: String,
        factory: (ComponentContext) -> T
    ): Pair<T, String> {
        val component = retainedComponent(key = key, handleBackButton = true, factory = factory)
        return component to key
    }
}
```

Logcat  : 
```
FATAL EXCEPTION: main
Process: com.neonmusic.app, PID: 3248
java.lang.RuntimeException: Unable to start activity ComponentInfo{com.neonmusic.app/com.neonmusic.app.MainActivity}: java.lang.IllegalArgumentException: SavedStateProvider with the given key is already registered
	at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3635)
	at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3792)
	at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:103)
	at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)
	at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)
	at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2210)
	at android.os.Handler.dispatchMessage(Handler.java:106)
	at android.os.Looper.loopOnce(Looper.java:201)
	at android.os.Looper.loop(Looper.java:288)
	at android.app.ActivityThread.main(ActivityThread.java:7839)
	at java.lang.reflect.Method.invoke(Native Method)
	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:548)
	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1003)
Caused by: java.lang.IllegalArgumentException: SavedStateProvider with the given key is already registered
	at androidx.savedstate.SavedStateRegistry.registerSavedStateProvider(SavedStateRegistry.kt:110)
	at com.arkivanov.essenty.statekeeper.AndroidExtKt.StateKeeper(AndroidExt.kt:31)
	at com.arkivanov.essenty.statekeeper.AndroidExtKt.stateKeeper(AndroidExt.kt:54)
	at com.arkivanov.decompose.RetainedComponentKt.retainedComponent(RetainedComponent.kt:101)
	at com.arkivanov.decompose.RetainedComponentKt.retainedComponent(RetainedComponent.kt:52)
	at com.arkivanov.decompose.RetainedComponentKt.retainedComponent$default(RetainedComponent.kt:45)
	at com.neonmusic.app.MainActivity.retainedComponentWithKey(MainActivity.kt:33)
	at com.neonmusic.app.MainActivity.onCreate(MainActivity.kt:20)
	at android.app.Activity.performCreate(Activity.java:8051)
	at android.app.Activity.performCreate(Activity.java:8031)
	at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1329)
	at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3608)
	at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3792)Â 
	at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:103)Â 
	at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)Â 
	at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)Â 
	at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2210)Â 
	at android.os.Handler.dispatchMessage(Handler.java:106)Â 
	at android.os.Looper.loopOnce(Looper.java:201)Â 
	at android.os.Looper.loop(Looper.java:288)Â 
	at android.app.ActivityThread.main(ActivityThread.java:7839)Â 
	at java.lang.reflect.Method.invoke(Native Method)Â 
	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:548)Â 
	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1003)Â Â 
```


I am facing this issue only for android 

Do code for desktop also but it will working fine.
Here Desktop entry point

```
fun main() = application {
    Window(
        onCloseRequest = ::exitApplication,
        title = ""neon-music-kmp"",
    ) {
        val root = remember { RootComponent(DefaultComponentContext(LifecycleRegistry())) }
        val dashBoardRoot = remember { DashBoardRootComponent(DefaultComponentContext(LifecycleRegistry())) }
        App(root, dashBoardRoot)
    }
}
```

Can any one explain me where i need to give unique key for android retainedComponent ?
Tell me if i am wrong in anything."
218,679b66f0cbe8284d7a7309bb,issue,feature implementation/enhancement -> prototyping -> implementing proof of concept,https://github.com/tokenami/tokenami/issues/380,"refactor ts-plugin/index.ts
it's a mess that has evolved over time with a lot of help from ChatGPT (so yeah, messy). the current approach was fine to get going and prove the concept but need to tidy this up a bit and make it a little easier to maintain. can probably take some inspiration from other tsplugin architectures for this e.g. https://github.com/microsoft/typescript-styled-plugin"
219,679b6789cbe8284d7a730a32,issue,feature implementation/enhancement -> scripting,https://github.com/Letaryat/poor-sharptimer/issues/93,"Conversion of old style json PlayerReplayData
Hi

You deprecated the old json format. So the replay data cannot be read. I asked ChatGPT to create a script in bash which does the conversion of the old replaydata in the folder. I thought I'll share it with you, if you wanna somehow implement this logic in a new plugin release or let people help themselves (hopefully they use Linux).

Have a good evening,
B4C0N"
220,679b6789cbe8284d7a730b5d,issue,feature implementation/enhancement -> feature enhancement,https://github.com/perazz/fortran-lapack/issues/65,"Create specialized versions of BLAS subroutines with fewer arguments and run-time decisions
I asked ChatGPT o1-preview to create specialized versions of stdlib_dgemm for the cases where the matrix multiplication is done with the original matrix `a` or its transpose. It created `stdlib_dgemm_a_orig` and `stdlib_dgemm_a_trans` below. I have not checked them. In general, what is knowable at compile time (whether to use the matrix or its transpose) should not be done at run time.

```
pure subroutine stdlib_dgemm_a_orig(transb, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
    ! Specialized DGEMM subroutine where A is not transposed (transa = 'N')
    ! Performs the operation: C = alpha * A * op(B) + beta * C

    ! Scalar Arguments
    character, intent(in) :: transb
    integer, intent(in) :: m, n, k
    real(dp), intent(in) :: alpha, beta
    integer, intent(in) :: lda, ldb, ldc

    ! Array Arguments
    real(dp), intent(in) :: a(lda, *)
    real(dp), intent(in) :: b(ldb, *)
    real(dp), intent(inout) :: c(ldc, *)

    ! Local Scalars
    integer :: i, j, l, info, nrowb
    real(dp) :: temp
    logical :: notb
    real(dp), parameter :: zero = 0.0_dp, one = 1.0_dp

    ! Intrinsic Functions
    intrinsic :: max

    ! Set notb as true if B is not transposed
    notb = (transb == 'N' .or. transb == 'n')
    if (notb) then
        nrowb = k
    else
        nrowb = n
    end if

    ! Test the input parameters.
    info = 0
    if ((.not. notb) .and. (.not. (transb == 'T' .or. transb == 't'))) then
        info = 1
    else if (m < 0) then
        info = 2
    else if (n < 0) then
        info = 3
    else if (k < 0) then
        info = 4
    else if (lda < max(1, m)) then
        info = 7
    else if (ldb < max(1, nrowb)) then
        info = 9
    else if (ldc < max(1, m)) then
        info = 12
    end if
    if (info /= 0) then
        call stdlib_xerbla('DGEMM ', info)
        return
    end if

    ! Quick return if possible.
    if ((m == 0) .or. (n == 0) .or. (((alpha == zero) .or. (k == 0)) .and. (beta == one))) return

    ! If alpha is zero.
    if (alpha == zero) then
        if (beta == zero) then
            do j = 1, n
                do i = 1, m
                    c(i, j) = zero
                end do
            end do
        else
            do j = 1, n
                do i = 1, m
                    c(i, j) = beta * c(i, j)
                end do
            end do
        end if
        return
    end if

    ! Start the operations.
    if (notb) then
        ! Form C := alpha*A*B + beta*C.
        do j = 1, n
            if (beta == zero) then
                do i = 1, m
                    c(i, j) = zero
                end do
            else if (beta /= one) then
                do i = 1, m
                    c(i, j) = beta * c(i, j)
                end do
            end if
            do l = 1, k
                temp = alpha * b(l, j)
                do i = 1, m
                    c(i, j) = c(i, j) + temp * a(i, l)
                end do
            end do
        end do
    else
        ! Form C := alpha*A*B**T + beta*C
        do j = 1, n
            if (beta == zero) then
                do i = 1, m
                    c(i, j) = zero
                end do
            else if (beta /= one) then
                do i = 1, m
                    c(i, j) = beta * c(i, j)
                end do
            end if
            do l = 1, k
                temp = alpha * b(j, l)
                do i = 1, m
                    c(i, j) = c(i, j) + temp * a(i, l)
                end do
            end do
        end do
    end if

    return
end subroutine stdlib_dgemm_a_orig

pure subroutine stdlib_dgemm_a_trans(transb, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc)
    ! Specialized DGEMM subroutine where A is transposed (transa = 'T')
    ! Performs the operation: C = alpha * A**T * op(B) + beta * C

    ! Scalar Arguments
    character, intent(in) :: transb
    integer, intent(in) :: m, n, k
    real(dp), intent(in) :: alpha, beta
    integer, intent(in) :: lda, ldb, ldc

    ! Array Arguments
    real(dp), intent(in) :: a(lda, *)
    real(dp), intent(in) :: b(ldb, *)
    real(dp), intent(inout) :: c(ldc, *)

    ! Local Scalars
    integer :: i, j, l, info, nrowb
    real(dp) :: temp
    logical :: notb
    real(dp), parameter :: zero = 0.0_dp, one = 1.0_dp

    ! Intrinsic Functions
    intrinsic :: max

    ! Set notb as true if B is not transposed
    notb = (transb == 'N' .or. transb == 'n')
    if (notb) then
        nrowb = k
    else
        nrowb = n
    end if

    ! Test the input parameters.
    info = 0
    if ((.not. notb) .and. (.not. (transb == 'T' .or. transb == 't'))) then
        info = 1
    else if (m < 0) then
        info = 2
    else if (n < 0) then
        info = 3
    else if (k < 0) then
        info = 4
    else if (lda < max(1, k)) then
        info = 7
    else if (ldb < max(1, nrowb)) then
        info = 9
    else if (ldc < max(1, m)) then
        info = 12
    end if
    if (info /= 0) then
        call stdlib_xerbla('DGEMM ', info)
        return
    end if

    ! Quick return if possible.
    if ((m == 0) .or. (n == 0) .or. (((alpha == zero) .or. (k == 0)) .and. (beta == one))) return

    ! If alpha is zero.
    if (alpha == zero) then
        if (beta == zero) then
            do j = 1, n
                do i = 1, m
                    c(i, j) = zero
                end do
            end do
        else
            do j = 1, n
                do i = 1, m
                    c(i, j) = beta * c(i, j)
                end do
            end do
        end if
        return
    end if

    ! Start the operations.
    if (notb) then
        ! Form C := alpha*A**T*B + beta*C
        do j = 1, n
            do i = 1, m
                temp = zero
                do l = 1, k
                    temp = temp + a(l, i) * b(l, j)
                end do
                if (beta == zero) then
                    c(i, j) = alpha * temp
                else
                    c(i, j) = alpha * temp + beta * c(i, j)
                end if
            end do
        end do
    else
        ! Form C := alpha*A**T*B**T + beta*C
        do j = 1, n
            do i = 1, m
                temp = zero
                do l = 1, k
                    temp = temp + a(l, i) * b(j, l)
                end do
                if (beta == zero) then
                    c(i, j) = alpha * temp
                else
                    c(i, j) = alpha * temp + beta * c(i, j)
                end if
            end do
        end do
    end if

    return
end subroutine stdlib_dgemm_a_trans

```"
221,679b6789cbe8284d7a730bc5,issue,"process -> automating commit/issue/PR creation -> creating commit/issue/PR description,process -> automating commit/issue/PR creation -> creating commit/issue/PR description -> motivating the need for a proposed change",https://github.com/Blueforcer/awtrix3/issues/629,"[FEATURE REQUEST] Direct Pixel Control via MQTT (matrix Property)
Hi,
Here below my request generated by ChatGPT (I'm dot a developer):

I would like to request the implementation of a feature that allows direct pixel manipulation on the AWTRIX display through MQTT. Specifically, this feature could be implemented as a matrix property in MQTT payloads, enabling users to define the position and color of individual pixels programmatically.

For example:

```
{
  ""duration"": 5,
  ""matrix"": [
    {""x"": 0, ""y"": 0, ""color"": ""#FF0000""},
    {""x"": 1, ""y"": 0, ""color"": ""#00FF00""},
    {""x"": 2, ""y"": 0, ""color"": ""#0000FF""}
  ]
}
```
Use Case: This feature would open up new possibilities for creative and dynamic displays, such as:

Displaying custom animations or progress bars.
Visualizing real-time data, like energy usage or temperature trends, directly as a grid of colors or patterns.
Allowing greater control over the AWTRIX display for users who want to go beyond text and icons.
Benefits:

Enhanced Customization: Users can create dynamic, pixel-perfect displays for specific use cases without relying on pre-designed icons.
Compatibility: This aligns AWTRIXâ€™s functionality with modern LED matrix frameworks, which often support pixel-level control.
Community Engagement: This feature could attract more users and contributors who are looking for highly flexible, programmable displays.
Proposed Implementation:

Add a matrix field to the custom MQTT payload specification.
Allow the matrix array to define:
x: Horizontal position (0â€“7 for an 8x8 matrix).
y: Vertical position (0â€“7 for an 8x8 matrix).
color: Pixel color in HEX format (e.g., #FF0000 for red).
Combine this property with duration and icon/text fields, ensuring backward compatibility with existing functionality.
Example Payload: To display a single row of blue pixels:

```
{
  ""duration"": 5,
  ""matrix"": [
    {""x"": 0, ""y"": 6, ""color"": ""#0000FF""},
    {""x"": 1, ""y"": 6, ""color"": ""#0000FF""},
    {""x"": 2, ""y"": 6, ""color"": ""#0000FF""}
  ]
}
```
Potential Challenges:

Compatibility with existing rendering logic.
Memory or processing constraints for handling real-time pixel data.
Conclusion: Direct pixel control would be a highly valuable addition to AWTRIXâ€™s feature set, enabling advanced display options for power users and developers. This would also keep AWTRIX competitive with alternative smart LED matrix solutions.

Thank you for considering this request! Please let me know if I can provide further details or examples."
222,679b682dcbe8284d7a730d96,issue,learning -> how to use a library/framework/tool,https://github.com/sibprogrammer/xq/issues/108,"Show the path for the match
**Problem Statement**

I am surprised we do not have a nice good tool to show the `path` to the match. 

I asked chatgpt and it told me to try `xq 'path(.. | select(. == ""your_search_value""))' file.xml` and obviously that did not work since there seems to be no such ""advanced"" search options in `xq`.

I wondered if there could be some ""search"" mode which also would depict the path to the hit or may be you would recommend alternatives?

Thanks in advance."
223,679b682dcbe8284d7a730e12,issue,software quality -> fixing -> functional bugs,https://github.com/ReVanced/revanced-patches/issues/3945,"bug(YouTube - SponsorBlock): Button initialize failure, buttons do not show on some devices with custom roms
### Bug description

[Copied from thread I made in the official Discord:](https://discord.com/channels/952946952348270622/1308305076921827328)

> I have a Blackview MEGA 1 tablet and get the toast error upon YT startup ""SponsorBlockViewController: initialize failure"", causing the ""Skip button"" option of SB to not function (no buttons shown) and SB to have no buttons at all when attempting to contribute. It seems to be specific to Blackview devices due to the OEM-customized ROM. My device is rooted with Magisk, but disabling all modules and app root access does not fix the problem. A search on 4PDA (Russian analogue of XDA-Developers) reveals others experiencing the issue on Blackview devices: https://4pda.to/forum/index.php?act=search&source=all&forums%5B%5D=all&query=sponsorblockviewcontroller&x=23&y=12&subforums=1
> 
> Here's the logcat: https://pastebin.com/ZrkxzH7d
> I attempted to use ChatGPT to find a fix; I'm not yet experienced with building in java to test it. It blames a class mismatch with the drawable being called by SB RippleDrawable and the ROM only having StateListDrawable. https://github.com/acidcitrus/revanced-patches/commit/a2fc1bccc44cdb21c9dc3e07e70ffae03f32066c
> 
> I'd appreciate if someone could look this over and see if it makes sense.

### Error logs

```shell
initialize failure
android.view.InflateException: Binary XML file line #26 in app.revanced.android.youtube:layout/revanced_sb_inline_sponsor_overlay: Binary XML file line #26 in app.revanced.android.youtube:layout/revanced_sb_inline_sponsor_overlay: Error inflating class app.revanced.extension.youtube.sponsorblock.ui.NewSegmentLayout
Caused by: android.view.InflateException: Binary XML file line #26 in app.revanced.android.youtube:layout/revanced_sb_inline_sponsor_overlay: Error inflating class app.revanced.extension.youtube.sponsorblock.ui.NewSegmentLayout
Caused by: java.lang.reflect.InvocationTargetException
	at java.lang.reflect.Constructor.newInstance0(Native Method)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:343)
	at android.view.LayoutInflater.createView(LayoutInflater.java:858)
	at android.view.LayoutInflater.createViewFromTag(LayoutInflater.java:1014)
	at android.view.LayoutInflater.createViewFromTag(LayoutInflater.java:965)
	at android.view.LayoutInflater.rInflate(LayoutInflater.java:1144)
	at android.view.LayoutInflater.inflate(LayoutInflater.java:660)
	at android.view.LayoutInflater.inflate(LayoutInflater.java:538)
	at android.view.LayoutInflater.inflate(LayoutInflater.java:485)
	at app.revanced.extension.youtube.sponsorblock.ui.SponsorBlockViewController.initialize(SponsorBlockViewController.java:64)
	at kzd.aa(PG:102)
	at kzd.x(PG:1)
	at knr.<init>(PG:24)
	at fxc.c(PG:5)
	at fxc.a(PG:3040)
	at qsd.a(PG:79)
	at kol.y(PG:198)
	at koi.N(PG:34)
	at cg.im(PG:26)
	at alet.e(PG:242)
	at alet.g(PG:926)
	at de.aA(PG:130)
	at de.aC(PG:83)
	at de.I(PG:98)
	at bc.e(PG:7)
	at knq.b(PG:86)
	at knq.c(PG:1)
	at com.google.android.apps.youtube.app.watchwhile.MainActivity.onPostCreate(PG:42)
	at android.app.Instrumentation.callActivityOnPostCreate(Instrumentation.java:1458)
	at android.app.ActivityThread.handleStartActivity(ActivityThread.java:3776)
	at android.app.servertransaction.TransactionExecutor.performLifecycleSequence(TransactionExecutor.java:221)
	at android.app.servertransaction.TransactionExecutor.cycleToPath(TransactionExecutor.java:201)
	at android.app.servertransaction.TransactionExecutor.executeLifecycleState(TransactionExecutor.java:173)
	at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:97)
	at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2347)
	at android.os.Handler.dispatchMessage(Handler.java:106)
	at android.os.Looper.loopOnce(Looper.java:201)
	at android.os.Looper.loop(Looper.java:288)
	at android.app.ActivityThread.main(ActivityThread.java:7968)
	at java.lang.reflect.Method.invoke(Native Method)
	at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:569)
	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1019)
Caused by: java.lang.ClassCastException: android.graphics.drawable.StateListDrawable cannot be cast to android.graphics.drawable.RippleDrawable
	at app.revanced.extension.youtube.sponsorblock.ui.NewSegmentLayout.initializeButton(NewSegmentLayout.java:114)
	at app.revanced.extension.youtube.sponsorblock.ui.NewSegmentLayout.<init>(NewSegmentLayout.java:54)
	at app.revanced.extension.youtube.sponsorblock.ui.NewSegmentLayout.<init>(NewSegmentLayout.java:39)
	at app.revanced.extension.youtube.sponsorblock.ui.NewSegmentLayout.<init>(NewSegmentLayout.java:35)
	... 42 more
```

### Solution

https://github.com/acidcitrus/revanced-patches/commit/a2fc1bccc44cdb21c9dc3e07e70ffae03f32066c

### Additional context

ReVanced official, DokeOS 4.0 - Android 13, YouTube 19.43.41, both GMSCore and root mount affected. Isolated to Blackview device, as my other 4+ Android devices from different OEMs have no issues running the same ReVanced patches (root & microG, A9-A15).

### Acknowledgements

- [x] I have checked all open and closed bug reports and this is not a duplicate.
- [x] I have chosen an appropriate title.
- [x] All requested information has been provided properly."
224,679b682dcbe8284d7a730e79,issue,development environment -> setting/implementing a new configuration file,https://github.com/serverpod/serverpod/issues/2972,"Concurrent modification during iteration: Instance(length:17) of '_GrowableList'.
**Describe the bug**

I tried to use chatgpt to generate serverpod model files for a new app. I teached serverpod with the 
https://github.com/serverpod/serverpod_docs/blob/main/docs/06-concepts/02-models.md

and my app idea. 

Chatgpt generated a bunch of .spy.yaml files which are looking good but makes serverpod generate crash with
Concurrent modification during iteration: Instance(length:17) of '_GrowableList'.

To simplify things it tried generate only the code for one this model files (I know that generated model files are not correct,
but it would be lovely to get an error message instead of an exception)

**To Reproduce**
Steps to reproduce the behavior:
1. serverpod create myapp

2. cd myapp;cd myapp_server

3. create inside lib/src/models/user.spy.yaml 
class: User
table: users
fields:
  id: int, isPrimaryKey, autoIncrement
  username: String, unique
  email: String, unique
  displayName: String
  calendarEvents: List<CalendarEvent>, relation
  createdAt: DateTime, default=now
  updatedAt: DateTime, default=now

4. serverpod generate
5. See error

Stack-Trace:
Yikes! It is possible that this error is caused by an internal issue with the Serverpod tooling.
We would appreciate if you filed an issue over at Github. Please include the stack trace below and
describe any steps you did to trigger the error.
https://github.com/serverpod/serverpod/issues

```
Concurrent modification during iteration: Instance(length:17) of '_GrowableList'.

#0      ListIterator.moveNext (dart:_internal/iterable.dart:348:7)
#1      ModelDependencyResolver.resolveModelDependencies.<anonymous closure>
(package:serverpod_cli/src/analyzer/models/entity_dependency_resolver.dart:17:51)
#2      Iterable.forEach (dart:core/iterable.dart:347:35)
#3      ModelDependencyResolver.resolveModelDependencies
(package:serverpod_cli/src/analyzer/models/entity_dependency_resolver.dart:15:51)
#4      SerializableModelAnalyzer.resolveModelDependencies
(package:serverpod_cli/src/analyzer/models/model_analyzer.dart:95:36)
#5      StatefulAnalyzer._updateAllModels
(package:serverpod_cli/src/analyzer/models/stateful_analyzer.dart:113:31)
#6      StatefulAnalyzer.validateAll
(package:serverpod_cli/src/analyzer/models/stateful_analyzer.dart:71:5)
#7      performGenerate (package:serverpod_cli/src/generator/generator.dart:29:25)
<asynchronous suspension>
#8      StdOutLogger.progress (package:cli_tools/src/logger/loggers/std_out_logger.dart:141:20)
<asynchronous suspension>
#9      GenerateCommand.run (package:serverpod_cli/src/commands/generate.dart:69:17)
<asynchronous suspension>
#10     CommandRunner.runCommand (package:args/command_runner.dart:212:13)
<asynchronous suspension>
#11     BetterCommandRunner.runCommand
(package:cli_tools/src/better_command_runner/better_command_runner.dart:140:7)
<asynchronous suspension>
#12     ServerpodCommandRunner.runCommand
(package:serverpod_cli/src/runner/serverpod_command_runner.dart:103:5)
```

**Expected behavior**
Instead of an exception I except to get an error message.

I figured out, the main reason for this exception is this:
calendarEvents: List<CalendarEvent>, relation

Deleting ""List"" from the line and I get an error message instead of the exception.

**Serverpod versions**
 -  _CLI Version:  2.1.5
 - _Library version: 2.1.5

**Platform information**
Apple MacBookPro M1

Flutter 3.24.4 â€¢ channel stable â€¢ https://github.com/flutter/flutter.git
Framework â€¢ revision 603104015d (4 weeks ago) â€¢ 2024-10-24 08:01:25 -0700
Engine â€¢ revision db49896cf2
Tools â€¢ Dart 3.5.4 â€¢ DevTools 2.37.3

**Additional context**
Hope this helps, that took a while for figuring this out and open an issue.
This happens also on the latest 2.2.0 beta

Ok, I also learned that I have to feed the AI with https://github.com/serverpod/serverpod_docs/tree/main/docs/06-concepts/06-database/03-relations

"
225,679b68b7cbe8284d7a731027,issue,learning -> how to use a library/framework/tool,https://github.com/Jugran/lyrics-in-terminal/issues/46,"May I ask what URL to use for google lyrics?
Hello,

I just learned that Google lyrics search is one of the three databases this program uses.

May I ask the URL or the wget that you are using to get lyrics from Google? I asked ChatGPT but it does not seem to know the proper syntax to create a working lyrics request command.

Thank you :)"
226,679b68b7cbe8284d7a731035,issue,software quality -> fixing -> supporting debugging,https://github.com/rmcelreath/rethinking/issues/449,"Can't work ""Full luxury Bayes"" from 2023 lecture 12
I'm working my way through _Statistical Rethinking_ and have run into some trouble running the models in the bonus material of the 2023 lecture 12. I initially had a lot of trouble replicating the posteriors shown on the slides, but that may be due to sampling differences since we're not using a ```set.seed()```. However, when I attempted to run the ""full luxury Bayes"" model at the end of the bonus material, Stan couldn't run the model at all.

Before attempting to run the chains, I got an error message saying, ""line 27, column 8: Unreachable
    statement (following a reject, fatal_error, break, continue, or return)
    found, is this intended?""

The code at line 27, column 8 of the Stan text file is the next line after a return line, which from what I gather from ChatGPT is part of the problem: Stan can't run two different returns as it terminates after the first return. Here is the Stan code I'm seeing:
```
        return normal_lpdf( X | mu , sigma ); // line 26
        for ( i in 1:size(Y) ) {
            p[i] = a[g[start+i-1]] + bxy * X[start+i-1] + bzy * Z[g[start+i-1]] + buy * u[g[start+i-1]];
            p[i] = inv_logit(p[i]);
        }
        return bernoulli_lpmf( Y | p );
```

Once the code tried to run, it produced these errors: ""Chain 1 Unrecoverable error evaluating the log probability at the initial value.
Chain 1 Exception: Exception: normal_lpdf: Random variable has size = 200, but Location parameter has size 1; and they must be the same size."" It also referred to line 27, and then referred to this section of the Stan code as well:

```
target += reduce_sum( reducer , Y , 1 , 
            Xbar,
            Ng,
            Z,
            a,
            X,
            g,
            u,
            z,
            bzy,
            buy,
            bxy,
            aX,
            bux,
            abar,
            tau,
            sigma )
```

For reference, here is the full R code that I am using, from the 2023 lecture 12 slides & video:

```r
N_groups <- 30
N_id <- 200
a0 <- (-2)
bZY <- (-0.5)
g <- sample(1:N_groups,size=N_id,replace=TRUE) # sample into groups
Ug <- rnorm(N_groups,1.5) # group confounds
X <- rnorm(N_id, Ug[g] ) # individual varying trait
Z <- rnorm(N_groups) # group varying trait (observed)
Y <- rbern(N_id, p=inv_logit( a0 + X + Ug[g] + bZY*Z[g] ) )
dat <- list(Y=Y,X=X,g=g,Ng=N_groups,Z=Z)
xbar <- sapply( 1:N_groups , function(j) mean(X[g==j]) )
dat$Xbar <- xbar
```

```r
mru <- ulam(
    alist(
        # Y model
        Y ~ bernoulli(p),
        logit(p) <- a[g] + bxy*X + bzy*Z[g] + buy*u[g],
        transpars> vector[Ng]:a <<- abar + z*tau,
        # X model
        X ~ normal(mu,sigma),
        mu <- aX + bux*u[g],
        vector[Ng]:u ~ normal(0,1),
        # priors
        z[g] ~ dnorm(0,1),
        c(aX,bxy,buy,bzy) ~ dnorm(0,1),
        bux ~ dexp(1),
        abar ~ dnorm(0,1),
        tau ~ dexp(1),
        sigma ~ dexp(1)
    ) , data=dat , chains=4 , cores=4 , sample=TRUE )
``` 

I'm not sure if there was a code update in Stan/R/Rethinking/etc. that makes the code no longer work, if it's something unique to my computer, if it's an error on my part, etc. Any help would be greatly appreciated."
227,679b68b7cbe8284d7a731170,issue,development environment -> setting/implementing a new configuration file,https://github.com/OfficialIncubo/BeatDrop-Music-Visualizer/issues/30,"[JFYI] Using ChatGPT  to srite .ini  config file for sprites settings 
![image](https://github.com/user-attachments/assets/0c846e1e-0de3-4754-97d2-a587bf5d9c53)
"
228,679b6942cbe8284d7a7312f6,issue,"software quality -> fixing -> functional bugs,software quality -> fixing -> supporting debugging",https://github.com/NotePlan/plugins/issues/603,"Dashboard: CSS errors
Rollup continues to complain (and slow down) due to CSS it doesn't understand. I have updated to the latest Rollup and have added rollup modules for postCSS etc. to try to get them to understand it. But no joy.
```
[plugin postcss] jgclark.Dashboard/src/react/css/Dashboard.css: Lexical error on line 1: Unrecognized text.

  Erroneous area:
1: l*1.4
^..^
[plugin postcss] jgclark.Dashboard/src/react/css/Dashboard.css: Lexical error on line 1: Unrecognized text.

  Erroneous area:
1: l*0.8
^..^
[plugin postcss] jgclark.Dashboard/src/react/css/Dashboard.css: Lexical error on line 1: Unrecognized text.

  Erroneous area:
1: l*1.4
^..^
[plugin postcss] jgclark.Dashboard/src/react/css/Dashboard.css: Lexical error on line 1: Unrecognized text.

  Erroneous area:
1: l*1.4
^..^
[plugin postcss] jgclark.Dashboard/src/react/css/Dashboard.css: Lexical error on line 1: Unrecognized text.

  Erroneous area:
1: l*0.8
^..^
[plugin postcss] jgclark.Dashboard/src/react/css/Dashboard.css: Lexical error on line 1: Unrecognized text.

  Erroneous area:
1: l*1.4
^..^
[plugin postcss] jgclark.Dashboard/src/react/css/DashboardDialog.css: Lexical error on line 1: Unrecognized text.

  Erroneous area:
1: l*1.4
^..^
[plugin postcss] jgclark.Dashboard/src/react/css/DashboardDialog.css: Lexical error on line 1: Unrecognized text.

  Erroneous area:
1: l*1.4
^..^
[plugin postcss] jgclark.Dashboard/src/react/components/Header/Header.css: Lexical error on line 1: Unrecognized text.

  Erroneous area:
1: l*0.8
^..^
[plugin postcss] jgclark.Dashboard/src/react/components/Header/Header.css: Lexical error on line 1: Unrecognized text.

  Erroneous area:
1: l*0.8
^..^
[plugin postcss] np.Shared/src/react/DynamicDialog/DynamicDialog.css: Lexical error on line 1: Unrecognized text.

  Erroneous area:
1: l*0.8
^..^
[plugin postcss] np.Shared/src/react/DynamicDialog/DynamicDialog.css: Lexical error on line 1: Unrecognized text.

  Erroneous area:
1: l*0.8
^..^
[plugin postcss] np.Shared/src/react/DynamicDialog/DropdownSelect.css: Lexical error on line 1: Unrecognized text.

  Erroneous area:
1: l*1.4
^..^
[plugin postcss] np.Shared/src/react/DynamicDialog/DropdownSelect.css: Lexical error on line 1: Unrecognized text.

  Erroneous area:
1: l*1.4
^..^
```

I don't know anything about CSS of this type, so I asked ChatGPT what to do and here's what chat said...the last one (OPTION B) seems to make the most sense to me, and it's easier to understand in the CSS. I went ahead and made the required changes to the Rollup script. You would only need to change the CSS.

![Screen Cap 2024-12-01 at 18 00 39@2x](https://github.com/user-attachments/assets/260f377d-bf5c-44ee-bad7-f7adb944c30d)


![Screen Cap 2024-12-01 at 18 01 06@2x](https://github.com/user-attachments/assets/34d60653-e8d2-45d7-8499-812a3dfef758)


![Screen Cap 2024-12-01 at 18 01 49@2x](https://github.com/user-attachments/assets/fbe7f115-ee9a-473c-acd1-84c21d3cd690)



"
229,679b69d6cbe8284d7a7314e3,issue,software quality -> fixing -> supporting debugging,https://github.com/DyogenIBENS/Agora/issues/43,"If the duplicate gene names caused the problem
Dear Alexandra,
When I ran agora-vertebrates.py, the ancGenes.log showed that 'Traceback (most recent call last):File ""/public/xxx/Agora/Agora-master/src/ALL.reformatGeneFamilies.py"", line 91, in <module>  raise AssertionError(""Duplicated genes in the orthology group"") 
  AssertionError: Duplicated genes in the orthology group' so I used 'sort orthologyGroups.<species>.list | uniq -d' to check and delete the duplicate gene names in my orthologous group file. After that I ran it again,the same problem occured. So I asked ChatGPT and add some debugging information in the  ALL.reformatGeneFamilies.py like 
'for og in orthologyGroups:
    n += 1
    if len(og) != len(set(og)):
        print(f""Duplicated genes in orthology group {n}: {og}"", file=sys.stderr)
        raise AssertionError(""Duplicated genes in the orthology group"")'.
  After trying again, the ancGenes.log still showed the same as before, meanwhile output some possible ID in the log like'Updating the ancestral families of N0 ... adding names ... Duplicated genes in orthology group 9: ['Engystomops_pustulosus_rna-gnl|WGS_WNYA|PAC4GC_49059466_R1', 'Bufo_gargarizans_rna-XM_044272689.1_R0', 'Bombina_bombina_rna-XM_040438551.1_R0', 'Aquarana_catesbeiana_rna-gnl|WGS_LIAG|AB205_0145020A_mRNA_R4', 'Bufo_gargarizans_rna-XM_044299896.1_R2', 'Bombina_bombina_rna-XM_053713634.1_R4', 'Bufo_gargarizans_rna-XM_044299608.1_R0', ...]Traceback (most recent call last):
  File ""/public2/xxx/Agora/Agora-master/src/ALL.reformatGeneFamilies.py"", line 91, in <module> raise AssertionError(""Duplicated genes in the orthology group"")  AssertionError: Duplicated genes in the orthology group'.
But these ID are not the Duplicated genes because I have checked them. And I don't kown what's wrong with my input. Hopefully, you can assist me when you're available, and your suggestion and guidance will be highly appreciated.
thanks and regards
Wang"
230,679b69d6cbe8284d7a73151e,issue,software quality -> fixing -> supporting debugging,https://github.com/tasgon/observable/issues/101,"Crash to desktop when opening GUI
Minecraft Version 1.21.1
Modloader: Neoforge 21.1.83
Modpack: [Dig Burrow Build 2](https://www.curseforge.com/minecraft/modpacks/dig-burrow-build-2)
Observable Version 5.4.3

After setting Observable keybind to + on my numpad I try to open the profiler and game crashes to desktop. I asked ChatGPT about the crash report and it says somthing about The client tried to send a packet through observable:channel-c2s, which was not allowed.

[crash-2024-12-06_10.20.59-client.txt](https://github.com/user-attachments/files/18042206/crash-2024-12-06_10.20.59-client.txt)
"
231,679b69d6cbe8284d7a731551,issue,software quality -> fixing -> supporting debugging,https://github.com/dotnet/aspnetcore/issues/59371,"Problem with .NET 7 applications on Windows Server 2022
### Is there an existing issue for this?

- [x] I have searched the existing issues

### Describe the bug

The following is on an Azure VM with Windows Server 2022.
Versions: App is at .netCore 7.0
Asp Net Core hosting bundle installed: 3.1.14
We had a WebSocket issue, it stopped working and refused to work, we spent weeks trying to find the issue in the end gave up and decided to completely re-install IIS. The web app was working fine in IIS it was only the websockets not working.
I exported backup of IIS configuration, also backup the C:\windows\system32\inetsrv\config folder.
I removed the IIS features, restarted the VM.
I then added IIS features back in.
In IIS the URL re-write module was now missing so I added that again.
Re-created the AppPool and sites again.
The website did not work, It was giving an error on the web.config section
```
<system.webServer>
    <handlers>
        <add name=""aspNetCore"" path=""*"" verb=""*"" modules=""AspNetCoreModuleV2"" resourceType=""Unspecified"" />
    </handlers>
    <aspNetCore processPath=""dotnet"" arguments="".\MyApp.dll"" stdoutLogEnabled=""false"" stdoutLogFile="".\logs\stdout"" hostingModel=""inprocess"" />
</system.webServer>
```

I then realize the AspNetCoreModuleV2 is not loaded in the IIS module, I did a Google abd found posts stating I must repare hosting bundle, which I did, about 100 time by now. But it did not fix the issue.

The app is a .Net Core 7.0 app. This is installed, SDK as Runtime, here is the dotnet --info listing:
```
C:\Windows\System32\inetsrv>dotnet --info
.NET SDK:
 Version:   7.0.410
 Commit:    8b4848c3b2

Runtime Environment:
 OS Name:     Windows
 OS Version:  10.0.20348
 OS Platform: Windows
 RID:         win10-x64
 Base Path:   C:\Program Files\dotnet\sdk\7.0.410\

Host:
  Version:      7.0.20
  Architecture: x64
  Commit:       0fb6ac59fb

.NET SDKs installed:
  7.0.410 [C:\Program Files\dotnet\sdk]

.NET runtimes installed:
  Microsoft.AspNetCore.App 3.1.14 [C:\Program Files\dotnet\shared\Microsoft.AspNetCore.App]
  Microsoft.AspNetCore.App 6.0.22 [C:\Program Files\dotnet\shared\Microsoft.AspNetCore.App]
  Microsoft.AspNetCore.App 7.0.11 [C:\Program Files\dotnet\shared\Microsoft.AspNetCore.App]
  Microsoft.AspNetCore.App 7.0.20 [C:\Program Files\dotnet\shared\Microsoft.AspNetCore.App]
  Microsoft.NETCore.App 3.1.14 [C:\Program Files\dotnet\shared\Microsoft.NETCore.App]
  Microsoft.NETCore.App 6.0.22 [C:\Program Files\dotnet\shared\Microsoft.NETCore.App]
  Microsoft.NETCore.App 7.0.11 [C:\Program Files\dotnet\shared\Microsoft.NETCore.App]
  Microsoft.NETCore.App 7.0.20 [C:\Program Files\dotnet\shared\Microsoft.NETCore.App]
  Microsoft.WindowsDesktop.App 7.0.20 [C:\Program Files\dotnet\shared\Microsoft.WindowsDesktop.App]

Other architectures found:
  x86   [C:\Program Files (x86)\dotnet]
    registered at [HKLM\SOFTWARE\dotnet\Setup\InstalledVersions\x86\InstallLocation]

Environment variables:
  Not set

global.json file:
  Not found

Learn more:
  https://aka.ms/dotnet/info

Download .NET:
  https://aka.ms/dotnet/download
```

I have uninstalled AspNetCoreModuleV2 100 times with these commands:
`>Appcmd.exe uninstall module AspNetCoreModuleV2`

`appcmd.exe install module /name:AspNetCoreModuleV2 /image:""C:\Program Files\IIS\Asp.Net Core Module\V2\aspnetcorev2.dll""`

After the uninstall I will repair the hosting bundle, or sometimes uninstall it and install again. I reset iis, restart the VM 100 times.

The website does not run I get error 500 in Chrome with message ""Failed to laod response data: No resource with given identifier found""

In the Event Viewer I get this message:
```
Log Name:      Application
Source:        IIS AspNetCore Module V2
Date:          07/12/2024 09:09:47
Event ID:      1034
Task Category: None
Level:         Error
Keywords:      Classic
User:          N/A
Computer:      APP01.ortec-ivorecht.nl
Description:
Could not load configuration. Exception message:
Unable to get required configuration section 'system.webServer/aspNetCore'. Possible reason is web.config authoring error.
Event Xml:
<Event xmlns=""http://schemas.microsoft.com/win/2004/08/events/event"">
  <System>
    <Provider Name=""IIS AspNetCore Module V2"" />
    <EventID Qualifiers=""0"">1034</EventID>
    <Version>0</Version>
    <Level>2</Level>
    <Task>0</Task>
    <Opcode>0</Opcode>
    <Keywords>0x80000000000000</Keywords>
    <TimeCreated SystemTime=""2024-12-07T08:09:47.4882438Z"" />
    <EventRecordID>50864</EventRecordID>
    <Correlation />
    <Execution ProcessID=""0"" ThreadID=""0"" />
    <Channel>Application</Channel>
    <Computer>APP01.ortec-ivorecht.nl</Computer>
    <Security />
  </System>
  <EventData>
    <Data>Could not load configuration. Exception message:
Unable to get required configuration section 'system.webServer/aspNetCore'. Possible reason is web.config authoring error.</Data>
    <Data>Process Id: 6764.</Data>
    <Data>File Version: 17.0.23237.11. Description: IIS ASP.NET Core Module V2. Commit: 70048beee894074e9718206a0017d9b080ee66ff</Data>
  </EventData>
</Event>
```

What I observe is in the backup of C:\windows\system32\inetsrv\config\schema that there used to be a file aspnetcore_schema_v2.xml which is no longer in the new C:\windows\system32\inetsrv\config\schema folder of the new installation.

Other things I have tried:
I went onto this page https://learn.microsoft.com/en-us/aspnet/core/host-and-deploy/iis/hosting-bundle?view=aspnetcore-7.0
Followed the link there to the [NET Core Hosting Bundle installer (direct download)](https://dotnet.microsoft.com/permalink/dotnetcore-current-windows-runtime-bundle-installer).
From there follow it to the older versions on this page https://dotnet.microsoft.com/en-us/download/dotnet and here select ""Out of support versions"" then get the .Net Core 7.0 installation and from there select the Windows x64 installation, download and install that.
Previously we used the .Net Core hosting bundle 3.14 (which is what another colelague wrote down in the installation notes they installed), as I mention above I downloaded also that and re-sintalled several time.

I have Google extensively read everything I can, asked ChatGPT in several ways, everybody says check the module is installed in IIS, which it is, and to repair hosting bundle, which I did 100 times.

I also added line
`<section name=""aspNetCore"" overrideModeDefault=""Allow"" />`
in the applicationHost.config under section `<sectionGroup name=""system.webServer"">`

This has been taking us days and wasting soooo much time! I HAVE to get this working!



### Expected Behavior

IIS should serve my web app

### Steps To Reproduce

_No response_

### Exceptions (if any)

Event Viewer error:
```
<Event xmlns=""http://schemas.microsoft.com/win/2004/08/events/event"">
  <System>
    <Provider Name=""IIS AspNetCore Module V2"" />
    <EventID Qualifiers=""0"">1034</EventID>
    <Version>0</Version>
    <Level>2</Level>
    <Task>0</Task>
    <Opcode>0</Opcode>
    <Keywords>0x80000000000000</Keywords>
    <TimeCreated SystemTime=""2024-12-07T08:09:47.4882438Z"" />
    <EventRecordID>50864</EventRecordID>
    <Correlation />
    <Execution ProcessID=""0"" ThreadID=""0"" />
    <Channel>Application</Channel>
    <Computer>APP01.ortec-ivorecht.nl</Computer>
    <Security />
  </System>
  <EventData>
    <Data>Could not load configuration. Exception message:
Unable to get required configuration section 'system.webServer/aspNetCore'. Possible reason is web.config authoring error.</Data>
    <Data>Process Id: 6764.</Data>
    <Data>File Version: 17.0.23237.11. Description: IIS ASP.NET Core Module V2. Commit: 70048beee894074e9718206a0017d9b080ee66ff</Data>
  </EventData>
</Event>
```

### .NET Version

.Net Core 7.0

### Anything else?

_No response_"
232,679b6a5fcbe8284d7a731779,issue,documentation -> generating release note,https://github.com/commons-app/apps-android-commons/issues/6025,"Release notes generation tools
One thing that takes quite a bit of time for me on each release is the generation of release notes.

I like to keep each entry as detailed yet in the most concise manner possible so that someone randomly hopping in could quickly get a gist of the changes that have gone in that particular release. I try to use AI tools like ChatGPT to generate the same by feeding it the relevant context like the issue, MR links amd get the relevant release notesand give it a final polish before settling on the release notes entry.

It becomes tedious when there are several chanhes and it is a mundane task for sure. I wanted to start this thread to know if anyone here is aware of amy tool that could potentially automate this process. ðŸ™‚ 

Note: I do use the changelog generated by GitHub's 'Generate release notes' feature as a starting point. It unfortunately requires to tag the release for the same to work. As I wish for the release notes to be present in our Changelog file before tagging the release, it gets a bit tricky. So, if you're aware of tools that could help do the job of GitHub's release note generation that would be helpful too. Do share them here."
233,679b6a5fcbe8284d7a731798,issue,software quality -> testing -> generating test data,https://github.com/ta4j/ta4j/issues/1240,"LSMA Indicator calculation result into incorrect values 

Describe the bug
---------------
The `LSMAIndicator` implementation in the current snapshot (0.18-SNAPSHOT) 
version calculates incorrect values for the Least Squares Moving Average (LSMA). When tested with a specific dataset, the results do not match the expected values.

The discrepancy is evident when using the `LSMAIndicator` with:
- Bar Count: 20
- Offset: 0

The results deviate from the expected LSMA values in the provided `NIFTY_50_with_LSMA.csv` file.

To Reproduce
------------
Steps to reproduce the behavior:
1. Clone the `ta4j` repository and build the snapshot version.
2. Use the `LSMAIndicator` with the following parameters:
   - `barCount = 20`
   - `offset = 0`
3. Apply the indicator to the dataset in the attached `NIFTY_50_04112024_11122024.csv`.
4. Compare the calculated LSMA values against the expected values in `NIFTY_50_with_LSMA.csv`.

Expected behavior
-----------------
The calculated LSMA values should align with the data in the `NIFTY_50_with_LSMA.csv` file for the given dataset.

Attached Files
--------------
- `NIFTY_50_04112024_11122024.csv`: The dataset used for testing.
- `NIFTY_50_with_LSMA.csv`: The correct LSMA values for comparison.

Additional Context
------------------
1. Problem Details: ""As suggested by ChatGPT"" Not sure about it but it generated correct csv matching the broker/exchange data on the subset provided
   - The `LSMAIndicator` implementation does not correctly calculate the slope and intercept of the regression line. This causes the LSMA values to deviate from the expected output.
   - The issue might involve inaccuracies in how the `sx` (time-weighted sum of price deviations) and `sy` (time-weighted sum of time deviations) are computed or applied in the final formula.

2. Verification:
   - The correct LSMA values provided in `NIFTY_50_with_LSMA.csv` were generated using ChatGPT for testing purposes. However, the exact source of the discrepancy in the current implementation is not identified.

Example Code to Reproduce the Issue
-----------------------------------
import org.ta4j.core.BarSeries;
import org.ta4j.core.BaseBarSeries;
import org.ta4j.core.Bar;
import org.ta4j.core.bars.BaseBarBuilder;
import org.ta4j.core.num.Num;
import org.ta4j.core.num.DoubleNumFactory;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.time.Instant;
import java.time.ZoneId;
import java.time.ZonedDateTime;
import java.time.format.DateTimeFormatter;
import java.util.Locale;

public class LSMAIssueTest {
    public static void main(String[] args) {
        try {
            // Load data into BarSeries
            BarSeries series = loadBarSeries(""NIFTY_50_04112024_11122024.csv"");

            // Initialize LSMAIndicator
            Indicator<Num> closePrice = new ClosePriceIndicator(series);
            LSMAIndicator lsma = new LSMAIndicator(closePrice, 20, 0);

            // Print calculated LSMA values
            for (int i = 0; i < series.getBarCount(); i++) {
                System.out.println(""Index: "" + i + "", LSMA: "" + lsma.getValue(i));
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    private static BarSeries loadBarSeries(String csvFile) {
        BarSeries series = new BaseBarSeries(""TestSeries"", DoubleNumFactory.getInstance());
        DateTimeFormatter formatter = DateTimeFormatter.ofPattern(""ddMMyyyy::HHmm"", Locale.ENGLISH);

        try (BufferedReader br = new BufferedReader(new FileReader(csvFile))) {
            String line = br.readLine(); // Skip header
            while ((line = br.readLine()) != null) {
                String[] fields = line.split(""	""); // Assuming tab-separated values

                // Parse fields
                ZonedDateTime dateTime = ZonedDateTime.ofInstant(
                        Instant.from(formatter.parse(fields[4])), ZoneId.systemDefault()
                );
                double open = Double.parseDouble(fields[6]);
                double high = Double.parseDouble(fields[7]);
                double low = Double.parseDouble(fields[8]);
                double close = Double.parseDouble(fields[9]);
                double volume = Double.parseDouble(fields[5]);

                // Build and add Bar
                Bar bar = new BaseBarBuilder()
                        .timePeriod(java.time.Duration.ofMinutes(1))
                        .endTime(dateTime.toInstant())
                        .openPrice(open)
                        .highPrice(high)
                        .lowPrice(low)
                        .closePrice(close)
                        .volume(volume)
                        .build();
                series.addBar(bar);
            }
        } catch (IOException e) {
            throw new RuntimeException(""Error reading CSV file: "" + csvFile, e);
        }

        return series;
    }
}

Environment
-----------
- Library version: Latest snapshot version
- Java version: [java 21]
- Operating System: [macOS]

Suggested Fix or Workaround
---------------------------
The `LSMAIndicator` logic for slope and intercept calculation may need to be reviewed. The correct formula has been independently verified and can be cross-checked against the `NIFTY_50_with_LSMA.csv`.

Suggestions for Improvement:
1. Review the regression calculation in the `LSMAIndicator`, particularly the slope (`sx`) and intercept logic, for potential inaccuracies.


[NIFTY_50_04112024_11122024.csv](https://github.com/user-attachments/files/18117222/NIFTY_50_04112024_11122024.csv)
[NIFTY_50_with_LSMA.csv](https://github.com/user-attachments/files/18117224/NIFTY_50_with_LSMA.csv)

![NIFTY 50_INDICES_256265_2024-12-13_02-12-31](https://github.com/user-attachments/assets/fb6c2a1c-6a4c-4daf-9ffd-7f7fb714c1b0)


"
234,679b6a5fcbe8284d7a731828,issue,learning -> how to use a library/framework/tool -> generating code examples,https://github.com/chrome-php/chrome/issues/671,"Access page opened in new window
Hello,

There's a page where I click on an element  and Javascript opens new page in new window. How to access it? 
With the help of ChatGPT I have this code


```
    function openChart() {
        try {
            $session = $this->page->getSession();
            $discoverTargetsMessage = new Message('Target.setDiscoverTargets', ['discover' => true]);
            $session->sendMessage($discoverTargetsMessage);

            $this->js('document.querySelector(""a[class^=goToChartButton]"").click()'); // click here
            $targets = $this->browser->getTargets();


            $newTarget = null;
            foreach ($targets as $target) {
                $targetType = $target->getTargetInfo('type');
                $targetId = $target->getTargetInfo('targetId');
                if ($targetType === 'page' && $targetId !== $this->page->getSession()->getTargetId()) {
                    $newTarget = $target;
                    break;
                }
            }



            if ($newTarget) {
                $session = $this->browser->getConnection()->createSession($targetId);


        
                // waiting for page loaded
                $session->on('Page.loadEventFired', function () use ($session) {
                    $response = $session->sendMessageSync(new Message('Runtime.evaluate', [
                        'expression' => 'document.documentElement.outerHTML'
                    ]));
                    echo $response['result']['value']; 
                });
            } else {
                echo ""Error opening chart page\n"";
            }

        } 
    }
```

The problem is that
**Uncaught HeadlessChromium\Exception\CommunicationException: Message could not be sent. Reason: the connection is closed.**

How to fix this? And, if it possible, I would prefer to substitute $this->page so that I could use it outside of the function later."
235,679b7493cbe8284d7a731b38,issue,feature implementation/enhancement -> feature enhancement -> user experience -> gui-related improvements,https://github.com/fluentform/fluentform/issues/506,"Simple forced uppercase?
I'd like to see something of a styling option to force an input field to be automatically converted to uppercase text.
Reason: my form requires the user to enter their car's licenseplate information, but a as an example: lowercase 'L' looks a lot like an uppercase 'i'. l > I See the difference? I don't.

I'm not a programmer, I asked ChatGPT what to do, it told me to add ""`uppercase-text`"" to the field's Element Class and then add ""`.uppercase-text input {text-transform:uppercase;}`"" to the additional CSS part.
But that doesn't turn lowercase into uppercase unfortunately, and seeing as I've got no CSS knowledge or experience, I don't know what to do about it.

So yeah, that's my feature request; an easy way of forcing an input field's formatting."
236,679b757dcbe8284d7a731dad,issue,process -> automating commit/issue/pr creation -> translating commit/issue/pr description,https://github.com/mrcanelas/tmdb-addon/issues/48,"Idea to enhance the addon ?
Hello,
Iâ€™ve been using your addon for a very long time because Iâ€™m French, and itâ€™s very useful for us French people. However, Iâ€™d like to share some improvement ideas for the addon.

- In the ""Discover"" section, add a category called ""Category"" to search film by genre (horror, comedy, drama, thriller, etc.).
- And, besides the popular Movies/Series on the homepage, a category for trending Movies/Series and also a category for recent Movies/Series? These same categories could also be used in the Discover tab instead of just having Movies and Series available. Do you understand what I mean? (Iâ€™m using ChatGPT to translate this, so I hope itâ€™s correct.)

Here screenshot to understand more what i want to mean :
![image](https://github.com/user-attachments/assets/e879d33a-23b0-48ed-a593-02ded2573e3a)
On the **red circle** here, you can add Trend/Recent/populare Film/Serie and so we can search with other category 
On the **blue circle** you can add here the first idea i speak
And in the **green circle** you can switch some of the option in the blue circle and add another

For now, I only have these two ideas, but I think itâ€™s already a lot of work. If you agree to add them, I might submit other ideas in the future that could interest you.

Thank you in advance :D

Best regards,
razeN."
237,679b757dcbe8284d7a731f37,issue,software quality -> fixing -> functional bugs,https://github.com/tensorflow/tensorflow/issues/84104,"KeyError: ""There is no item named 'PetImages\\Cat\\0.jpg' in the archive"" When Running TensorFlow Locally(CPU) on Anaconda in VS Code.
### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

window11

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am a beginner and encountering an issue while trying to run TensorFlow locally using Anaconda in VS Code. The same code runs smoothly on Google Colab, but when executed locally, it fails during the dataset download process with the following error.

What I've Tried
Re-downloading TensorFlow and TensorFlow Datasets to ensure they are up to date.
Manually Unzipping the Dataset and verifying if 'PetImages/Cat/0.jpg' exists in the archive.
Re-adjusting Python, TensorFlow, and TensorFlow Datasets Versions to match those in Colab by using Python 3.10.11 instead of Python 3.10.12.
Recreating the Virtual Environment in Anaconda to ensure a clean setup.
Downloading the Cats vs Dogs Dataset from Different Sources, but the issue persists.
Asking ChatGPT for assistance, but the issue remains unresolved.

Additional Information
On Google Colab, the same code runs without any issues, and the dataset downloads successfully.
In VS Code, the error consistently occurs during the dataset download process, indicating that 'PetImages/Cat/0.jpg' is missing from the archive.
Network Stability: I have a stable internet connection, and downloads complete without interruption, but the error persists.

Questions
Why does the KeyError occur in VS Code but not in Google Colab?
Could this be related to the way the dataset is being downloaded or unzipped locally?
Are there any compatibility issues between the Python/TensorFlow versions and the dataset?
Request for Help
I would greatly appreciate any guidance or suggestions on how to resolve this issue. Thank you in advance for your assistance!


### Standalone code to reproduce the issue

```shell
import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np



CatsVsDogs_OrgData, info=tfds.load(name='cats_vs_dogs', with_info=True,
                  split=tfds.Split.TRAIN)
```


### Relevant log output

```shell
PS C:\Users\jbb86\æ¡Œé¢\åœ–æ¨£è¾¨è­˜> & C:/Users/jbb86/æ¡Œé¢/åœ–æ¨£è¾¨è­˜/.venv/Scripts/python.exe c:/Users/jbb86/æ¡Œé¢/åœ–æ¨£è¾¨è­˜/.venv/CatsVsDogs.py
2025-01-03 22:38:53.599253: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-03 22:38:54.247816: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\Users\jbb86\tensorflow_datasets\cats_vs_dogs\4.0.1...
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 824887076/824887076 [00:00<00:00, 803489819418.28 MiB/s]
Dl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 974.06 url/s]
Generating splits...:   0%|                                   2025-01-03 22:38:55.958212: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""c:\Users\jbb86\æ¡Œé¢\åœ–æ¨£è¾¨è­˜\.venv\CatsVsDogs.py"", line 7, in <module>
    CatsVsDogs_OrgData, info=tfds.load(name='cats_vs_dogs', with_info=True,
  File ""C:\Users\jbb86\æ¡Œé¢\åœ–æ¨£è¾¨è­˜\.venv\lib\site-packages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__
    return function(*args, **kwargs)
  File ""C:\Users\jbb86\æ¡Œé¢\åœ–æ¨£è¾¨è­˜\.venv\lib\site-packages\tensorflow_datasets\core\load.py"", line 661, in load
    _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)
  File ""C:\Users\jbb86\æ¡Œé¢\åœ–æ¨£è¾¨è­˜\.venv\lib\site-packages\tensorflow_datasets\core\load.py"", line 517, in _download_and_prepare_builder
    dbuilder.download_and_prepare(**download_and_prepare_kwargs)
  File ""C:\Users\jbb86\æ¡Œé¢\åœ–æ¨£è¾¨è­˜\.venv\lib\site-packages\tensorflow_datasets\core\logging\__init__.py"", line 176, in __call__
    return function(*args, **kwargs)
  File ""C:\Users\jbb86\æ¡Œé¢\åœ–æ¨£è¾¨è­˜\.venv\lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 756, in download_and_prepare
    self._download_and_prepare(
  File ""C:\Users\jbb86\æ¡Œé¢\åœ–æ¨£è¾¨è­˜\.venv\lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 1752, in _download_and_prepare
    split_infos = self._generate_splits(dl_manager, download_config)
  File ""C:\Users\jbb86\æ¡Œé¢\åœ–æ¨£è¾¨è­˜\.venv\lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 1727, in _generate_splits
    future = split_builder.submit_split_generation(
  File ""C:\Users\jbb86\æ¡Œé¢\åœ–æ¨£è¾¨è­˜\.venv\lib\site-packages\tensorflow_datasets\core\split_builder.py"", line 436, in submit_split_generation
    return self._build_from_generator(**build_kwargs)
  File ""C:\Users\jbb86\æ¡Œé¢\åœ–æ¨£è¾¨è­˜\.venv\lib\site-packages\tensorflow_datasets\core\split_builder.py"", line 496, in _build_from_generator
    for key, example in utils.tqdm(
  File ""C:\Users\jbb86\æ¡Œé¢\åœ–æ¨£è¾¨è­˜\.venv\lib\site-packages\tqdm\std.py"", line 1181, in __iter__
    for obj in iterable:
  File ""C:\Users\jbb86\æ¡Œé¢\åœ–æ¨£è¾¨è­˜\.venv\lib\site-packages\tensorflow_datasets\image_classification\cats_vs_dogs.py"", line 117, in _generate_examples
    new_fobj = zipfile.ZipFile(buffer).open(fname)
  File ""C:\Users\jbb86\AppData\Local\Programs\Python\Python310\lib\zipfile.py"", line 1516, in open
    zinfo = self.getinfo(name)
  File ""C:\Users\jbb86\AppData\Local\Programs\Python\Python310\lib\zipfile.py"", line 1443, in getinfo
    raise KeyError(
KeyError: ""There is no item named 'PetImages\\\\Cat\\\\0.jpg' in the archive""
```
"
238,679b757dcbe8284d7a731f96,issue,software quality -> fixing -> functional bugs,https://github.com/raspiblitz/raspiblitz/issues/4916,"SSH connection refused
**Describe the bug**
A clear and concise description about the issue you encounter.

I can no longer ssh into raspiblitz on raspberry pi 5. I always get a connection refused message.
**To Reproduce**
Steps to reproduce the behavior.
![IMG_4720](https://github.com/user-attachments/assets/fe42ec28-9353-4345-b1b3-7d3d50070115)

I was having issues with some of the applications not working and used ChatGPT to try and address them (checking channels using raspiblitz menu in command didnâ€™t work, loop not working in RTL, among other things). Chatgpt somewhat buggered it up more so I decided I would start over. Backed up my LND and reflashed the SD card. After booting I was able to ssh in once, and then after that I get the connection refused message. 
-I have attempted adding an ssh file with no extension to the boot partition.
-I have turned off firewall and tried again with no luck.
-I have tried booting with an external monitor plugged in but I only get system status messages (pictures
 is attached) and the keyboard doesnâ€™t do anything (using a Bluetooth usb for keyboard) so I canâ€™t do anything with a command line.
-I was able to access the Web UI but now Iâ€™m stuck on waitsync with the note of provision done. I wasnâ€™t able to backup my LND again so the backup is probably a day or so old.

Iâ€™m fine with starting over except that I have funds on the lightning network that I donâ€™t want to lose. I went into the SSD and copied off the app-data folder and the lnd folder but Iâ€™m afraid to wipe the SSD and SD to start over in risk of losing the funds.

**Debug file**
If possible, please add a debug report. To do that, drag a text file with the debug report in here.

None."
239,679b757dcbe8284d7a731f9b,issue,software quality -> fixing -> supporting debugging,https://github.com/FrescoLogic/FL2000/issues/99,"VGA with VGA/USB 3.0 adapter to USB 2.0 !!
Hello everyone!
Thank you in advance for the space to share and solve the problems and for your time in reading them.

I'm just starting out in programming and I'm trying something (which I understand isn't possible at the moment) but I'm sharing my doubts in case there's someone else out there with the same problem (or solution).

I am trying to connect an extra 3rd screen (in extension mode) to my HP 15ay notebook with Kali Linux as operating system (it only has one HDMI port to which I have a 2nd screen connected that works correctly and I only have USB 2.0 ports available...It also doesn't have USB C ports), via a VGA/USB 3.0 connector, knowing their incompatibilities and limitations...

The adapter is  (lsusb) ""Bus 002 Device 002: ID 1d5c:2000 Fresco Logic FL2000/FL2000DX VGA/DVI/HDMI Adapter"" (which I understand is compatible with Kali Linux but I have problems with the drivers).

I installed the Driver for Fresco Logic FL2000, downloaded and installed the necessary dependencies

sudo apt update
sudo apt install git build-essential dkms linux-headers-linux-headers-6.11.2-rt-amd64

I cloned the driver repository

git clone https://github.com/FrescoLogic/FL2000.git

but it doesn't recognize the monitor and I can't compile...

I installed and edited the Kernel headers and the ""Makefile"" file

nano Makefile
KERNEL_PATH = /usr/src/linux-headers-linux-headers-6.11.2-rt-amd64

and when compiling the Kernel module and trying to load the module by running ""make"", it does not generate the file called fl2000.ko.
I understand It is necessary to adjust the driver code to be compatible with my kernel version...

After being able to solve almost all the errors (EXCEPT ONE!) with the help of chatgpt (such as problems associated with the get_user_pages, mmap_sem, vm_map_ram functions), modifying the fl2000_surface.c file, I understand that the problem I need to solve is with the ""flags"" (vm_flags) that are not defined or available in my kernel version...yet!

I wanted to know if there is an updated version of the driver code (FL2000) compatible with my kernel (since I don't want to compile it with an older version..)


So this is the error I get when running ""make"":

home/kali/FL2000/src/fl2000_fops.c: In function â€˜set_vm_flagsâ€™:
/home/kali/FL2000/src/fl2000_fops.c:136:19: error: assignment of read-only member â€˜vm_flagsâ€™
  136 |     vma->vm_flags = old_flags | flags;
      |                   ^
/home/kali/FL2000/src/fl2000_fops.c: In function â€˜clear_vm_flagsâ€™:
/home/kali/FL2000/src/fl2000_fops.c:141:19: error: assignment of read-only member â€˜vm_flagsâ€™
  141 |     vma->vm_flags = old_flags & ~flags;
      |                   ^
make[3]: *** [/usr/src/linux-headers-6.11.2-common-rt/scripts/Makefile.build:249: /home/kali/FL2000/src/fl2000_fops.o] Error 1
make[2]: *** [/usr/src/linux-headers-6.11.2-common-rt/Makefile:1951: /home/kali/FL2000/src] Error 2
make[1]: *** [/usr/src/linux-headers-6.11.2-common-rt/Makefile:236: __sub-make] Error 2
make[1]: Leaving directory '/usr/src/linux-headers-6.11.2-rt-amd64'
make: *** [Makefile:42: all] Error 2

And this is how I got the file ""fl2000_fops.c :

// fl2000_fops.c
//
// (c)Copyright 2017, Fresco Logic, Incorporated.
//
// Purpose:
//

#include ""fl2000_include.h""

#define USE_VM_INSERT_PFN       0
 
int fl2000_open(struct inode * inode, struct file * file)
{
        int const minor = iminor(inode);
        struct usb_interface * interface;
        struct dev_ctx * dev_ctx;
        int ret_val;
        uint32_t open_count;

        ret_val = 0;
        interface = usb_find_interface(&fl2000_driver, minor);
        if (!interface) {
                dbg_msg(TRACE_LEVEL_ERROR, DBG_PNP,
                        ""no interface?."");
                ret_val = -ENODEV;
                goto exit;
        }

        dev_ctx = usb_get_intfdata(interface);
        if (!dev_ctx) {
                dbg_msg(TRACE_LEVEL_ERROR, DBG_PNP,
                        ""no dev_ctx?."");
                ret_val = -ENODEV;
goto exit;
        }

        open_count = InterlockedIncrement(&dev_ctx->open_count);

        if (open_count > 1) {
                dbg_msg(TRACE_LEVEL_ERROR, DBG_PNP,
                        ""open_count(%u) exceeds 1?"", open_count);
                InterlockedDecrement(&dev_ctx->open_count);
                ret_val = -EBUSY;
                goto exit;
        }

        dbg_msg(TRACE_LEVEL_INFO, DBG_PNP, ""open_count(%u)"", open_count);
        dbg_msg(TRACE_LEVEL_INFO, DBG_PNP,
                ""render_ctx: free(%u), ready(%u), busy(%u), surface(%u)"",
                dev_ctx->render.free_list_count,
                dev_ctx->render.ready_list_count,
                dev_ctx->render.busy_list_count,
                dev_ctx->render.surface_list_count
                );

        file->private_data = dev_ctx;
        kref_get(&dev_ctx->kref);
exit:
        return ret_val;
}

int fl2000_release(struct inode * inode, struct file * file)
{
        struct dev_ctx * const dev_ctx = file->private_data;
        struct render_ctx * render_ctx;
        uint32_t i;
uint32_t open_count;

        if (dev_ctx == NULL)
                return -ENODEV;

        // wake up any sleeping process.
        //
        if (waitqueue_active(&dev_ctx->ioctl_wait_q)) {
                dbg_msg(TRACE_LEVEL_INFO, DBG_PNP,
                        ""wake up pending process"");
                wake_up_interruptible(&dev_ctx->ioctl_wait_q);
        }

        fl2000_render_stop(dev_ctx);
        fl2000_dongle_stop(dev_ctx);
        fl2000_surface_destroy_all(dev_ctx);

        /*
         * bug12414: on customer's platform, the underlying stack(eg. Asmed>
         * failed to complete the previously sent URB. We re-initialize
         * the render_ctx into default state during client file close opera>
         */
        if (dev_ctx->render.ready_list_count != 0) {
                dbg_msg(TRACE_LEVEL_INFO, DBG_PNP,
                        ""ready_list_count(%u)"",
                        dev_ctx->render.ready_list_count);
        }

        INIT_LIST_HEAD(&dev_ctx->render.free_list);
        spin_lock_init(&dev_ctx->render.free_list_lock);
        dev_ctx->render.free_list_count = 0;

        INIT_LIST_HEAD(&dev_ctx->render.ready_list);
spin_lock_init(&dev_ctx->render.ready_list_lock);
        dev_ctx->render.ready_list_count = 0;

        INIT_LIST_HEAD(&dev_ctx->render.busy_list);
        spin_lock_init(&dev_ctx->render.busy_list_lock);
        dev_ctx->render.busy_list_count = 0;

        for (i = 0; i < NUM_OF_RENDER_CTX; i++) {
                render_ctx = &dev_ctx->render.render_ctx[i];

                INIT_LIST_HEAD(&render_ctx->list_entry);
                render_ctx->dev_ctx = dev_ctx;
                render_ctx->pending_count = 0;

                list_add_tail(&render_ctx->list_entry,
                        &dev_ctx->render.free_list);

                InterlockedIncrement(&dev_ctx->render.free_list_count);
        }
        open_count = InterlockedDecrement(&dev_ctx->open_count);
        dbg_msg(TRACE_LEVEL_INFO, DBG_PNP, ""open_count(%u)"", open_count);

        kref_put(&dev_ctx->kref, fl2000_module_free);

        return 0;
}

/*
 * this function is triggered from vm_mmap() in fl2000_ioctl_test_alloc_sur>
 * finish the last step of mapping system ram.
 * treat the memory as device ram.
 */
#include <linux/mm.h> // Incluye las definiciones para estructuras de memor>

static void set_vm_flags(struct vm_area_struct *vma, unsigned long flags) {
    unsigned long old_flags = vma->vm_flags;
    vma->vm_flags = old_flags | flags;
}

static void clear_vm_flags(struct vm_area_struct *vma, unsigned long flags)>
    unsigned long old_flags = vma->vm_flags;
    vma->vm_flags = old_flags & ~flags;
}

#include <linux/fs.h>
#include <linux/kernel.h>  // Para usar pr_info y pr_err

// FunciÃ³n auxiliar para configurar las banderas
static void fl2000_configure_vm_flags(struct vm_area_struct *vma, unsigned >
    // Crear una copia temporal de vm_flags
    unsigned long temp_flags = vma->vm_flags;

    // Modificar la copia temporal
    temp_flags |= set_flags;   // Establecer las banderas indicadas
    temp_flags &= ~clear_flags; // Limpiar las banderas indicadas

    // Reasignar al campo original (indirectamente, si es posible)
    // Dependiendo de las restricciones del kernel, podrÃ­as necesitar un pa>
    vma->vm_flags = temp_flags;
}


int fl2000_mmap(struct file *file, struct vm_area_struct *vma)
{
    struct dev_ctx *const dev_ctx = file->private_data;
unsigned long len = vma->vm_end - vma->vm_start;
    unsigned long num_pages = (len + PAGE_SIZE - 1) >> PAGE_SHIFT;
    unsigned int ret_val = 0;
    unsigned int i;

    pr_info(""vm_start(0x%lx), vm_end(0x%lx), num_pages(0x%lx)\n"",
            vma->vm_start, vma->vm_end, num_pages);

    vma->vm_private_data = dev_ctx;

    // Establecer las banderas manualmente en vma->vm_flags
    static int fl2000_mmap(struct file *file, struct vm_area_struct *vma) {
    // Otras operaciones de mmap
    ...
    // Configurar las banderas
    fl2000_configure_vm_flags(vma, VM_DONTEXPAND | VM_DONTDUMP, VM_PFNMAP);

    // Continuar con el resto de la funciÃ³n
    ...
    return 0;
}

    for (i = 0; i < num_pages; i++) {
        unsigned long usr_addr = vma->vm_start + (i << PAGE_SHIFT);
        struct page *page = dev_ctx->start_page + i;

        ret_val = vm_insert_page(vma, usr_addr, page);
        if (ret_val) {
            pr_err(""vm_insert_page(usr_addr 0x%lx), page_count(%u) failed %>
                   usr_addr, page_count(page), ret_val);
            break;
        }
    }
pr_info(""vm_flags(0x%lx)\n"", vma->vm_flags);
    return ret_val;
}

 
Thank you very much in advance and I hope the problem can be understood and I would appreciate the solution just to know that it is possible!

Otherwise I will buy a DockStation or another PC, which is what I am trying to avoid, haha, thanks, greetings!















"
240,679b760ecbe8284d7a731fcd,issue,learning -> how to use a library/framework/tool,https://github.com/FelixHenninger/lab.js/issues/253,"Help with assingning participants to experimental and control groups
Hello everyone,
I'm Zeynep. I'm trying to design an experiment for my master's thesis. Unfortunately I don't know Javascript. So I'm struggling a little bit finding my way through lab.js.
Basically I want to know how to randomly assign participants to two conditions on lab.js.

To better explain myself I want to mention my experiment.
Simply there are 2 conditions: Anger and Sadness. I want half of the participant to see Anger condition but not Sadness, and the other half vice versa; to see Sadness condition but not Anger. 

So I build my hierarchy as follows:

Main_Loop
   a. Anger_Loop
         Sequence
            Page1
            Page2
            Page3
            Page4
   b. Sadness_Loop
         Sequence
            Page1
            Page2
            Page3
            Page4

In Main_Loop Content section:
I wrote Anger_Loop to parameter0 and Sadness_Loop to parameter1
In the line below I wrote again Anger_Loop and Sadness Loop.
I wrote ""1"" to sample box and chose 'Sampled without replacement (in blocks)'

I tried to get help from ChatGPT and it gave me some codes but it didn't work. It shows Anger_Loop in every trial. 
The scripts I tried:

const sequence = Math.random() < 0.5 ? 'Anger_Loop' : 'Sadness_Loop';
console.log('selected loop:', sequence);
return sequence;

------------------------------------------

const sequence = Math.random() < 0.5 ? 'Anger_Loop' : 'Sadness_Loop';
return sequence;
if (Math.random() < 0.5) {
    return 'Anger_Loop';
} else {
    return 'Sadness_Loop';
}


Can you see where the mistake is? What can I do?

I'd appreciate your help. Thanks in advance!"
241,679b760ecbe8284d7a73206d,issue,"feature implementation/enhancement -> prototyping,software quality -> refactoring",https://github.com/onyx-dot-app/onyx/issues/3616,"Web Connector: Inadequate Mimics in requests.get and Ineffective Playwright Mimics for Cloudflare-Protected Hosts
### Analysis and Attempt to Resolve Website Scraping Errors

Out of curiosity and professional interest, I attempted to scrape the entire company's support website (customer knowledgebase) but encountered the following error:

```
RuntimeError: Failed to fetch 'https://xxxxx/': Error accessing https://xxxxx/: Forbidden (403) for https://xxxx/
```

I can provide the full URL to any developer interested in addressing this issue, as it pertains to my company's public customer support website hosted on ZenDesk behind Cloudflare as I suppose based on headers). However, I prefer not to share it publicly here.

Response headers from curl (obfuscated a bit):
```
< x-zendesk-origin-server: app-server-xxxx-xxxx
< x-zendesk-processed-host-header: xxxx
< X-Zendesk-Zorg: yes
< Set-Cookie: __cf_bm=xxxx; path=/; expires=Mon, 06-Jan-25 23:14:31 GMT; domain=xxxx; HttpOnly; Secure; SameSite=None
< Report-To: {""endpoints"":[{""url"":""https:\/\/xxxx.cloudflare.com\/report\/v4?s=xxxx""}],""group"":""xxxx"",""max_age"":604800}
< NEL: {""success_fraction"":0.01,""report_to"":""xxxx"",""max_age"":604800}
< Set-Cookie: __cfruid=xxxx; path=/; domain=xxxx; HttpOnly; Secure; SameSite=None
< Set-Cookie: _cfuvid=xxxx; path=/; domain=xxxx; HttpOnly; Secure; SameSite=None
< Server: cloudflare
```

### Initial Observations

Interestingly, a simple `curl` request worked without issue:

```
curl -L -v https://xxxxx/ -A ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36""
```

Even with basic headers, the request succeeded:

```
> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36
> Accept: */*
```

### Debugging and Modifications

I began troubleshooting `/backend/onyx/connectors/web/connector.py` to identify the root cause. My initial modification involved adding **User-Agent** and **Accept** headers to all `requests.get` calls:

```python
headers = {
    ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"",
    ""Accept"": ""*/*""
}
response = requests.get(url, headers=headers)
```

Unfortunately, this change did not resolve the issue. The function `check_internet_connection` still returned the same **403 Forbidden** error when fetching the first URL.

### Playwright Integration

Suspecting that `requests` was insufficient to bypass the restrictions, I used ChatGPT to refactor `/backend/onyx/connectors/web/connector.py` to leverage **Playwright**. The idea was to utilize Playwright's browser mimics to bypass restrictions.

However, the problem persisted, and the **403 Forbidden** error continued to occur.

### Intercepting and Analyzing Headers

I hypothesized that Playwright might still be sending headers that Cloudflare identified as ""bot-like."" To investigate further, I intercepted and logged all headers with the following code:

```python
page.on(""request"", lambda request: print(f""Request headers for {request.url}: {request.headers}""))
```

This revealed some suspicious headers that Playwright added by default to mimic a browser more effectively:

```
{'sec-ch-ua': '""Not A(Brand"";v=""99"", ""HeadlessChrome"";v=""121"", ""Chromium"";v=""121""', 
 'sec-ch-ua-mobile': '?0', 
 'sec-ch-ua-platform': '""Linux""'}
```

These headers likely triggered Cloudflare's detection mechanisms.

### Header Interception and Customization

Using ChatGPT's suggestions, I implemented a method to intercept and customize headers for outgoing requests. This approach replaced the problematic headers while retaining critical ones like **User-Agent**. The modified code is as follows:

```python
page.route(""**/*"", lambda route, request: route.continue_(
    headers={
        ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"",
        ""Accept"": ""text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"",
        ""Accept-Language"": ""en-US,en;q=0.9"",
        ""Connection"": ""keep-alive"",
    }
))
```

(Note: The **Accept** header, while seemingly unnecessary, was included based on ChatGPT's recommendations and left unchanged since the code works as intended.)

### Current State and Next Steps

Currently, I have a functional prototype inspired by ChatGPT, but the code is not well-polished. I would greatly appreciate it if someone could thoroughly review and refine this implementation before integrating it into production. The concept is effective, but the code requires significant improvements. Apologies for its current state.

**Attachment**:  
[connector.zip](https://github.com/user-attachments/files/18325809/connector.zip)

About installation:
**Software**: Locally hosted on Ubuntu 24.04.1 LTS + docker + ollama + llama3.2 / mistral
**Hardware**: 12th Gen Intel(R) Core(TM) i9-12900, 64Gb RAM, NVIDIA GeForce RTX 3060 Ti 8Gb VRAM, Samsung SSD 980 PRO 1TB, WDC WD30PURZ-85A 2.7 TB"
242,679b760ecbe8284d7a7321ea,issue,feature implementation/enhancement -> migrating/reusing code,https://github.com/ukushu/TextProgressBar/issues/3,"Here's a .NET9 version
Thanks for your great work. It was just what I was looking for.

With a little help from ChatGPT I made a .NET9 version which still works very nicely.
I thought I share it here for others to use:

```csharp
using System.ComponentModel;

#pragma warning disable WFO1000

namespace WinFormApplication;

public class TextProgressBar : ProgressBar
{
    private Font _textFont = new(FontFamily.GenericSerif, 11, FontStyle.Bold | FontStyle.Italic);
    private SolidBrush _textBrush = new(Color.Black);
    private SolidBrush _progressBrush = new(Color.LightGreen);
    private ProgressBarDisplayMode _visualMode = ProgressBarDisplayMode.CurrProgress;
    private string _customText = string.Empty;

    [Category(""Appearance"")]
    [Description(""Font of the text displayed on the progress bar."")]
    public Font TextFont
    {
        get => _textFont;
        set
        {
            _textFont = value ?? throw new ArgumentNullException(nameof(TextFont));
            Invalidate();
        }
    }

    [Category(""Appearance"")]
    [Description(""Color of the text displayed on the progress bar."")]
    public Color TextColor
    {
        get => _textBrush.Color;
        set
        {
            if (_textBrush.Color != value)
            {
                _textBrush.Dispose();
                _textBrush = new SolidBrush(value);
                Invalidate();
            }
        }
    }

    [Category(""Appearance"")]
    [Description(""Color of the progress bar."")]
    public Color ProgressColor
    {
        get => _progressBrush.Color;
        set
        {
            if (_progressBrush.Color != value)
            {
                _progressBrush.Dispose();
                _progressBrush = new SolidBrush(value);
                Invalidate();
            }
        }
    }

    [Category(""Behavior"")]
    [Description(""Defines the text display mode of the progress bar."")]
    public ProgressBarDisplayMode VisualMode
    {
        get => _visualMode;
        set
        {
            if (_visualMode != value)
            {
                _visualMode = value;
                Invalidate();
            }
        }
    }

    [Category(""Behavior"")]
    [Description(""Custom text to display on the progress bar."")]
    public string CustomText
    {
        get => _customText;
        set
        {
            _customText = value;
            Invalidate();
        }
    }

    public TextProgressBar()
    {
        SetStyle(ControlStyles.UserPaint | ControlStyles.OptimizedDoubleBuffer | ControlStyles.AllPaintingInWmPaint,
            value: true);
    }

    protected override void OnPaint(PaintEventArgs e)
    {
        var g = e.Graphics;

        DrawProgressBar(g);
        DrawText(g);
    }

    private void DrawProgressBar(Graphics g)
    {
        var rect = ClientRectangle;

        ProgressBarRenderer.DrawHorizontalBar(g, rect);
        rect.Inflate(-3, -3);

        if (Value > 0)
        {
            var fillWidth = (int)((Value - Minimum) / (float)(Maximum - Minimum) * rect.Width);
            var fillRect = new Rectangle(rect.X, rect.Y, fillWidth, rect.Height);
            g.FillRectangle(_progressBrush, fillRect);
        }
    }

    private void DrawText(Graphics g)
    {
        if (VisualMode == ProgressBarDisplayMode.NoText)
            return;

        var text = GetDisplayText();
        var textSize = g.MeasureString(text, _textFont);
        var location = new PointF((Width - textSize.Width) / 2, (Height - textSize.Height) / 2);

        g.DrawString(text, _textFont, _textBrush, location);
    }

    private string GetDisplayText()
    {
        return VisualMode switch
        {
            ProgressBarDisplayMode.Percentage => $""{(int)((Value - Minimum) / (float)(Maximum - Minimum) * 100)}%"",
            ProgressBarDisplayMode.CurrProgress => $""{Value}/{Maximum}"",
            ProgressBarDisplayMode.TextAndPercentage =>
                $""{_customText}: {(int)((Value - Minimum) / (float)(Maximum - Minimum) * 100)}%"",
            ProgressBarDisplayMode.TextAndCurrProgress => $""{_customText}: {Value}/{Maximum}"",
            _ => _customText,
        };
    }

    protected override void Dispose(bool disposing)
    {
        if (disposing)
        {
            _textBrush.Dispose();
            _progressBrush.Dispose();
            _textFont.Dispose();
        }

        base.Dispose(disposing);
    }

    public enum ProgressBarDisplayMode
    {
        NoText,
        Percentage,
        CurrProgress,
        CustomText,
        TextAndPercentage,
        TextAndCurrProgress,
    }
}

```"
243,679b769ccbe8284d7a732232,issue,software quality -> fixing -> functional bugs,https://github.com/unchihugo/FluentFlyout/issues/47,"[BUG] The flyouts hide timer doesn't reset when called while already shown
**Describe the bug**
Easiest to describe by an example:
1. Press Caps Lock to show the flyout - flyout should be visible for, e.g., 3 seconds
2. Press Caps Lock while the flyout is shown - the 3-second timer should reset here, but it doesn't.

This is inconvenient, especially when you press the Caps Lock just before the timer expires (e.g., at 2.8 seconds). The disappear animation has already started, and the update isn't even visible at that point.

**Desktop (please complete the following information):**
 - OS: Windows 11
 - Version [e.g. v1.2.2]

**Additional context**
I haven't worked with C# and WPF, but I took the time to look at the code for showing the flyout, specifically the unit `LockWindows.xaml.cs`. Since I'm not familiar with the implemented mechanisms, I asked ChatGPT to alter the behavior to work as described above. This was his response:
```cs
/*
  Solution
  To properly reset the timer and ensure the behavior you want, you can enforce proper cancellation and waiting for the previous execution to complete before starting a new one.
  
  Hereâ€™s how you can fix it:
  
  Track the active task and ensure it completes or cancels properly.
  Use a lock to prevent overlapping executions.
  
  Key Changes:
  activeTask: Tracks the currently running task for ShowLockFlyout.
  lockObj: Ensures thread safety when managing cts and activeTask.
  Cancel and Restart: The method waits for the previous task to be canceled or completes the cancellation before starting a new one.
  
  Benefits:
  Timer Resets: Calling ShowLockFlyout again before the previous execution completes will now cancel the previous instance and reset the timer properly.
  Thread Safety: Ensures no overlapping logic even if ShowLockFlyout is called concurrently from different threads.
*/
private Task? activeTask;
private readonly object lockObj = new object();

public async void ShowLockFlyout(string key, bool isOn)
{
    setStatus(key, isOn);

    // Prevent overlapping executions by canceling the current task
    lock (lockObj)
    {
        cts.Cancel(); // Cancel the previous task
        cts = new CancellationTokenSource(); // Create a new CancellationTokenSource
        var token = cts.Token;

        // If there's an active task, wait for it to complete
        if (activeTask != null && !activeTask.IsCompleted)
        {
            try
            {
                activeTask.Wait(token);
            }
            catch (Exception)
            {
                // Ignore if it was canceled
            }
        }

        // Start a new task
        activeTask = Task.Run(async () =>
        {
            Visibility = Visibility.Visible;

            if (Visibility == Visibility.Hidden)
            {
                mainWindow.OpenAnimation(this, true);
            }

            try
            {
                while (!token.IsCancellationRequested)
                {
                    await Task.Delay(SettingsManager.Current.LockKeysDuration, token);
                    mainWindow.CloseAnimation(this, true);
                    await Task.Delay(mainWindow.getDuration());
                    Hide();
                    break;
                }
            }
            catch (TaskCanceledException)
            {
                // Do nothing on cancellation
            }
        }, token);
    }
}
```
"
244,679b769ccbe8284d7a732250,issue,learning -> how to use a library/framework/tool,https://github.com/home-assistant/core/issues/135369,"14 Entities are unavailable on the Vitodens100 Heating Boiler
### The problem

Hi,

I have integrated my Heating Boiler yesterday in HA, but from the 32 entities which seems to be detected, only 18 are available, the other 14 not. In the HA the only relevant info that I found is : **Unavailable**

![image](https://github.com/user-attachments/assets/5acfe419-7bfc-4e21-b76b-45284bda446e)

I googled and asked ChatGPT about advices, but I cannot see what could have gone wrong during the integration. Thank you.


### What version of Home Assistant Core has the issue?

2025.1.2

### What was the last working version of Home Assistant Core?

N/A

### What type of installation are you running?

Home Assistant Container

### Integration causing the issue

Viessman ViCare

### Link to integration documentation on our website

https://www.home-assistant.io/integrations/vicare

### Diagnostics information

_No response_

### Example YAML snippet

_No response_

### Anything in the logs that might be useful for us?

_No response_

### Additional information

_No response_"
245,679b769ccbe8284d7a7322c1,issue,software quality -> fixing -> supporting debugging,https://github.com/google-deepmind/mujoco/issues/2352,"freejoint seemingly stuck at 0 0 0
### Intro

Hello -

I am a researcher at OHSU, I use MuJoCo for my research on animals.  I got a bit stuck made a simple bot in Blender, and hopefully can get unstuck so thanks in advance for your help! I've scoured the documentation about freejoints and ChatGPT and Claude.ai were not able to help resolve this, I think I need another set of eyes because I bet this is a simple fix.


### My setup

mujoco-3.2.6
python 3.12.4
macOS 13.7.1


### What's happening? What did you expect?

I tried to set the freejoint to the center of mass in a simple bot I made in Blender. But I'm unable to set the freejoint to any position except 0 0 0, unless I configure the body/meshes to be at opposite locations. 

For example, this XML block:
<img width=""699"" alt=""Screenshot 2025-01-12 at 11 57 30"" src=""https://github.com/user-attachments/assets/d86d9830-8a34-4c32-ad34-c4545cf8774e"" />

Yields this result in the UI:
<img width=""419"" alt=""Screenshot 2025-01-12 at 11 29 15"" src=""https://github.com/user-attachments/assets/1e42fd7e-597b-452c-b7aa-7e50ff0076e6"" />

As I understand it, the freejoint should be in the COM of the mesh with this configuration. However, I'm only able to get the freejoint to the COM by changing the XML tags to be in very offset positions, like this:

<img width=""695"" alt=""Screenshot 2025-01-12 at 12 04 29"" src=""https://github.com/user-attachments/assets/d72a40c4-9495-4945-baa5-463535f389fd"" />

which results:

<img width=""420"" alt=""Screenshot 2025-01-12 at 12 15 40"" src=""https://github.com/user-attachments/assets/64f033cc-cd5d-4272-8dab-273ee4b9c215"" />

I have pored over the documentation and asked ChatGPT but I must be missing something. As I understand it, the freejoint should be in the COM of the body it lives under, so setting it to 0 0 0 should ideally make it coincident with the origin of the mesh, which is correctly defined in Blender.  Or potentially I'm missing a very important detail somewhere to make this work strangely. 

### Steps for reproduction

1. Load the model + the .stl file in the .zip.
2. change the pos values as defined in the XML to/ 0 0 0 and/or pos=""-.596 -0.15516 2.775"" or pos=""0.5 0 -2.775"".
3. Try to position the freejoint within the COM of the mesh/body.


### Minimal model for reproduction


[HIPS.stl.zip](https://github.com/user-attachments/files/18391074/HIPS.stl.zip)



<mujoco model=""t-rex"">
  <visual>
    <global offwidth=""1920"" offheight=""1080"" />
  </visual>  
  <compiler inertiafromgeom=""true"" meshdir=""/folder/stl"" /> 

<!-- <option gravity=""0 0 -9.81"" timestep=""0.005""  integrator=""Euler"" /> -->
<option gravity=""0 0 -9.81"" timestep=""0.005""/>

<default>
  <joint limited=""true"" damping=""1"" pos=""0 0 0"" armature=""0"" />
  <geom condim=""1"" type=""mesh"" />
  <motor ctrllimited=""true"" forcelimited=""false"" />
  <default class=""collision"">
  </default>
 
</default>
    
       <asset>
        <!-- mesh filesg -->

        <mesh name=""HIPS"" file=""HIPS.stl""/>
     
  </asset>

  <worldbody>

    <light directional=""true"" pos=""-0.5 0.5 3"" dir=""0 0 -1"" />
    
    <geom name=""floor"" pos=""0 0 0"" size=""50 50 .1"" class=""collision"" type=""plane"" rgba=""1 0.83 0.61 0.5"" />


    <!-- <body name=""HIPS"" pos=""-.596 -0.15516 2.7754"" euler=""0 0 0""> -->
    <body name=""HIPS"" pos=""-.596 -0.15516 2.775"">


      <joint name=""root"" type=""free""/>
      <inertial pos=""0 0 0"" mass=""100"" />
      <geom name=""HIPS"" mesh=""HIPS"" type=""mesh"" pos=""0.5 0 -2.775""/>
    </body>

    </worldbody>

</mujoco>

### Code required for reproduction

This issue is in the UI of MuJoCo, or potentially in the XML I made, no running code of mine seems related to this.

### Confirmations

- [X] I searched the [latest documentation](https://mujoco.readthedocs.io/en/latest/overview.html) thoroughly before posting.
- [X] I searched previous [Issues](https://github.com/google-deepmind/mujoco/issues) and [Discussions](https://github.com/google-deepmind/mujoco/discussions), I am certain this has not been raised before."
246,679b769ccbe8284d7a7323e9,issue,feature implementation/enhancement -> migrating/reusing via programming language translation,https://github.com/Pirate-Weather/pirate-weather-code/issues/52,"Use python translations library
### Describe the issue

@alexander0042 You mentioned that you had converted the JavaScript translations library to python locally but I don't think its been moved to the development endpoint right? I ask because the development endpoint went down shortly after you restarted it last week.

I did update the [translations repo](https://github.com/Pirate-Weather/translations) with the code snippets from ChatGPT and finished up the other languages. The repository is on PyPI and is added as a requirement so should be easy to update your code to use that repo.

### Acknowledgements

- [X] This issue is related to the self-hosting code and not an API issue.
- [X] I have read through the [README](https://github.com/Pirate-Weather/pirate-weather-code/blob/main/README.md) before opening this issue.
- [X] I have written an informative title."
247,679b769ccbe8284d7a732468,issue,learning -> how to use a library/framework/tool,https://github.com/JsMacros/JsMacros/issues/181,"useless tutorials
I have the basic JavaScript knowledge required to develop this script, but this development tutorial is really bad. After reading it, I can only write simple timed scripts. It doesn't teach anything about complex event handling. I turned to ChatGPT for help, but even ChatGPT couldn't write working script code using your framework. You should really consider how to improve the tutorial for this mod.

Please provide more examples, not just how to install the mod.
https://jsmacros.wagyourtail.xyz/?/tutorial.html"
248,679b7729cbe8284d7a73255e,issue,"software quality -> fixing -> performance-related,software quality -> fixing -> supporting debugging",https://github.com/s9y/Serendipity/issues/860,"notice on single entry page for include/functions_config.inc.php
`Notice: Only variables should be assigned by reference in ./include/functions_config.inc.php on line 1426.`

From ChatGPT

> The error occurs because PHP no longer allows assigning a reference (&) directly to the return value of a function. The specific problematic line is:
> 
> $_groups =& serendipity_db_query(...);
> This is not valid in modern PHP versions because a function's return value is not considered a variable that can hold a reference.
> 

ChatGPT want to change the function like this:

```
function &serendipity_getGroups($authorid, $sequence = false) {
    global $serendipity;

    // FunktionsrÃ¼ckgabewert in eine normale Variable speichern
    $_groups = serendipity_db_query(
        ""SELECT g.id  AS confkey,
                g.name AS confvalue,
                g.id   AS id,
                g.name AS name
         FROM {$serendipity['dbPrefix']}authorgroups AS ag
              LEFT OUTER JOIN {$serendipity['dbPrefix']}groups AS g
              ON g.id = ag.groupid
         WHERE ag.authorid = "" . (int)$authorid,
        false,
        'assoc'
    );

    if (!is_array($_groups)) {
        $groups = array();
    } else {
        // Referenz auf das Array setzen
        $groups = $_groups;
    }

    if ($sequence) {
        $rgroups = array();
        foreach ($groups as $grouprow) {
            $rgroups[] = $grouprow['confkey'];
        }
    } else {
        // Referenz auf Gruppen setzen
        $rgroups = $groups;
    }

    return $rgroups;
}
```

Didn't try it until now. Any recommendations?"
249,679b77b8cbe8284d7a73274a,issue,software quality -> fixing -> supporting debugging,https://github.com/Epic-Fight/epicfightmod/issues/1906,"[Bug|Crash]: java.lang.ArrayIndexOutOfBoundsException: Index -1 out of bounds for length 152
### Please check if a similar issue has already been reported.

- [x] I checked this type of issue has never been reported.

### Please check you're using proper versions.

- [x] I checked all of them in the right version.

### Please check if this issue's from out of our area

- [x] I understood the problem.

### The name of the mod

_No response_

### After all of these, please check if you're just pressing the ok buttons to skip this process.

- [x] I read the checklist and I promise I'll provide all the information that is needed to fix the issue

### Minecraft Version

1.20.1

### The Issue

I created a custom model for the Kobolds! mod in Blender 3.6. I was able to succesfully import the model and armature into the Datapack Editor, but after I try to get into the animation select menu by clicking into the magnifiyng glass icon, the games crashes giving me this issue's title. I'll place the crash log and my model here. I asked ChatGPT about it, and it most likely is an error with animations, but I couldn't tell, I'm new to all this.

[crash-2025-01-20_16.44.34-client.txt](https://github.com/user-attachments/files/18526123/crash-2025-01-20_16.44.34-client.txt)

[kobolds.json](https://github.com/user-attachments/files/18526126/kobolds.json)

### Steps to Reproduce

At Blender:
+ Select Model and Armature -> Export it with the json addon -> having all squares ticked or excluding animations export didn't make any difference

At Minecraft:
+ Import the json file -> choosing my custom model and armature -> hovering over to Living Animations -> add category and clicking the magnifying glass to open the animations menu -> crash"
250,679b77b8cbe8284d7a7327c2,issue,process -> automating commit/issue/PR creation -> creating commit/issue/PR description,https://github.com/YouTube-Enhancer/extension/issues/759,"Small CSS fix for themes
**Is your feature request related to a problem? Please describe.**
Iâ€™d like to contribute a small CSS fix for the built-in themes. I noticed that when a new notification is present, the icon badge next to the bell remains the default YouTube red color, which doesnâ€™t align with the theme. It would be great if this could be updated to match the theme's styling.

**Describe the solution you'd like**
I donâ€™t have a lot of experience with CSS, but I know the basics. By inspecting the element, I found the CSS class `yt-spec-icon-badge-shape__badge`. I believe this class could be set to one of the colors from the chosen theme.

**Describe alternatives you've considered**
Initially, I wanted to address this as a custom theme fix but realized that I would need to choose between a built-in variant or my custom CSS theme. Then, I considered creating a Tampermonkey script to fix it, but I thought it would be worth bringing up here first.

**Additional context**
I did use ChatGPT to rewrite what I said above. I type like a dumbass and I wanted my issue to be concise.
I would love to contribute my own fix, but I cannot find any repo with all the theme variants.

Thanks for reading.
You guys are doing great work. :)"
251,679b77b8cbe8284d7a732860,issue,"software quality -> fixing -> functional bugs,software quality -> fixing -> supporting debugging",https://github.com/exo-explore/exo/issues/639,"download_file is corrupting the file if downloaded_size is not equal to total_size
I was getting errors like ""Error processing prompt: Invalid control character at: line 563 column 62 (char 47769)"" while running exo. While investigating the problem I came across a corrupted index.json file:

> % cat /Volumes/Palladium/cache/huggingface/hub/models--mlx-community--DeepSeek-R1-Distill-Qwen-1.5B-3bit/snapshots/17131b28b8190d7b2aceb9b98926f263606a11ee/model.safetensors.index.json | tail -n 10
>         ""model.layers.9.self_attn.q_proj.scales"": ""model.safetensors"",
>         ""model.layers.9.self_attn.q_proj.weight"": ""model.safetensors"",
>         ""model.layers.9.self_attn.v_proj.bias"": ""model.safetensors"",
>         ""model.layers.9.self_attn.v_proj.biases"": ""model.safetensors"",
>         ""model.layers.9.self_attn.v_proj.scales"": ""model.safetensors"",
>         ""model.layers.9.self_attn.v_proj.weight"": ""model.safetensors"",
>         ""model.norm.weight"": ""model.safetensors""
>     }
> }
> Range not satisfiableRange not satisfiable%

With the help of ChatGPT (o1), I tracked it down to a bug in hf_helpers.py:

> 
> elif response.status == 416:
>     # Range not satisfiable
>     content_range = response.headers.get('Content-Range', '')
>     try:
>         total_size = int(content_range.split('/')[-1])
>         if downloaded_size == total_size:
>             if DEBUG >= 2: print(f""File fully downloaded on first pass: {file_path}"")
>             ...
>             return
>     except ValueError:
>         if DEBUG >= 1: print(f""Failed to parse Content-Range header: {content_range}. Starting download from scratch..."")
>         return await download_file(..., use_range_request=False)

As ChatGPT says, ""After that code block, thereâ€™s no return or raise if downloaded_size != total_size. The function simply â€œfalls throughâ€ to this part.""

This looks like a bug to me. Here is the fix suggested by ChatGPT o1 (after a few suggestions I made):

```
diff --git a/exo/download/hf/hf_helpers.py b/exo/download/hf/hf_helpers.py
index 8be12b9..ff3156c 100644
--- a/exo/download/hf/hf_helpers.py
+++ b/exo/download/hf/hf_helpers.py
@@ -202,6 +202,29 @@ async def download_file(
           if progress_callback:
             await progress_callback(RepoFileProgressEvent(repo_id, revision, file_path, downloaded_size, downloaded_this_session, total_size, 0, timedelta(0), ""complete""))
           return
+        if downloaded_size > total_size:
+            # Local file is bigger than remote file => definitely corrupted
+            if DEBUG >= 1:
+                print(
+                    f""Local file size ({downloaded_size}) is greater than remote file size ({total_size}). ""
+                    ""Removing local file and restarting download from scratch...""
+                )
+            # Remove the local file and redownload entirely
+            await aios.remove(local_path)
+            return await download_file(
+                session, repo_id, revision, file_path, save_directory, progress_callback, use_range_request=False
+            )
+
+        if downloaded_size < total_size:
+            # We haven't yet downloaded the full file, so the requested range is invalid
+            if DEBUG >= 1:
+                print(
+                    f""Partial file mismatch: local={downloaded_size}, total={total_size}. ""
+                    ""Retrying from scratch without range requests...""
+                )
+            return await download_file(
+                session, repo_id, revision, file_path, save_directory, progress_callback, use_range_request=False
+            )
       except ValueError:
         if DEBUG >= 1: print(f""Failed to parse Content-Range header: {content_range}. Starting download from scratch..."")
         return await download_file(session, repo_id, revision, file_path, save_directory, progress_callback, use_range_request=False)
```

Seems to work. Let me know."
252,679b77b8cbe8284d7a73289a,issue,software quality -> fixing -> functional bugs,https://github.com/twopirllc/pandas-ta/issues/880,"ImportError: cannot import name 'NaN' from 'numpy' (/Users/anees.a/.venv/lib/python3.12/site-packages/numpy/__init__.py). Did you mean: 'nan'?
**Which version are you running? The lastest version is on Github. Pip is for major releases.**

```bash
pip3 install pandas_ta
Collecting pandas_ta
  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Requirement already satisfied: pandas in /Users/anees.a/.venv/lib/python3.12/site-packages (from pandas_ta) (2.2.3)
Requirement already satisfied: numpy>=1.26.0 in /Users/anees.a/.venv/lib/python3.12/site-packages (from pandas->pandas_ta) (2.1.3)
Requirement already satisfied: python-dateutil>=2.8.2 in /Users/anees.a/.venv/lib/python3.12/site-packages (from pandas->pandas_ta) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /Users/anees.a/.venv/lib/python3.12/site-packages (from pandas->pandas_ta) (2024.1)
Requirement already satisfied: tzdata>=2022.7 in /Users/anees.a/.venv/lib/python3.12/site-packages (from pandas->pandas_ta) (2024.1)
Requirement already satisfied: six>=1.5 in /Users/anees.a/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->pandas_ta) (1.16.0)
Building wheels for collected packages: pandas_ta
  Building wheel for pandas_ta (pyproject.toml) ... done
  Created wheel for pandas_ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218986 sha256=6d59621e9c405876539bffef379a5edc455d8910818bc1ca8260f0bbde756b5a
  Stored in directory: /Users/anees.a/Library/Caches/pip/wheels/fd/ed/18/2a12fd1b7906c63efca6accb351929f2c7f6bbc674e1c0ba5d
Successfully built pandas_ta
Installing collected packages: pandas_ta
Successfully installed pandas_ta-0.3.14b0
```

**Do you have _TA Lib_ also installed in your environment?**

Yes
```bash
brew install ta-lib
```

**Have you tried the _development_ version? Did it resolve the issue?**
No

**Describe the bug**
```bash
python3
Python 3.12.4 (main, Jun  6 2024, 18:26:44) [Clang 15.0.0 (clang-1500.3.9.4)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pandas as pd
>>> import pandas_ta as ta
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/anees.a/.venv/lib/python3.12/site-packages/pandas_ta/__init__.py"", line 116, in <module>
    from pandas_ta.core import *
  File ""/Users/anees.a/.venv/lib/python3.12/site-packages/pandas_ta/core.py"", line 18, in <module>
    from pandas_ta.momentum import *
  File ""/Users/anees.a/.venv/lib/python3.12/site-packages/pandas_ta/momentum/__init__.py"", line 34, in <module>
    from .squeeze_pro import squeeze_pro
  File ""/Users/anees.a/.venv/lib/python3.12/site-packages/pandas_ta/momentum/squeeze_pro.py"", line 2, in <module>
    from numpy import NaN as npNaN
ImportError: cannot import name 'NaN' from 'numpy' (/Users/anees.a/.venv/lib/python3.12/site-packages/numpy/__init__.py). Did you mean: 'nan'?
>>> exit()
```

**To Reproduce**
Do the above

**Expected behavior**
Successful import

**Additional context**
This suggestion from ChatGPT worked for me.

![Image](https://github.com/user-attachments/assets/ff311c5c-7f32-4add-a7ab-bd8ffff6fc3e)
"
253,679b77b8cbe8284d7a732958,issue,software quality -> fixing -> supporting debugging,https://github.com/mikeizbicki/cmc-csci143/issues/588,"Difficulty searching reviews-notw-full.json by book title
I'm having trouble searching the `reviews-notw-full.json` file by book title. The JSON format of the file doesn't include the book title field directly. *I used chatGPT to trouble shoot when writing the script, which could have lead to things being overcomplicated.*

The two things I needed advice on are: 

1. Modify the command to select only 20 reviews and pass them to the LLM.
2. Properly implement the `summarize_reviews.sh` script to summarize reviews for any given book title.

## Command Example for Passing Reviews to LLM

Current command I use to pass top 20 reviews to the LLM:

```
$ echo ""Write a short 2-3 sentence summary of the following book reviews. The reviews are: $(cat ./reviews-notw-full.json | jq '.review_text' | head -n 20)"" | llm
```
## Current `summarize_reviews.sh`

```bash
#!/bin/sh

# Create a temporary directory and change into it
tempdir=$(mktemp -d)
cd ""$tempdir""

# Check if a book title is provided
if [ -z ""$1"" ]; then
  echo ""Usage: sh summarize_reviews.sh \""<book title>\""""
  exit 1
fi

# Set the book title
BOOK_TITLE=""$1""

REVIEWS_FILE=""/data/users_bigdata/mplush/reviews-notw-full.json""

# Check if the reviews file exists
echo ""Looking for file at: $REVIEWS_FILE""  # Debugging line
if [ ! -f ""$REVIEWS_FILE"" ]; then
  echo ""Error: Reviews file for '$BOOK_TITLE' not found.""
  exit 1
fi

# Generate and pass the summary prompt to the LLM
echo ""book to summarize is: $BOOK_TITLE""
echo ""Write a short 2-3 sentence summary of the following book reviews. The reviews are: $(cat ""$REVIEWS_FILE"" | shuf | jq '.review_text' | head -n 20)"" | llm

# Clean up the temporary directory after the script finishes
rm -rf ""$tempdir""
```

Regardless of what book title is entered, the script only provides summaries for **""The Name of the Wind""**. For example:

```bash
$ sh summarize_reviews.sh ""The Name of the Wind""
Looking for file at: /data/users_bigdata/mplush/reviews-notw-full.json
book to summarize is: The Name of the Wind
Here is a short 2-3 sentence summary of the book reviews: The Name of the Wind by Patrick Rothfuss has received widespread acclaim for its engaging story, well-developed characters, and detailed world-building. Many reviewers have praised the book's unique blend of fantasy and realism, as well as its relatable protagonist, Kvothe, who is both talented and flawed. Despite some criticisms of the book's slow pace and abrupt ending, the majority of reviewers have highly recommended the book, with many considering it one of the best fantasy novels they've ever read.
```

```bash
$ sh summarize_reviews.sh ""The Wealth of Nations""
Looking for file at: /data/users_bigdata/mplush/reviews-notw-full.json
book to summarize is: The Wealth of Nations
Here is a short 2-3 sentence summary of the book reviews:

The Name of the Wind by Patrick Rothfuss has received overwhelmingly positive reviews, with many readers praising its engaging storyline, well-developed characters, and immersive world-building. However, some reviewers have noted that the book's pacing can be slow at times, and that the female characters are somewhat one-dimensional and lack agency. Overall, the book has been praised for its unique narrative style and its ability to evoke strong emotions in readers, with many reviewers eagerly anticipating the next installment in the series.
```

## Missing Concept

I need help understanding how to:

1. Filter reviews from `reviews-notw-full.json` by the given book title.
2. Adjust the `jq` in the script to extract reviews for the entered title.

Thank you for your help!
"
254,679a78f224d155be01f00451,pull-request,naming,https://github.com/seed-hypermedia/mintter/pull/1298,"Web Publishing APIs + Codegen (supersedes #1291)
This PR supersedes #1291!

We changed the way we generate protobuf code for TypeScript. The old PR was intended only for API definitions, but it ended up having code using those APIs, and now we can't merge it because the generated code now looks differently.

So, this PR adds only API definition + codegen using the new libraries. Please, don't merge any code into this branch!

I tried to adopt all of the discussions and open points from the old PR + I made some changes which I'm happy to discuss.

Some of the changes:

1. I merged both local API and the web site API into one proto file. The site/sites distinction was very confusing even for me (considering I was the one who created them).
2. I finally named method `RemoveSite` instead of `DeleteSite`. I guess @ericvicenti was right, and it's a more appropriate name. I guess we have `AddSite` instead of `CreateSite` for the same reason, so no big deal diverging from API guidelines.
3. I named (with the help of ChatGPT ðŸ˜€) ""WebPublicationRecord"" the ""instance"" of a `document+version+path` on a given server. The server can have more than one of these records for a single document.
4. I added method to the local API to list these records for a given document among all of the configured sites.
5. There's also a method the actual Site is exposing to list all of the publications it has.

Let me know your thoughts!"
255,679a7a6a24d155be01f00a2a,pull-request,learning -> program comprehension -> getting familiar with the codebase,https://github.com/opentypejs/opentype.js/pull/572,"optimize performance by replacing modulus with bitwise operator
<!--- Provide a general summary of your changes in the Title above -->

## Description
<!--- Describe your changes in detail -->
Replaces all usage of modulus ""% 2"" with the bitwise AND operator ""& 1""

## Motivation and Context
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->
Inspired by https://github.com/opentypejs/opentype.js/pull/569#discussion_r1114926698

## How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->
Ran all tests (and asked ChatGPT if those statements are really interchangeable in JavaScript ðŸ˜…)

## Screenshots (if appropriate):

## Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [X] Performance improvement

## Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [X] I did `npm run test` and all tests passed green (including code styling checks).
- [ ] I have added tests to cover my changes.
- [ ] My change requires a change to the documentation.
- [ ] I have updated the **README** accordingly.
- [X] I have read the **CONTRIBUTING** document.
"
256,679a7a6a24d155be01f00a93,pull-request,feature implementation/enhancement -> scripting,https://github.com/zorn/franklin/pull/151,"[Fixes #148] Write script for quick branch / PR creation.
Fixes #148

I used ChatGPT to help me crafting the shell scripts as a kind of experiment (I have not used it before). It was impressive.

<img width=""821"" alt=""Screen Shot 2023-02-24 at 1 55 08 PM"" src=""https://user-images.githubusercontent.com/52168/221276577-136df018-a6bc-47ed-a1c0-f9bfa2e7b940.png"">
<img width=""835"" alt=""Screen Shot 2023-02-24 at 2 45 33 PM"" src=""https://user-images.githubusercontent.com/52168/221276578-bc97cde9-9bd0-40c6-bd10-79b3a6fca07f.png"">
<img width=""828"" alt=""Screen Shot 2023-02-24 at 2 33 42 PM"" src=""https://user-images.githubusercontent.com/52168/221276579-20470201-f57b-45af-a3e1-adb0cfdc2744.png"">
<img width=""838"" alt=""Screen Shot 2023-02-24 at 1 58 43 PM"" src=""https://user-images.githubusercontent.com/52168/221276582-eae6b2e4-a93d-4fde-8a96-86f91455ea69.png"">
"
257,679a7b1024d155be01f00b24,pull-request,process -> automating commit/issue/PR creation -> creating commit/issue/PR description,https://github.com/RIOT-OS/RIOT/pull/19332,"sys/tiny_strerror: make use of flash_utils.h
<!--
The RIOT community cares a lot about code quality.
Therefore, before describing what your contribution is about, we would like
you to make sure that your modifications are compliant with the RIOT
coding conventions, see https://github.com/RIOT-OS/RIOT/blob/master/CODING_CONVENTIONS.md.
-->

### Contribution description

`tiny_strerror` was written specifically for AVR, so it makes sense to also use `flash_utils` there.


### Testing procedure

Modify `hello_world` to print an error:

```patch
 #include <stdio.h>
+#include <string.h>
+#include <errno.h>
 
 int main(void)
 {
@@ -28,5 +30,7 @@ int main(void)
     printf(""You are running RIOT on a(n) %s board.\n"", RIOT_BOARD);
     printf(""This board features a(n) %s MCU.\n"", RIOT_MCU);
 
+    puts(strerror(EAGAIN));
+
     return 0;
 }
```

#### master

```
   text	   data	    bss	    dec	    hex	filename
   5520	   1098	    901	   7519	   1d5f	/home/benpicco/dev/RIOT/examples/hello-world/bin/avr-rss2/hello-world.elf
```

#### this PR

```
   text	   data	    bss	    dec	    hex	filename
   6532	    118	    917	   7567	   1d8f	/home/benpicco/dev/RIOT/examples/hello-world/bin/avr-rss2/hello-world.elf
```

```
2023-02-27 22:24:40,383 # main(): This is RIOT! (Version: 2023.04-devel-523-g6b501)
2023-02-27 22:24:40,385 # Hello World!
2023-02-27 22:24:40,388 # You are running RIOT on a(n) avr-rss2 board.
2023-02-27 22:24:40,393 # This board features a(n) atmega256rfr2 MCU.
2023-02-27 22:24:40,394 # EAGAIN
```

### Issues/PRs references

Thanks to ChatGPT for helping with the tedious work!
<!--
Examples: Fixes #1234. See also #5678. Depends on PR #9876.

Please use keywords (e.g., fixes, resolve) with the links to the issues you
resolved, this way they will be automatically closed when your pull request
is merged. See https://help.github.com/articles/closing-issues-using-keywords/.
-->
"
258,679a877224d155be01f018fb,pull-request,process -> automating commit/issue/PR creation -> creating commit/issue/PR description,https://github.com/yv989c/ChartsForBenchmarkDotNet/pull/3,"Feature: Add Shareable URL for Chart Content and Settings
### Description
This pull request adds a new feature that allows users to share the content and settings of the chart currently displayed on the screen. The feature is implemented by generating a shareable URL that contains all the relevant data needed to reproduce the chart.

The shareable URL can be accessed by clicking the new ""Share"" button on the chart toolbar. When clicked, the button generates a URL that can be copied and shared with others. When a recipient clicks the URL, they will be taken to a new page that displays the chart using the same data and settings as the original chart.

### Changes
- Added a new ""Share"" button to the chart toolbar
- Implemented logic to generate a shareable URL that contains the chart data and settings
### Testing
- Manual testing was performed to ensure that the shareable URL was generated correctly and that the chart displayed correctly when accessed from the URL
- Unit tests were added to test the shareable URL generation logic (yeah, sure... ðŸ˜…)
### Notes
- This feature will make it easier for users to share charts with others, and can also be used for collaboration and troubleshooting purposes.
- The shareable URL is not encrypted or protected, so caution should be taken when sharing sensitive data.

---

ðŸ¤£ All of the above was mostly generated by ChatGPT (free) with the following prompt:
```
Please write me a pull request notes for a feature that will allow the sharing of the content and settings of the chart presented on the screen. The commit that implemented the feature has ""feat: shareable URL"" in the message.
```"
259,679a877224d155be01f01946,pull-request,development environment -> continuous integration -> integrating ChatGPT reviewing in CI,https://github.com/iotexproject/iotex-core/pull/3829,"test chatgpt code review
for test,don't merge"
260,679a877224d155be01f0194b,pull-request,development environment -> continuous integration -> integrating ChatGPT reviewing in CI,https://github.com/iotexproject/iotex-core/pull/3833,"test chatgpt code review
test"
261,679a886424d155be01f01a5e,pull-request,software quality -> code review,https://github.com/iotexproject/iotex-core/pull/3837,"[api] ServerV2.Stop() use correct context
# Description
From ChatGPT code review

Fixes #(issue)

## Type of change
Please delete options that are not relevant.
- [x] Bug fix (non-breaking change which fixes an issue)
- [] New feature (non-breaking change which adds functionality)
- [] Code refactor or improvement
- [] Breaking change (fix or feature that would cause a new or changed behavior of existing functionality)
- [] This change requires a documentation update

# How Has This Been Tested?
Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration
- [x] make test
- [] fullsync
- [] Other test (please specify)

**Test Configuration**:
- Firmware version:
- Hardware:
- Toolchain:
- SDK:

# Checklist:
- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules
"
262,679a886424d155be01f01b70,pull-request,"documentation -> generating readme,documentation -> generating user guide",https://github.com/tatsu-lab/stanford_alpaca/pull/160,"docs(README): add pip command for hugging face library
I didn't know how to do this so I asked ChatGPT xD"
263,679a8ac024d155be01f0262d,pull-request,development environment -> continuous integration -> integrating ChatGPT reviewing in CI,https://github.com/ravilushqa/gpt-pullrequest-updater/pull/1,"WIP refactor
### Description
This WIP pull request includes several changes to improve the functionality, security, and performance of the application. It includes a new GitHub Workflow using ChatGPT for automated code reviews, a new GithubClient struct with methods to interact with GitHub Pull Requests, updated dependencies for improved security and stability, and a refactored codebase with updates to the GitHub Go SDK version.

### Changes
- Added a GitHub Workflow for automated code reviews using ChatGPT in .github/workflows/cr.yml.
- Added a new GithubClient struct with methods to interact with Github Pull Requests in github.go.
- Added several new dependencies to the 'require' block in go.mod.
- Updated dependencies to latest versions for improved security and stability in go.sum.
- Refactored code and updated GitHub Go SDK version in main.go.
- Added a new function to the OpenAIClient struct to enable chat completion using GPT 3.5 Turbo model in openai.go."
264,679a947324d155be01f0270a,pull-request,documentation -> improving writing,https://github.com/ringsaturn/tzf/pull/116,"Fix grammar and wording in README
I used ChatGPT to revise the README file and corrected grammar errors and inappropriate wording. "
265,679a947324d155be01f02757,pull-request,documentation,https://github.com/supercollider/supercollider/pull/6015,"(supecollider/supercollider - improving documentation) Update docs with the help of ChatGPT
(supecollider/supercollider - improving documentation) Update docs with the help of ChatGPT
please feel free to merge this to my previous pull requests"
266,679a947324d155be01f028cd,pull-request,feature implementation/enhancement -> migrating/reusing via programming language translation,https://github.com/Flipnote-Collective/kwz-parser/pull/10,"Add support for reading individual sections (for example KTN)
I know that duplicating self.buffer to self.data is bad, but I'm doing this to avoid other functions from breaking.

Fun fact: This is a direct conversion of getSectionData from the PHP version of this to Python. I also used ChatGPT for some more difficult steps in the conversion. :smile:"
267,679a96e824d155be01f031d3,pull-request,documentation -> generating readme,https://github.com/osism/k8s-capi-images/pull/185,"README: use tables instead of lists
Generated with the help of ChatGPT"
268,679a96e824d155be01f03224,pull-request,development environment -> continuous integration -> integrating ChatGPT reviewing in CI,https://github.com/taikoxyz/taiko-mono/pull/13786,"feat(repo): enable OpenAI-based review
Try to use ChatGPT to review our PRs automatically."
269,679a977b24d155be01f03377,pull-request,feature implementation/enhancement -> implementing a new feature,https://github.com/enviodev/docs/pull/11,"added tabs for event handlers in different languages
Used placeholder code generated from chatGPT for Javascript and Typescript so long

"
270,679a977b24d155be01f034a4,pull-request,learning -> how to use a library/framework/tool,https://github.com/WordPress/gutenberg/pull/51011,"Improve (and relax) search vs direct URL entry detection in Link Control
<!-- Thanks for contributing to Gutenberg! Please follow the Gutenberg Contributing Guidelines:
https://github.com/WordPress/gutenberg/blob/trunk/CONTRIBUTING.md -->

## What?
<!-- In a few words, what is the PR actually doing? -->

Improves the matching for URLs vs Entity Searches when typing in the Link Control search box.

Also removes the direct entry ""suggestion"" when an entity is returned.

Part of https://github.com/WordPress/gutenberg/issues/50178

## Why?
<!-- Why is this PR necessary? What problem is it solving? Reference any existing previous issue(s) or PR(s), but please add a short summary here, too -->

It's required by https://github.com/WordPress/gutenberg/issues/50178 to simplify the process of:

- selecting a search result
- adding a direct entry URL
- creating a page (note: this option will become less of a concern as we will move this as part of https://github.com/WordPress/gutenberg/issues/50891).

## How?
<!-- How is your PR addressing the issue at hand? What are the implementation details? -->

Updates the `isURLLike` method by removing the very ""open"" `isURL` check which allowed pretty much anything to be technically valid. That's correct but in our case we want to limit the ""URL"" suggestion prompt to be only when it's prettry obviously a domain. So this result should now only show if:

- it's a valid protocol (http(s), ftp, mailto...etc)
- it's an internal link `#something`
- it starts with `www.`
- it has something that looks like a TLD (`wordpress.org`).

That's likely to exclude some edge cases that are technically valid URLs. However the approach previously which tried to account for any possible URL made the UX poorer by saying ""this is a URL"" even when it probably wasn't (e.g. `heelloiamnotaurl`).

This trades accuracy for an improved UX.


Note that I used ChatGPT to help me scaffold out the regex for the TLD helper. 


## Testing Instructions
<!-- Please include step by step instructions on how to test this PR. -->
<!-- 1. Open a Post or Page. -->
<!-- 2. Insert a Heading Block. -->
<!-- 3. etc. -->

- Add a paragraph
- Add a link.
- Try searching for a page.
- See that search results display but without any result for ""direct entry""
- Try another link, this time entering things that would seem like ""URLs"" to you. 
- Check the system accurately recognises these and does not show entity search results but rather displays the ""URL"" suggestion for you to click on.
- You should never see both entity results and direct entry URL result at the same time.
- Check that it's still possible to manually enter any input even if it's not recognised as a URL. In the future we need to follow up with _validation_ which will _strictly_ check the type of input is a valid URL using `isURL`.

### Testing Instructions for Keyboard
<!-- How can you test the changes by using the keyboard only? Please note, this is required for PRs that change the user interface (UI). This ensures the PR can be tested for any possible accessibility regressions. -->

## Screenshots or screencast <!-- if applicable -->
"
271,679a981124d155be01f03618,pull-request,software quality -> fixing,https://github.com/envoyproxy/gateway/pull/1467,"fix: User newer version of octokit for retest GHA
Thank you ChatGPT for helping me out

Relates to https://github.com/envoyproxy/gateway/pull/1462#issuecomment-1567637034"
272,679aa25424d155be01f03a70,pull-request,feature implementation/enhancement -> feature enhancement,https://github.com/Mapsui/Mapsui/pull/2058,"Add color constants
We already had a few constants and all web colors as string. Asked ChatGPT to generate all the constants."
273,679aa2e124d155be01f03c2a,pull-request,feature implementation/enhancement -> feature enhancement,https://github.com/DontShaveTheYak/cloud-radar/pull/238,"allow ssm dynamic references to not include a version
I did not mean to refactor the entire function. I just didn't realize how many edge cases there were when handling dynamic references. I used chatGPT for the regex. It took many tries before it gave me a working one. When I asked it why it took so long for us to derive the correct answer, it replied with a very generic list of reasons of why it might have taken so long. The top of that list was miscommunication. I asked it if it thought we miscommunicated and surprisingly it was able to clearly articulate how we miscommunicated. I am both excited and afraid of what the next 10 years will bring."
274,679aa36124d155be01f03e4d,pull-request,documentation -> generating code comments,https://github.com/mem0ai/mem0/pull/43,"docs: clean_string function documentation
Regex is hard to read, so I added some chatgpt generated documentation to know what's going on."
275,679aa36124d155be01f03edd,pull-request,software quality -> fixing -> supporting debugging,https://github.com/WiFiBeat/WiFiBeat/pull/5,"Fixed compilation errors. Updated deprecated func.
Please review this as I am not a C++ developer and used chatGPT to help me get this compiled and running. 

1. **Deprecation of `std::ptr_fun<int, int>` and `std::not1` in string trimming functions**

   The `std::ptr_fun` and `std::not1` functions were deprecated in C++17 and removed in C++20. These functions were used in the `wifibeat::utils::stringHelper::ltrim` and `wifibeat::utils::stringHelper::rtrim` functions. To ensure compatibility with C++17 and later, we replaced these functions with equivalent lambda expressions.

   In `wifibeat::utils::stringHelper::ltrim`:

   ```cpp
   void wifibeat::utils::stringHelper::ltrim(std::string &s) {
       s.erase(s.begin(), std::find_if(s.begin(), s.end(), [](int ch) {
           return !std::isspace(ch);
       }));
   }
   ```

   In `wifibeat::utils::stringHelper::rtrim`:

   ```cpp
   void wifibeat::utils::stringHelper::rtrim(std::string &s) {
       s.erase(std::find_if(s.rbegin(), s.rend(), [](int ch) {
           return !std::isspace(ch);
       }).base(), s.end());
   }
   ```

2. **Truncation of `snprintf` output**

   The output of the `snprintf` function in `wifibeat::utils::stringHelper::hex2string` was getting truncated. This was because the size of the buffer passed to `snprintf` did not account for the null-terminating character. We adjusted the buffer size to prevent truncation.

   In `wifibeat::utils::stringHelper::hex2string`:

   ```cpp
   snprintf(ret + (i *3), 3, ""%02x"", data[i + offset]);
   ```
   and
   ```cpp
   snprintf(ret + (i * 2), 3, ""%02x"", data[i + offset]);
   ```

3. **'PMKSA' not recognized in `Tins::RSNInformation::AKMSuites`**

   The code contained a switch case for `RSNInformation::AKMSuites::PMKSA`, but 'PMKSA' is not a recognized member of `Tins::RSNInformation::AKMSuites`. To resolve this, we removed the `RSNInformation::AKMSuites::PMKSA` case.

   Changes in the switch statement:

   ```cpp
   switch(item) {
       case RSNInformation::AKMSuites::PSK:
           jo->Add(""type"", 1ULL);
           jo->Add(""value"", (oui * 256) + 1);
           jo->Add(""value_parsed"", string(""PSK""));
           break;
       default:
           jo->Add(""type"", string(""unknown""));
           break;
   }
   ```

4. **Missing `libnl-genl-3` during compilation**

   During the compilation of the project, the linker could not find the `libnl-genl-3` library. This was resolved by installing the `libnl-genl-3-dev` package on the system.

   ```sh
   sudo apt-get install libnl-genl-3-dev
   ```

These changes were necessary to resolve compilation errors and update deprecated functions. The changes ensure that the code is compatible with modern versions of C++ and that the necessary libraries are correctly linked during compilation."
276,679aa36124d155be01f03f83,pull-request,generating/manipulating data -> ui translations,https://github.com/RobotsBuildingEducation/RobotsBuildingEducation/pull/16,"Refactor for GPT bilingual mode
If you are a Patreon subscriber and want to work on this together, please schedule a meeting via DMs using this template:

- [setting up a meeting](https://www.patreon.com/posts/1-on-1s-draft-1-81120404?utm_medium=sharesheet&utm_source=sharesheet&utm_campaign=postshare_creator&utm_content=join_link)

# What is this? I'm new.

This is a pull request.
It's used to perform code reviews.
It's a process where you're able to describe and compare the code you changed against the code that's being changed.

Given the nature of our organization and community, we'll introduce pull requests early and allow them to grow and evolve over time.

# Why Was This Code Changed

I took a look at the issues list #13 and thought about the size of the application. It's supposed to be small and simple to use. It should be so easy, a robot can work with it!

So we're starting off by organizing the words used. We're doing this to make the application _bilingual friendly_. The reason we're doing this is because bilingual speaking is interesting and a lot of people speak it. 

The process of transforming your application into a bilingual one is by simply mapping out the words & using chatGPT to create appropriate replications. The toggle/selection picks the head of the list, in this case ""english,  spanish"" and processes the words in the application.

<img width=""802"" alt=""image"" src=""https://github.com/RobotsBuildingEducation/RobotsBuildingEducation/assets/65219666/c84e2ae2-fce2-4e02-87d2-bb0f8cb523a2"">
<img width=""824"" alt=""image"" src=""https://github.com/RobotsBuildingEducation/RobotsBuildingEducation/assets/65219666/05e84324-c6ea-4704-9bfb-c2f7c4cb3299"">


# What changed

So far

**1. Added a ""scholarships"" counter from the database and inserted it into the Impact Wallet display**
<img width=""1268"" alt=""image"" src=""https://github.com/RobotsBuildingEducation/RobotsBuildingEducation/assets/65219666/41c0acc3-4b1a-4fed-8670-2150b8826e81"">

**2. Added a words template & spanish demonstration from chatGPT**

"
277,679aa36124d155be01f03f8c,pull-request,feature implementation/enhancement -> implementing a new feature,https://github.com/ForumMagnum/ForumMagnum/pull/7491,"Add structured data for SEO
See [here](https://developers.google.com/search/docs/appearance/structured-data/intro-structured-data) for the motivation. I'm not sure how much of an effect this will have, since in most cases these are things google could work out from reading the page. But hopefully it will help to show which things are most important.

Things that are useful for reviewer to look out for:
 - Whether any values used can be null in a way that will throw an error or result in the data being malformed
 - Whether these are all properties allowed by schema.org (just check the ones you're suspicious of). I used ChatGPT to help with this so it may have hallucinated some (I did check the more surprising ones)

â”†Issue is synchronized with this [Asana task](https://app.asana.com/0/1201302964208280/1204916979840175) by [Unito](https://www.unito.io)
"
278,679aa36124d155be01f03ff2,pull-request,feature implementation/enhancement -> feature enhancement -> user experience -> gui-related improvements,https://github.com/Simon-Initiative/oli-torus/pull/3796,"[ENHANCEMENT] Add some marketing to torus homepage
This PR adds some marketing materials to the torus homepage. I used ChatGPT to help create some of this content.

<img width=""1469"" alt=""Screenshot 2023-06-28 at 12 44 12 PM"" src=""https://github.com/Simon-Initiative/oli-torus/assets/6248894/fc467122-8787-4de3-8975-12cff6dcc742"">

"
279,679aa55224d155be01f045f3,pull-request,development environment -> continuous integration -> implementing/fixing jobs/actions,https://github.com/PlasmaPy/PlasmaPy/pull/2249,"Add GitHub Action to add no changelog label for automated pull requests
This PR adds a GitHub Action that will add the ""No changelog entry needed"" label to pull requests that include `Update pinned requirements` or `autoupdate`, or contain the pattern `Bump * from * to *`.  

I used ChatGPT to generate the GitHub Action:

> Absolutely, I can help you create a GitHub Action workflow for that. Below is a draft of a workflow that uses the `actions/github-script` action to apply the logic you've described. This is a JavaScript-based action which will give us the flexibility we need. 
> 
> Create a new file in the `.github/workflows` directory of your repository. You can name it anything, but for this example, let's call it `labeler.yml`. In this file, paste the following:
> 
> ```yaml
> name: ""Pull Request Labeler""
> 
> on:
>   pull_request:
>     types: [opened, synchronize, reopened, edited]
> 
> jobs:
>   labeler:
>     runs-on: ubuntu-latest
>     steps:
>       - name: Label the PR
>         uses: actions/github-script@v5
>         with:
>           github-token: ${{ secrets.GITHUB_TOKEN }}
>           script: |
>             const { payload, github } = context;
> 
>             // Regular expression to match the PR title pattern ""Bump * from * to *""
>             const bumpPattern = /^Bump .* from .* to .*/;
> 
>             // Fetch the PR
>             const { data: pullRequest } = await github.rest.pulls.get({
>               owner: context.repo.owner,
>               repo: context.repo.repo,
>               pull_number: context.issue.number,
>             });
> 
>             // Check if PR title matches any of the conditions
>             if (
>               pullRequest.title.includes('Update pinned requirements') ||
>               pullRequest.title.includes('autoupdate') ||
>               bumpPattern.test(pullRequest.title)
>             ) {
>               // If yes, then apply the label ""No changelog entry needed""
>               await github.rest.issues.addLabels({
>                 owner: context.repo.owner,
>                 repo: context.repo.repo,
>                 issue_number: context.issue.number,
>                 labels: ['No changelog entry needed'],
>               });
>             }
> ```
> 
> The action starts running every time a pull request is opened, synchronized, reopened, or edited. The script fetches the pull request details and checks the title against your conditions. If any of the conditions is met, it applies the label ""No changelog entry needed"". Note that the label should exist in your repository, otherwise you will need to create it.
> 
> Also, ensure you have the `actions/github-script` action available in your repository. The version used in this example is `v5`, you may need to adjust this based on the latest available version at your time of implementation."
280,679aa64124d155be01f0498c,pull-request,naming,https://github.com/flux-framework/flux-core/pull/5360,"cmd: add flux-watch(1)
Putting this up as a WIP, in case others have suggestions or comments on this added command.

This PR adds a new command, `flux watch` for lack of a better name (I asked ChatGPT and one of its suggestions was `flux gazemancer` but that was just a bit too outlandish). This command basically does the same thing as `flux submit --watch` but can monitor all running jobs, all inactive jobs, or a specific set of jobs. (It has a similar set of options as `flux jobs`, `flux pstree` etc.).

This allows users to submit many jobs then watch then with `flux watch`, e.g. at the end of a batch script, or if you need to run multiple `submit` or `bulksubmit` commands to submit a set of jobs and then want to monitor them all, e.g. `flux watch --all --progress`...

I still need to think more about the set of options here, write tests, etc. But that should go fairly quickly since this is actually a pretty simple command."
281,679aa64124d155be01f04abd,pull-request,feature implementation/enhancement -> feature enhancement -> user experience -> gui-related improvements,https://github.com/ArtBlocks/artbot/pull/559,"Better Trivia infra and responses
Got ChatGPT to generate a bunch of cheeky success messages

Also better infra"
282,679ab02d24d155be01f04b77,pull-request,feature implementation/enhancement -> implementing a new feature,https://github.com/Vencord/Installer/pull/88,"Update find_discord_darwin.go to check for home folder
Full disclaimer: I have not checked this code nor am I strong in Golang. I simply asked ChatGPT to include application home folder search as that's where I have my discord installed and the home user's Application directory is a standard directory in macOS that exists when you first initialize your machine."
283,679ab02d24d155be01f04d06,pull-request,software quality -> refactoring,https://github.com/internetarchive/bookreader/pull/1210,"add custom TTS playback speed
--> what:
the TTS (text to speech) speed has fixed settings of 0.25-0.5-0.75-1.00-1.25- and so on. 

I implemented a custom speed option. 
So now you will be able to type in numbers between 0 to 5 and with 2 decimal places. 
So the value 1.1 will work. 

--> how:
After setting up my the dev environment, this is where I had like 74 updates in git for making the files multi line. 
I added it in BookReader/plugins/plugin.tts.js

After having a multiline implementation i asked ChatGPT to convert the new code i wrote into single line format that the current plugin.tts.js requires. 

So thats where we are. After testing it I am now making a pull request. 

Apologies for this if it was done the wrong way. 

Pictures of working version below: 


<img width=""711"" alt=""Screenshot 2023-08-10 at 17 23 28"" src=""https://github.com/internetarchive/bookreader/assets/137233276/78044b30-bfe6-4fdf-9083-92f9f2279f48"">


<img width=""703"" alt=""Screenshot 2023-08-10 at 17 23 44"" src=""https://github.com/internetarchive/bookreader/assets/137233276/6e7d99c6-8b99-45f4-b088-1e7a2a8b02ea"">


<img width=""701"" alt=""Screenshot 2023-08-10 at 17 23 56"" src=""https://github.com/internetarchive/bookreader/assets/137233276/e5e48428-48b1-4cd6-9d13-434f66c9a919"">
"
284,679ab0eb24d155be01f04f08,pull-request,"documentation -> generating code comments,software quality -> code review",https://github.com/cumc/xqtl-protocol/pull/621,"update fastenloc with chatgpt comments
Add the comments from chatgpt and some small modifications."
285,679ab0eb24d155be01f0508e,pull-request,learning->brainstorming,https://github.com/gallery-so/gallery/pull/1833,"Fix stale composer refresh
### Summary of Changes

Loom Overview, which contains Demo: https://www.loom.com/share/5060a674c9ac4aa7b2b8c60c3de317cb?sid=ca87088c-a695-491b-813a-330adb15ce9f

**TL;DR â€“ when firing a mutation, we need to explicitly specify which fragments we want to update in the relay cache.** 

Without specifying the `PostComposerModalFragment` to be updated after calling `useSyncTokens`, the tokens remained stale in the post composer despite refreshing. The exact same issue occurred in the Editor in the past, which was fixed with #1659.

The way we need to remember to list which fragments to update in the relay cache is very annoying. I asked ChatGPT why this is necessary, and it provided some pros / cons:

<img width=""754"" alt=""image"" src=""https://github.com/gallery-so/gallery/assets/12162433/3519ea09-1bb5-45cd-a88b-4f79b0e2bd9e"">

<img width=""741"" alt=""image"" src=""https://github.com/gallery-so/gallery/assets/12162433/a3650a35-3615-48b8-a267-7c0b33c789a9"">
"
286,679ab0eb24d155be01f05160,pull-request,software quality -> testing -> generating tests,https://github.com/mongodb/chatbot/pull/113,"(DOCSP-32279): Update prompts to not mention context when don't know answer
Jira: https://jira.mongodb.org/browse/DOCSP-32279

## Changes

- Update prompts to not mention context when don't know answer
- LLM testing to validate this works
- Add util function to get test cases from YAML to TypeScript

## Notes

- Used ChatGPT to help come up with the qualitative tests, which means we are now using LLMs to generate test cases for LLM powered app with a LLM-based testing library 
"
287,679ab1b224d155be01f05369,pull-request,feature implementation/enhancement -> feature enhancement -> user experience -> gui-related improvements,https://github.com/StampyAI/stampy-ui/pull/313,"iframe dynamic height using postMessage
fix for dynamic height, testing on https://aisafety.softr.app/new-stampy

(advice from ChatGPT 4 helped ðŸ˜…)"
288,679ab27624d155be01f055bd,pull-request,feature implementation/enhancement -> migrating/reusing via programming language translation,https://github.com/antfu/eslint-config/pull/246,"fix(ts): tsconfig.json order item
<!-- DO NOT IGNORE THE TEMPLATE!

Thank you for contributing!

Before submitting the PR, please make sure you do the following:

- Read the [Contributing Guide](https://github.com/antfu/contribute).
- Check that there isn't already a PR that solves the problem the same way to avoid creating a duplicate.
- Provide a description in this PR that addresses **what** the PR is solving, or reference the issue that it solves (e.g. `fixes #123`).
- Ideally, include relevant tests that fail without this PR but pass with it.

-->

### Description

<!-- Please insert your description here and provide especially info about the ""what"" this PR is solving -->

I submitted #243 for this PR, but I found an error here, I'm very sorry because I'm using ChatGPT to convert it so it may have some mistakes. I made some corrections by carefully comparing the differences.

<img width=""1410"" alt=""1"" src=""https://github.com/antfu/eslint-config/assets/61053131/11367c26-6603-4082-acf2-e3edf61d2dc3"">



### Linked Issues


### Additional context

<!-- e.g. is there anything you'd like reviewers to focus on? -->
"
289,679ab35e24d155be01f05929,pull-request,feature implementation/enhancement -> scripting,https://github.com/lwindolf/multi-status/pull/5,"Create install_multi-status.sh
Hi Lars,
I created this install script to go along with your project. I did use ChatGPT to make the process a little faster, but I did look over it and test it before this pull request.

I incorporated your install instructions. One thing I found is that the ""./backend/update.pl"" command has to be run inside that folder (likely due to references to other ""./backend"" files) and specify the full path of frontend/data.json (or use bash shortcuts like ../frontend/data.json).

I did my testing on this with Ubuntu 20.04 and nothing newer so far."
290,679ab35e24d155be01f05953,pull-request,feature implementation/enhancement -> feature enhancement -> user experience -> gui-related improvements,https://github.com/cvzi/Youtube-Genius-Lyrics-userscript/pull/60,"fix: Message for Installing YouTube Music Genius Lyrics
<img width=""1462"" alt=""Screen Shot 2023-09-28 at 21 03 51"" src=""https://github.com/cvzi/Youtube-Music-Genius-Lyrics-userscript/assets/44498510/2384af0b-9374-474d-8400-5b380ac67425"">

This is to let you if they want to show genius lyrics they can install another script called XXXXXXX.


However, the message is a bit unclear. Users do not know why this is show.
`â–¶ï¸ If you would like to see lyrics here as well, you can now install a new userscript specifically for music.youtube.com:`


I have asked ChatGPT to correct it, it gives the following:
`â–¶ï¸ The ""Youtube Genius Lyrics"" UserScript is only applied on youtube.com. To view Genius lyrics on music.youtube.com, please install the separate UserScript, ""Youtube Music Genius Lyrics"".`


Besides, I noticed that openuserjs's links are placed in the message. Should they been changed to the script hosted on either GreasyFork or GitHub?"
291,679ab35e24d155be01f05b5a,pull-request,feature implementation/enhancement -> scripting,https://github.com/Satellite-im/Uplink/pull/1290,"chore(appium): close update alert from gh runner before tests
### What this PR does ðŸ“–

- Recently the MacOS GH runners started to show an alert for update available on the OS that is being positioned in front of the application
- The problem is that webdriverio is not ignoring this alert and its failing test execution when trying to interact with elements located behind of this alert
- Asked to chatgpt for an applescript that could close these alerts before starting tests, so trying adding a new step before running tests. Example of the alert displayed on GH runner:
<img width=""810"" alt=""image"" src=""https://github.com/Satellite-im/Uplink/assets/35935591/d91ae84b-0f34-4d6e-a95c-883edfaeea78"">



### Which issue(s) this PR fixes ðŸ”¨

- Resolve #

<!--Add the ticket Github number such as #Resolve #001 to automatically link the PR to the issue-->

### Special notes for reviewers ðŸ—’ï¸

### Additional comments ðŸŽ¤

"
292,679abe2624d155be01f05f81,pull-request,software quality -> refactoring,https://github.com/alkem-io/server/pull/3350,"ORM relations loaded with typing
With help of chatGPT took not long at all...should be safe enough. "
293,679abe2624d155be01f0615b,pull-request,process -> automating commit/issue/PR creation -> creating commit/issue/PR description -> motivating the need for a proposed change,https://github.com/achievements-app/psn-api/pull/151,"fix: add type declaration to package exports
Resolves [#145](https://github.com/achievements-app/psn-api/issues/145)

I tested changes locally with `npm link`. Everything works fine.

### Explanation from ChatGPT Â¯\_(ãƒ„)_/Â¯

> Adding types to package.json exports helps typescript find the type declarations of your package.
> 
> Before you added the ""types"" field to your ""exports"", TypeScript couldnâ€™t find your type declarations through the ""exports"" field. But when you set moduleResolution to ""node"", TypeScript fell back to looking for an index.d.ts file in your moduleâ€™s root directory. Since your type declarations were there, TypeScript was able to find them, and you didnâ€™t see any error.
> 
> However, itâ€™s generally better to specify all your moduleâ€™s entry points (including type declarations) in the ""exports"" field. This way, youâ€™re not relying on TypeScriptâ€™s fallback behavior, which might not work in all environments or future versions of TypeScript. Plus, it makes your packageâ€™s interface clearer to other developers. Thatâ€™s why adding ""types"": ""./dist/index.d.ts"" to your package.json is a good solution."
294,679abffc24d155be01f06706,pull-request,documentation -> generating api specification,https://github.com/hezarai/hezar/pull/124,"Improve docstrings
## Description
Some codes did not have any or proper docstrings. This PR adds docstrings anywhere possible.

## Changes
- Add docstrings to preprocessors
- Add docstrings to datasets and data collators
- Add docstrings to metrics
- Add docstrings to embeddings
- Cleanup some utils

## Related Issues
This PR is partially related to #108. 


## Checklist
<!-- Make sure all items in the checklist are completed before submitting the PR. Remove items that are not applicable. -->

- [x] I have read and followed the project's contributing guidelines.
- [x] My code follows the project's coding style.
- [x] I have tested my changes thoroughly.
- [x] I have updated the documentation if necessary.
- [x] All existing tests pass.
- [ ] I have added new tests to cover my changes.
- [x] My changes do not introduce any new warnings or errors.

## Notes
I used ChatGPT to produce docstrings :)))
"
295,679abffc24d155be01f067d9,pull-request,naming,https://github.com/swup/swup/pull/836,"Manage rapid navigation
**Description**

- Combine/extend @hirasso's 2 PRs: [rapid navigation](https://github.com/swup/swup/pull/825) + [queue navigation](https://github.com/swup/swup/pull/834)
- Add `Visit.state` property and `visit:abort` hook

**Features**

- Convert the `Visit` object to a class
- Pass around the `Visit` object to avoid global mutations
- Add a `state` property to the `Visit` object
  - created â†’ queued â†’ started â†’ leaving â†’ loaded â†’ entering â†’ completed / aborted / failed
- Queue new visit if the current visit is already past the `loaded` state
  - Otherwise, abort the visit and return early in strategic places
  - When aborting only during the `entering` phase, we can have fewer return clauses throughout the code
- Only run hooks if visit is not aborted
- Create `visit:abort` hook to allow undoing dom changes like in `visit:end`
- Always clear `is-leaving` class

**Notes**

- Regarding the naming (`visit.aborted`, `visit:abort`), I asked ChatGPT for a few suggestions and that's what it came up with. I switched to `abort` to see how it feels. I actually quite like it, slightly more than `cancel`. But we can switch back to `cancel` no problem.

**Checks**

- [x] The PR is submitted to the `master` branch
- [x] The code was linted before pushing (`npm run lint`)
- [x] All tests are passing (`npm run test`)
- [x] New or updated tests are included
- [ ] The documentation was updated as required"
296,679ac0de24d155be01f06b95,pull-request,software quality -> fixing -> performance-related,https://github.com/ZephyrProtocol/zephyr/pull/13,"Update string_tools.h
This allows the GENESIS_TX to be parsed and no longer freezes the iOS device when creating a wallet.  Please double check this as I used ChatGPT LOL


""In this code, I've added a helper function hex_char_to_byte to convert a single hex character to a byte. The parse_hexstr_to_binbuff function now iterates over the input string two characters at a time, converting each pair of hex characters into a byte and adding it to the result. This approach ensures that trailing zeros are correctly processed. The function also handles potential errors, such as invalid hex characters and odd-length hex strings."""
297,679ac1a124d155be01f06d5b,pull-request,feature implementation/enhancement -> prototyping -> generate color palette,https://github.com/ethdebug/format/pull/44,"Switch site to Docusaurus
As discussed in the [2023-11-29 biweekly call](https://hackmd.io/y4mSQc_EROqTOx4KWfM6jQ), this PR prepares for a formal-schema-first approach by switching from Sphinx to Docusaurus.

Based on my research, Docusaurus appears to be the most mature documentation site generator that supports JSON Schema draft 2020-12 (sphinx-jsonschema only supports draft 4, which is now ten years old!). I remember reading that draft 2020-12 should be ""close to stable"", so I think it makes sense for us to use that as our JSON Schema version moving forward.

If you'd like to [preview these changes](https://ethdebug.on.fleek.co/format/), I quickly set up a [Fleek](https://fleek.co/) project, which provides a service to build+deploy GH repos as static IPFS sites behind their CDN. This service might be nice for us to use for PR/branch previews, since they provide that feature out of the box. (This PR just keeps the existing GHA deployment to gh-pages, though... I don't think we want to switch entirely to Fleek? At least now in this PR.)

For a breakdown of the various moving parts, this PR includes changes that:
- Remove Sphinx stuff
- Prepare formal schema by starting a `schemas/` directory with some rough sketches in YAML
- Setup Docusaurus within `web/` directory:
  - Initialize Docusaurus and remove all the default stuff (logos, blog, etc.)
  - Establish `docs/` subdirectory for human-readable content about ethdebug, our efforts, etc.
  - Establish `spec/` subdirectory for direct specification materials, using Docusaurus's mechanism for keeping two ""docs"" directories.
  - Author basic homepage so ethdebug has a proper home on the web
  - Define initial version of function for loading schemas from root `schema/` directory, and reference this schema in the `spec/` subdirectory.
  - Extend theme to provide access to FontAwesome for icons
  - Extend theme to include ""external link"" badge on external links in Markdown files
  - Include a temporary logo generated by ChatGPT
  - Override default colors with temporary colorscheme suggested by ChatGPT
- Update GitHub Actions workflows for new toolchain"
298,679ac1a124d155be01f06e5a,pull-request,development environment -> continuous integration -> implementing/fixing jobs/actions,https://github.com/loculus-project/loculus/pull/655,"deployment: avoid unnecessarily redeclaring helm variables
When you enter a helm loop the contents of `.Values` changes so it no longer has the root values. We've previously been dealing with this by assigning the variables we needed outside of the loop, but its a bit verbose. I finally asked ChatGPT and it told me about `$.Values` where the `$` is the root context. Let's use that?"
299,679ac22d24d155be01f07139,pull-request,feature implementation/enhancement -> implementing a new feature,https://github.com/enesky/Doodle/pull/69,"feat: Details Screen
## <sup><sup>&#x1F534; [Mandatory]</sup></sup> What's the change?
- Details Screen's design - navigation - logic are completed
- HomeScreen bugs fixed
- Release build errors fixed
- Added new service usages from Jikan API

## <sup><sup>&#x1F534; [Mandatory]</sup></sup> Why do we need this change?
- The design and logic of the detail screen were completed because we needed to improve the user experience.
- Home screen bugs were fixed because these bugs were negatively impacting user interaction with the app.
- Release build errors were fixed to ensure a smooth deployment of the app to users.
- New service usages from the Jikan API were added to provide users with fresh content.
-> These are generated by ChatGPT :)

## Related issues/PRs/Jira ticket links
- https://github.com/enesky/Doodle/issues/70
- https://github.com/enesky/Doodle/issues/71

## <sup><sup>&#x1F534; [Mandatory if it's UI related]</sup></sup> Screenshots
<img src=""https://github.com/enesky/Doodle/assets/25669076/b2630efa-588e-46ad-bab5-0f5e54281b21"" alt=""Details Screen top"" width=""300"" height=""auto"">
<img src=""https://github.com/enesky/Doodle/assets/25669076/9b8ecc6f-0fe9-4d29-bb14-b24caf90ed78"" alt=""Details Screen bottom"" width=""300"" height=""auto"">"
300,679ac22d24d155be01f07158,pull-request,documentation -> generating readme,https://github.com/SSWConsulting/SSW.Rules.Content/pull/7651,"XSâœ” â—¾ added intro to rules on UAT

> 1. What triggered this change? (PBI link, Email Subject, conversation + reason, etc)

call with @adamcogan 

> 2. What was changed?

Added intro from ChatGPT with changes 

> 3. Did you do pair or mob programming (list names)?

Adam
"
301,679accb324d155be01f07484,pull-request,feature implementation/enhancement -> feature enhancement -> upgrade to latest version of a library,https://github.com/librenms/librenms/pull/15736,"Update SQL override section after switch to SQL strict mode
Since 23.10.0 the currently documented SQL override rule does not work. This one I got from ChatGPT does work so far. 

For reference and fixes #15700

Please give a short description what your pull request is for

DO NOT DELETE THE UNDERLYING TEXT

#### Please note

> Please read this information carefully. You can run `./lnms dev:check` to check your code before submitting.

- [x] Have you followed our [code guidelines?](https://docs.librenms.org/Developing/Code-Guidelines/)
- [ ] If my Pull Request does some changes/fixes/enhancements in the WebUI, I have inserted a screenshot of it.
- [ ] If my Pull Request makes discovery/polling/yaml changes, I have added/updated [test data](https://docs.librenms.org/Developing/os/Test-Units/).

#### Testers

If you would like to test this pull request then please run: `./scripts/github-apply <pr_id>`, i.e `./scripts/github-apply 5926`
After you are done testing, you can remove the changes with `./scripts/github-remove`.  If there are schema changes, you can ask on discord how to revert.
"
302,679accb324d155be01f07657,pull-request,generating/manipulating data,https://github.com/0x2447196/raypeatarchive/pull/10,"Bioelectric fields, Regeneration, and the Lactic Acid Myth (Newsletter to text)
Used ChatGPT to fixing spacing between words + manual edits"
303,679acd8e24d155be01f077e2,pull-request,feature implementation/enhancement -> feature enhancement,https://github.com/statamic/cms/pull/9408,"[4.x] Push listing page state to URL query params
This PR pushes the listing components `parameters` to the URL, so that you can bookmark, refresh and navigate back/forwards while maintaining the selected sort, filters, search, columns, page etc.

This is an opt-in feature of the listing mixin, since it might not make sense for all types of listing, especially when you have more than one on the same page (eg. Dashboard). To enable it set `pushQuery` to `true`.

I have enabled it for the main entry, term and user listings.

The `additionalParameters` are not pushed to the URL, since the listing mixin wont know what they are or how to handle them.

I asked ChatGPT for the `utf8atob` function, it seems to work fine but I have no idea if it's perfect.

Closes https://github.com/statamic/ideas/issues/93"
304,679acd8e24d155be01f079b1,pull-request,software quality -> fixing -> supporting debugging,https://github.com/DefinitelyTyped/DefinitelyTyped/pull/68435,"[npm:@types/iyzipay] Small Namespace Variable Names' Update
I am the owner of this declaration package. This package is currently released.

- [X] I made a small mistake regarding **namespace** elements of class Iyzipay. There was a naming error ( Locale ->LOCALE ) (I was using ChatGpt to handle eslint naming-convention errors. I didn't realize it manipulated this part. I have checked the remaining parts, everything is okay after this )

- [X] I have removed @types/node, it should be unnecessary

Old version variable names:
```typescript
const Locale: Ilocale;
const PaymentGroup: IpaymentGroup;
const BasketItemType: IbasketItemType;
const PaymentChannel: IpaymentChannel;
const SubMerchantType: IsubMerchantType;
const Currency: Icurrency;
const ApmType: IapmType;
const RefundReason: IrefundReason;
const PlanPaymentType: IplanPaymentType;
const SubscriptionPricingPlanInterval: IsubscriptionPricingPlanInterval;
const SubscriptionUpgradePeriod: IsubscriptionUpgradePeriod;
const SubscriptionStatus: IsubscriptionStatus;
const SubscriptionInitialStatus: IsubscriptionInitialStatus;
```

New variable names:
```typescript
const LOCALE: Ilocale;
const PAYMENT_GROUP: IpaymentGroup;
const BASKET_ITEM_TYPE: IbasketItemType;
const PAYMENT_CHANNEL: IpaymentChannel;
const SUB_MERCHANT_TYPE: IsubMerchantType;
const CURRENCY: Icurrency;
const APM_TYPE: IapmType;
const REFUND_REASON: IrefundReason;
const PLAN_PAYMENT_TYPE: IplanPaymentType;
const SUBSCRIPTION_PRICING_PLAN_INTERVAL: IsubscriptionPricingPlanInterval;
const SUBSCRIPTION_UPGRADE_PERIOD: IsubscriptionUpgradePeriod;
const SUBSCRIPTION_STATUS: IsubscriptionStatus;
const SUBSCRIPTION_INITIAL_STATUS: IsubscriptionInitialStatus;
```

Thanks"
305,679acd8e24d155be01f079fd,pull-request,documentation -> generating api specification,https://github.com/Xetera/ghost-cursor/pull/116,"ChatGPT generated methods documentation
I was very surprised there is no documentation on any of the methods."
306,679ace5424d155be01f07c3c,pull-request,"documentation -> generating readme,documentation -> improving writing",https://github.com/holoviz/holoviz/pull/393,"Condense release guide
I used ChatGPT to generate this."
307,679ad03324d155be01f082e9,pull-request,"development environment -> continuous integration -> implementing/fixing jobs/actions,feature implementation/enhancement -> scripting",https://github.com/bahrmichael/factorio-tycoon/pull/272,"chore: workflow to verify crowdin contributions
Starting work on a new GHA workflow to verify the integrity of the crowdin files, based on a suggestion from @winex: https://github.com/bahrmichael/factorio-tycoon/commit/2a0ffc3b0559d7b6a70b21c8e99d2d5a55c32625#commitcomment-139268604

The goal is to verify that all keys from the english locale are present in the other languages, that no groupings are missing (e.g. `[entity-names]` at the beginning of a file, and that there are no duplicate keys that could crash the game on startup).

This is still a work in progress since there are a bunch of edge cases that I didn't have the time to work through yet. The script is mostly generated by ChatGPT and open for discussion and improvement."
308,679ad03324d155be01f08512,pull-request,development environment -> continuous integration -> implementing/fixing jobs/actions,https://github.com/segmentio/backo-go/pull/11,"various updates to modernize the repo
* switch to testify/assert
   * addresses #10 
* remove undocumented gitmodules stuff & use go modules more normally
* removed circleci (a webhook in the project, an out-of-band operation, not done in code)
* add GitHub Actions workflow for testing (thanks ChatGPT for helping craft one so quickly)
* Update go version to latest available stable release (1.21)"
309,679adc8d24d155be01f08ef8,pull-request,generating/manipulating data,https://github.com/flytegg/ls-discord-bot/pull/243,"feat(counting): add insults for when a user fails on counting or counâ€¦
Added a bunch of ChatGPT generated insults and just send them randomly in both cases the user fucks up when counting."
310,679adc8d24d155be01f09014,pull-request,process -> automating commit/issue/PR creation -> creating commit/issue/PR description,https://github.com/supercollider/supercollider/pull/6272,"[supercollider/supercollider - docs - enhancement] - Update README.md - fixing typos
#### [supercollider/supercollider - docs - enhancement] - Update README.md - fixing typos

*<sub>This pull request addresses some minor typographical errors found in the Supercollider documentation. I've carefully reviewed the README.md file and made corrections to ensure clarity and accuracy. By fixing these typos, users will have a smoother experience navigating through the documentation and understanding the features and functionalities of Supercollider. Thank you for considering this enhancement to improve the quality of the documentation. I'm committed to developing a robust documentation framework for Supercollider, characterized by iterative refinement, seamless integration, and effortless accessibility. My approach emphasizes transparency, ethics, and accuracy to ensure the integrity of the documentation remains intact throughout the enhancement process. While I've utilized Grammarly for proofreading, I've consciously opted against employing ChatGPT to avoid altering the essence of the document, prioritizing its integrity while still enhancing its quality. Hereby I used ChatGPT just for the sake of refining this commit comment's exclusively</sub>*"
311,679adc8d24d155be01f090d6,pull-request,software quality -> fixing -> performance-related,https://github.com/chaiNNer-org/chaiNNer/pull/2822,"Use `atexit` to always kill worker
Just for fun, I asked chatGPT about our problem with the worker process being orphaned when the host process was killed. It suggested using `atexit`, which I thought was a good idea. We already solved the problem using the `/shutdown` endpoint, but I think it's good to use `atexit` as a backup. 

I also think that we should keep `/shutdown`. It's useful to have because it allows the server to refuse to be shutdown, in case we need this ability in the future."
312,679ade3f24d155be01f09724,pull-request,learning -> how to use a library/framework/tool,https://github.com/pytorch/test-infra/pull/5237,"[HUD] Add PR title to the page title
Tried using `useEffect(()=> { document.title = ""...""; }, []};` trick initially, but this does not seem to work reliable, so end up adding `DynamicTitle` component that defines a context(thank you ChatGPT for the hint on how to read React docs), and define `useSetTitle(title:string)` hook to update page title from the components

Also make title of top level HUD pages smaller"
313,679adf2424d155be01f09c20,pull-request,"software quality -> fixing -> functional bugs,software quality -> fixing -> supporting debugging",https://github.com/JimmXinu/EpubSplit/pull/12,"return section names as expected
I can't rightly explain... but I asked chatgpt to check it out and got a fix. 

now when calling by function like so I get the proper results

```
def get_line_numbers_and_toc(epub_path):
    # Create an instance of SplitEpub
    epub_splitter = SplitEpub(epub_path)
    
    # Get the split lines
    split_lines = epub_splitter.get_split_lines()
    
    # Extract line numbers and ToC
    line_numbers_and_toc = []
    for line in split_lines:
        line_info = {
            'name': line['toc'][0] if line.get('toc') else 'Unknown',
            'page': line['num'],
        }
        line_numbers_and_toc.append(line_info)
    
    return line_numbers_and_toc
```

feel free to do with this what you will, I'm just happy to be getting the values 

https://github.com/JimmXinu/EpubSplit/issues/11"
314,679ae92e24d155be01f09e53,pull-request,software quality -> refactoring,https://github.com/neozhu/CleanArchitectureWithBlazorServer/pull/696,"Refactor Code Using ChatGPT
**Description:**
This PR aims to improve the code quality and maintainability of our project by leveraging the capabilities of ChatGPT for code refactoring. The primary goals of this refactor include enhancing readability, optimizing performance, and ensuring adherence to best coding practices."
315,679ae92e24d155be01f09f08,pull-request,generating/manipulating data -> ui translations,https://github.com/gizmore/gwf3/pull/125,"Feature/magic chall
The main part is the magic challenge, which I implemented. If there are any better ideas for the title, I am very open to suggestions. I have added locales for german, english, french and italian, but I used ChatGPT to generate the translations for german, french and italian XD."
316,679ae92e24d155be01f09fba,pull-request,learning -> how to use a library/framework/tool -> database querying,https://github.com/BrianHicks/tree-grepper/pull/490,"enable building only with Cargo
Hello! thank you for creating this awesome project, I'm finding it immensely useful and fun to use.
I tried using ast-grep many times, but I was never able to get it to work and have the query return what I want. I just tried tree-grepper and in about 10 minutes, with the help of the Tree-sitter ""`:Inspect`"" Playground in Neovim, which looks like this:

![Tree-sitter Inspect in Neovim ](https://github.com/BrianHicks/tree-grepper/assets/9993663/85b11450-12de-4483-b967-c308691847a2)

[...and with the help of ChatGPT to refine the tree-sitter query](https://chatgpt.com/share/f63bbdd1-8bf7-4197-99d1-c32d9b98c733), I was able to have the query return what I wanted exactly:

```json
[
  {
    ""file"": ""mod.rs"",
    ""file_type"": ""rust"",
    ""matches"": [
      {
        ""kind"": ""string_content"",
        ""name"": ""string"",
        ""text"": ""lrld"",
        ""start"": {
          ""row"": 1486,
          ""column"": 10
        },
        ""end"": {
          ""row"": 1486,
          ""column"": 14
        }
      },
      {
        ""kind"": ""string_content"",
        ""name"": ""string"",
        ""text"": ""sldr"",
        ""start"": {
          ""row"": 1489,
          ""column"": 10
        },
        ""end"": {
          ""row"": 1489,
          ""column"": 14
        }
      },
      {
        ""kind"": ""string_content"",
        ""name"": ""string"",
        ""text"": ""scnl"",
        ""start"": {
          ""row"": 1498,
          ""column"": 10
        },
        ""end"": {
          ""row"": 1498,
          ""column"": 14
        }
      },
      {
        ""kind"": ""string_content"",
        ""name"": ""string"",
        ""text"": ""rpt-any"",
        ""start"": {
          ""row"": 1542,
          ""column"": 10
        },
        ""end"": {
          ""row"": 1542,
          ""column"": 17
        }
      },
      ""............""
]
```

The query:
```scm
(
  (match_expression
    ""match""
    value: (identifier)
    body: (match_block
      ""{""
      (match_arm
        pattern: (match_pattern
            (string_literal
              ""\"""" 
              (string_content) @string
            )
        )
      )
    )
  )
)
```

but before actually getting to try it out, I had a lot of trouble and wasted quite a lot of time trying to get to try to use it, by building it, since trying out the Nix package manager on Arch didn't work for me, it didn't install correctly and outputs something something ""socket permission error"" when I try to use a Nix command.

I think enabling only a Cargo build option, without Nix, would be helpful to a lot more people and would enable a lot more people to try the software. a Cargo only build would also enable you to [publish a pre-built binary to crates.io](https://doc.rust-lang.org/book/ch14-04-installing-binaries.html#:~:text=Note%20that%20you,within%20other%20programs.), so it would make installing for people even more easier.

![](https://github.com/BrianHicks/tree-grepper/assets/9993663/8af93d77-1463-4124-8469-ec68d60a3e3c)


Edit: see prior issues about this, which this PR potentially fixes #229 #261 #293"
317,679ae9f224d155be01f0a099,pull-request,software quality -> code review -> spotting bugs,https://github.com/software-mansion/react-native-reanimated/pull/6160,"Add remark about reading SharedValue from the js thread
## Summary

Reading from a shared value on the js thread may block the JS thread until the UI runtime finishes whatever it's doing. This isn't really communicated clearly and adding it to the docs seems like the best option as we don't want to show warnings.

## Test plan

I asked ChatGPT to point out problems (it did).
"
318,679ae9f224d155be01f0a0ba,pull-request,"documentation -> generating api specification,documentation -> generating code comments,documentation -> improving writing",https://github.com/gicait/geoserver-rest/pull/155,"ai-based doc cleanup
Closes https://github.com/gicait/geoserver-rest/issues/46

This PR is actually very AI-heavy. I used ChatGPT to tidy up the existing docstrings and create ones where they were missing. I validated them afterwards and I found no problems. Despite this, I put a disclaimer on the API docs page because I believe that AI-generated contributions and content should always be disclosed.

Incidentally, [the api docs page is currently broken](https://geoserver-rest.readthedocs.io/en/latest/geo.html#the-geoserver-class). I built the docs locally and they seem to work now:

![image](https://github.com/gicait/geoserver-rest/assets/13814358/cc2d71b2-78ab-44b6-acd1-f295e96f7570)
"
319,679ae9f224d155be01f0a177,pull-request,documentation -> improving writing,https://github.com/Mapsui/Mapsui/pull/2695,"Rewrite projections.md
Asked chatgpt to rewrite the projections page. I have not reviewed this myself. Created a merge request to make reviewing easier."
320,679aeb8f24d155be01f0a8b4,pull-request,feature implementation/enhancement -> feature enhancement -> user experience -> gui-related improvements,https://github.com/DataTalksClub/course-management-platform/pull/59,"Style: enhance the style of some platform pages
Title: Enhance CSS for improved responsiveness and accessibility

## Description
In this pull request, I try to use ChatGPT to implement several CSS enhancements to improve the platform's user interface. 

## Changes Made
- enhance the page layout and make pages responsive
- create a better login page with social icons
- change how courses are displayed on the website
- replace links to the github repo and official site of data talks with icons

## Visual Changes

https://github.com/user-attachments/assets/044fefb1-cea5-47c8-8cd1-060930ab6c32

"
321,679aec6e24d155be01f0aa0f,pull-request,software quality -> refactoring,https://github.com/serlo/api.serlo.org/pull/1702,"chore(deploy): rewrite to bash to stop after errors
Unfortunately, handling error in typescript script was quite complicated and sometimes the CI passed, even though there was an error. The bash script fails fast.

coauthor: ChatGPT for rewriting the code so fast.
"
322,679aec6e24d155be01f0aade,pull-request,software quality -> testing -> generating tests,https://github.com/endatix/endatix/pull/44,"feat(endatix-41): Add unit tests for the classes from UseCases in Endatix.Core
# Add Endatix.Core.Tests project and cover the classes from UseCases with unit tests

## Description
Add Endatix.Core.Tests project and cover the classes from UseCases with unit tests. Tests are generated by ChatGPT with tiny changes after. The prompt will be stored as a part of another PR with explanations.

## Related Issues
https://github.com/endatix/endatix/issues/41

## Type of Change
- [x] Adding tests for existing features

## Checklist
- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] My changes generate no new warnings
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] Any dependent changes have been merged and published in downstream modules"
323,679aec6e24d155be01f0ab3b,pull-request,documentation -> improving writing,https://github.com/kirillzyusko/react-native-keyboard-controller/pull/533,"fix: `KeyboardToolbar` a11y wording
## ðŸ“œ Description

Corrected wording for `accessibilityHint`. Move from ""future simple"" to ""present simple"" tense.

## ðŸ’¡ Motivation and Context

I missed ""the"" article and decided to add it. But then I asked ChatGPT what else can be improved and ChatGPT suggested to write everything in present simple tense. I kind of agree, because we describe what button does, not what it will do.

## ðŸ“¢ Changelog

<!-- High level overview of important changes -->
<!-- For example: fixed status bar manipulation; added new types declarations; -->
<!-- If your changes don't affect one of platform/language below - then remove this platform/language -->

### JS

- changed `accessibilityHint`;

## ðŸ¤” How Has This Been Tested?

Tested on iPhone 15 Pro and e2e tests.

## ðŸ“ Checklist

- [x] CI successfully passed
- [x] I added new mocks and corresponding unit-tests if library API was changed
"
324,679af71024d155be01f0b0a4,pull-request,feature implementation/enhancement -> implementing a new feature,https://github.com/tidy-finance/r-tidyfinance/pull/96,"Inferred proper symbols with country codes from index constituents
I started out coding the exchange codes manually, but later asked ChatGPT to complete them. Seems like a good starting point for now, we can easily extend this later. 

I also had to update the stock data download because it threw an unexpected error when there were missing values (because they were NULL and not explicit NAs). 

I also added a currency column because YahooFinance returns different currencies, depending on the exchange-coded symbol. "
325,679af86b24d155be01f0b4f6,pull-request,feature implementation/enhancement -> migrating/reusing via programming language translation,https://github.com/apalache-mc/apalache/pull/2980,"Bat file to run Apalache on Windows
Hello :octocat: 

I've written this file months ago and gave it to my students that use windows. I'd like to add it to the released Apalache files mainly in order to call this bat from Quint when in Windows, and therefore have the same seamless integration on `quint verify` as we have on Linux and Mac OS. See https://github.com/informalsystems/quint/issues/1414. WDYT?

PS: I'm not familiar with bat at all. I've asked ChatGPT to convert the bash version into bat and inspected the results to make sure nothing weird was added. One thing I noticed is that killing the bat doesn't kill Apalache, which would be ideal. I looked at [this template](https://github.com/sbt/sbt-native-packager/blob/e72f2f45b8cab5881add1cd62743bfc69c2b9b4d/src/main/resources/com/typesafe/sbt/packager/archetypes/scripts/bat-template) as basis (as the bash refers to this set of templates) but couldn't find how to set that up.

<!-- Please ensure that your PR includes the following, as needed -->

- [ ] Tests added for any new code
- [ ] Ran `make fmt-fix` (or had formatting run automatically on all files edited)
- [ ] Documentation added for any new functionality
- [ ] [Entries added to `./unreleased/`][changelog format] for any new functionality

[changelog format]: https://github.com/informalsystems/apalache/blob/main/CONTRIBUTING.md#how-to-record-a-change
"
326,679afa0424d155be01f0bcce,pull-request,software quality -> refactoring,https://github.com/ReliaQualAssociates/ramstk/pull/1404,"refactor: simplify RAMSTCommonDB methods
## Does this PR introduce a breaking change?
- [ ] Yes
- [x] No

<!-- If yes, describe the impact and migration path below. -->

## Describe the purpose of this pull request.
To refactor the RAMSTKCommonDB and RAMSTKProgramDB classes to reflect the BaseDatabase meta-class refactoring.

## Describe how this was implemented.
Using ChatGPT to refactor classes.

## Describe any particular area(s) reviewers should focus on.
None

## Provide any other pertinent information.
<!-- Provide any other information that is import to this PR such as
screenshots if this impacts the GUI. -->


## Pull Request Checklist

- Code Style
  - [x] Code is following code style guidelines.

- Static Checks
  - [x] Failing static checks are only applicable to code outside the scope of
   this PR.

- Tests
  - [x] At least one test for all newly created functions/methods?

- Chores
  - [x] Issue(s) have been raised for problem areas outside the scope of
    this PR.  These problem areas have been decorated with an ISSUE: # comment.

<!-- Generated by sourcery-ai[bot]: start summary -->

## Summary by Sourcery

Refactor the RAMSTKCommonDB and BaseDatabase classes to simplify methods, improve error handling, and enhance type annotations. Update tests to improve output readability.

Enhancements:
- Simplify the initialization of attributes in the RAMSTKCommonDB class by removing unnecessary comments and unused attributes.
- Refactor input handling in the _do_add_administrator method to remove redundant comments and improve readability.
- Streamline the _do_create_database method by consolidating parameter descriptions and removing redundant comments.
- Improve error handling in the do_create_postgres_db function by logging errors and raising a DataAccessError.
- Refactor the do_execute_query method to return a more precise type using SQLAlchemy's Row and Sequence types.
- Update the get_database_session method to return both the engine and scoped session, enhancing its utility.

Tests:
- Change the color code in the test for the fail_read_license message broadcast to improve test output readability.

<!-- Generated by sourcery-ai[bot]: end summary -->"
327,679afa0424d155be01f0bd2c,pull-request,software quality -> fixing -> functional bugs,https://github.com/Kong/KongAir/pull/136,"fix: Fix the value property array and spelling errors in the Flights spec
I was trying to run this spec in Insomnia and was getting a `""value"" property type must be an array` error, so I used ChatGPT to fix it. This revision doesn't have any warning level errors when I run it in Insomnia, but the formatting also might not be 100% correct since AI helped fix it. 

There were also a few spelling errors that this fixes."
328,679afa0424d155be01f0bdfa,pull-request,process -> automating commit/issue/PR creation -> creating commit/issue/PR description,https://github.com/PrairieLearn/PrairieLearn/pull/10759,"Cache instantiated Python modules
I'm making this change to support some upcoming changes that will optimize the `pl-code` element to cache expensive-to-compute data. However, even on its own, this change already yields some nice improvements. I tested these changes on the `element/multipleChoice` question in the example course with five trials. I ran these tests with the `process-questions-in-worker` feature flag enabled.

Master:

```
freeform.generate: 0.74ms
freeform.prepare: 38.25ms
freeform.render: 35.43ms
freeform.generate: 0.55ms
freeform.prepare: 44.61ms
freeform.render: 41.02ms
freeform.generate: 0.27ms
freeform.prepare: 37.71ms
freeform.render: 38.84ms
freeform.generate: 0.34ms
freeform.prepare: 38.50ms
freeform.render: 32.91ms
freeform.generate: 0.49ms
freeform.prepare: 43.66ms
freeform.render: 36.21ms
```

On this branch:

```
freeform.generate: 0.77ms
freeform.prepare: 18.27ms
freeform.render: 19.76ms
freeform.generate: 0.65ms
freeform.prepare: 18.12ms
freeform.render: 23.98ms
freeform.generate: 0.11ms
freeform.prepare: 10.44ms
freeform.render: 16.50ms
freeform.generate: 0.59ms
freeform.prepare: 19.64ms
freeform.render: 22.99ms
freeform.generate: 0.62ms
freeform.prepare: 20.08ms
freeform.render: 25.06ms
```

Summary (blame ChatGPT for any wrong numbers):

| operation | `master` average | branch average | diff |
|--------|--------|--------|--------|
| `generate` | 0.48ms | 0.55ms | -14% |
| `prepare` | 40.55ms | 17.31ms | 57% |
| `render` | 36.88ms | 21.66ms | 41% |

`generate` is already very fast and doesn't use elements to begin with, so I'm inclined to say that the difference here is just due to normal variation.

Note that these changes will really only help with questions that use a given element multiple times. The improvement this will yield is proportional to the number of times an element is reused."
329,679afa0424d155be01f0be30,pull-request,documentation -> internationalization support,https://github.com/once-ui-system/magic-portfolio/pull/1,"`next-intl` localization
ðŸ‘‹ @lorant-one @zsofiakm!

Here is the first draft of the `next-intl` based i18n.

For reference, it's based on one of next-intl examples ([source](https://github.com/amannn/next-intl/tree/main/examples/example-app-router-without-i18n-routing), [demo](https://next-intl-example-app-router-without-i18n-routing.vercel.app/login)).

## Some notes

- I kept the default logic of setting/getting the locale via a cookie, not sure if/how we would want to adapt this.
- The locale toggle is dynamically generated from the configured locales. This might not be ideal if the number of locales is high, but it's probably sufficient in the context of a portfolio. (Switching to a dropdown is a possibility, but I like the toggle look.
- The `mdx` content (posts/projects) now lives in localized subdirectories. In the current example I kept the same filenames across the two locales, so that switching is transparent. (If the names differ, switching locale will lead to 404. It might be adaptable, to redirect properly, but I did not dig deep in the code, and I like the simplicity of having next-intl triggering the state change.)
- I translated the projects and blog posts with ChatGPT for the sake of the example. (Using Indonesian to fit the profile!)

## Questions

My main question at this stage is: **how do we want to handle the `content.js` file?**

`next-intl` allows to grab the localized content (from `i18n/messages/{locale}.json` via the `useTranslations` hook, but that's not callable from `content.js`.

I have two solutions on the top of my head, but neither might be the best here:
1. deep restructuring of the `content.js` file, only keeping ""settings""-like things there, and handle the localized content elsewhere:
  a. retrieve the various translations in the pages via scoped `t(X)` calls;
  b. define a specific `getContent((t) => {...}` function to call in the various pages, and return a relatively equivalent structure (not sure it would work properly);
2. somewhat similar to 1.b. but passing the needed content through props (not sure if more efficient)

Feedback and suggestions welcome!"
330,679afb3624d155be01f0c12b,pull-request,generating/manipulating data -> ui translations,https://github.com/garagepoort/StaffPlusPlus/pull/249,"Translation of GUI Titles
Allows translating the GUI title of:

- [x] `/my-reports`
- [x] `/my-warnings`

Also, why do we have files for  some languages (such as `lang_de.yml`) which is only English?

I used ChatGPT to translate to all other languages, apart from English.

Done for the request of bixelcraft on discord."
331,679afb3624d155be01f0c261,pull-request,software quality -> code review,https://github.com/Kong/kong/pull/13752,"chore(ci): run ai assisted changelog reviews
### Summary

Use chatgpt to review changelog entries. We pass the title, description and the changelog itself. The action will create suggestions in form of a review.

![image](https://github.com/user-attachments/assets/23acde71-5040-4f33-94e6-7ca6a2e68957)

Code will not be sent to OpenAI. This action will only be triggered when a changelog file is present. To re-trigger it, remove the comment/review.
"
332,679b04e824d155be01f0c419,pull-request,feature implementation/enhancement -> finding an api for a given task,https://github.com/cloudprober/cloudprober/pull/899,"[probes.external] Improve on how we export external probe process's logs
- Export external process's logs in a dedicated field: `process_stderr`
- This allow users to further parse external process's logs.

I asked ChatGPT if Datadog can do it. It seems it can:
https://chatgpt.com/share/671304a1-9944-8005-9568-cdd058c1547c

See https://github.com/cloudprober/cloudprober/issues/893 for some background."
333,679b04e824d155be01f0c480,pull-request,"development environment,feature implementation/enhancement -> implementing a new feature",https://github.com/CleverRaven/Cataclysm-DDA/pull/77212,"Add a new widget to display weight with the format ""##.#/##.#[unit]""
#### Summary

Features ""Add a new widget to display weight as ""##.#/##.#[unit]""""

#### Purpose of change

I felt like this was a glaring hole in the information you could display in the sidebar, considering all the random stuff you can display over there, so I fixed it.

#### Describe the solution

Added a new functions in display,cpp to pull current units and concat current weight with max weight, and calculate what percent of the max they are to color them appropriately, then added the requisite code to widget.cpp to call on this information. 

I then added a new widgets to legacy labels sidebar to display this new information, because that's the sidebar I use. If anyone wants it added to another sidebar, speak up.

#### Describe alternatives you've considered

Not doing the imperial version because pounds make no sense

#### Testing

![image](https://github.com/user-attachments/assets/bb0e8a44-4885-442b-ac04-c855e64ecf24)
![image](https://github.com/user-attachments/assets/1a7f89a9-0af4-484f-8db1-301e0532a566)

#### Additional context

I'd like to thank my gf for being patient with me while I ignored the show we were watching all day today, and I'd like to thank chatGPT for holding my hand and helping me setup the compiler in visual code, and helping me with c++ syntax because my coding experience is limited to java from 15 years ago, and lua from writing software in the minecraft computercraft mod

My head hurts

I'd also like to thank @Zenefess for giving me some guidance on how widgets work when I first thought about doing this a month or two ago

Should probably rename the existing carry_weight_text variables to denote that they handle the carry weight as a percent, because it's somewhat ambiguous"
334,679b069024d155be01f0cb36,pull-request,process -> automating commit/issue/PR creation -> creating commit/issue/PR description,https://github.com/monkeytypegame/monkeytype/pull/6018,"fix: swipe typing duplicate word issue fixed (@HemendraSinghShekhawat)
### Description

fixed: swipe type duplication error fixed for English.
NOTE: (not checked on another languages, don't know if that is even possible)
Asked ChatGPT on how to create a PR : ) (thanks for monkeytype btw)

related issue: #5624 

Swipe type video on local:

https://github.com/user-attachments/assets/0629f3dd-6c43-48c3-82d9-2175f61b52f7




### Checks

- [ ] Adding quotes?
  - [ ] Make sure to include translations for the quotes in the description (or another comment) so we can verify their content.
- [ ] Adding a language or a theme?
  - [ ] If is a language, did you edit `_list.json`, `_groups.json` and add `languages.json`?
  - [ ] If is a theme, did you add the theme.css?
    - Also please add a screenshot of the theme, it would be extra awesome if you do so!
- [x] Check if any open issues are related to this PR; if so, be sure to tag them below.
- [x] Make sure the PR title follows the Conventional Commits standard. (https://www.conventionalcommits.org for more info)
- [ ] Make sure to include your GitHub username prefixed with @ inside parentheses at the end of the PR title.

"
335,679b069024d155be01f0cb38,pull-request,software quality -> optimization,https://github.com/ItzCrazyKns/Perplexica/pull/453,"Used ChatGPT Free to optimize all agent prompts.
I used the Free version of ChatGPT to optimize all the prompts. This needs to be tested but it seems to work ok. Please review and test thoroughly before accepting these changes. I am not a prompt engineer I just wanted to see what would happen if I optimized all of the prompts. Its possible that these changes actually made things worse, but if so I can't tell yet."
336,679b073324d155be01f0ce21,pull-request,documentation -> generating readme,https://github.com/cargo-public-api/cargo-public-api/pull/654,"Add trademark notice to READMEs with Rust or Cargo in the project name
To fulfil requirements outlined in the Rust Project's trademark policy, more specifically ""Updated Rust Trademark Policy Draft - November 2024"". See https://drive.google.com/file/d/1hjTx11Fb-4W7RQLmp3R8BLDACc7zxIpG/view.

(I got help from ChatGPT to formulate the notice.)"
337,679b07ef24d155be01f0cee3,pull-request,"documentation -> generating readme,documentation -> improving writing",https://github.com/jkunczik/home-assistant-omada/pull/57,"Improve README structure and  more info
I added some info about developing to the README, and used ChatGPT to improve structure and wording."
338,679b07ef24d155be01f0cf9c,pull-request,feature implementation/enhancement -> feature enhancement -> user experience -> internationalization support,https://github.com/tauri-apps/tauri/pull/11727,"fix(windows): add Portuguese language in nsis bundle (fix #11725)
<!--
Before submitting a PR, please read https://github.com/tauri-apps/tauri/blob/dev/.github/CONTRIBUTING.md#pull-request-guidelines

1. Give the PR a descriptive title.

  Examples of good title:
    - fix(windows): fix race condition in event loop
    - docs: update example for `App::show`
    - feat: add `Window::set_fullscreen`

  Examples of bad title:
    - fix #7123
    - update docs
    - fix bugs

2. If there is a related issue, reference it in the PR text, e.g. closes #123.
3. If this change requires a new version, then add a change file in `.changes` directory with the appropriate bump, see https://github.com/tauri-apps/tauri/blob/dev/.changes/README.md
4. Ensure that all your commits are signed https://docs.github.com/en/authentication/managing-commit-signature-verification/signing-commits
5. Ensure `cargo test` and `cargo clippy` passes.
6. Propose your changes as a draft PR if your work is still in progress.
-->
Closes #11725

Using ChatGPT to translate using English and PortugueseBR as a reference. Let me know if the translations are wrong."
339,679b13c024d155be01f0da85,pull-request,software quality -> fixing -> supporting debugging,https://github.com/oslabs-beta/CacheIQL/pull/14,"Bug: Feature/client testing
Solved the error we were facing yesterday trying to import cacheIt. Due to how specific it was I used chatgpt to help but here's what I gathered from my own understanding:
Problem: So from what I'm understanding the errors occurred because the client side package wasn't properly compiling  typescript into javascript files and typescript declaration files which were the .d.ts names files. Since it wasn't compiling the declaration files Typescript couldn't find the types.
How it Occured:
So our typescript config has an option to create declaration files and we didn't enable that option
The OutDir and RootDir within the typescript config also weren't set properly. What we want to happen is for all the necessary files to be compiled to the dist folder
How I fixed it:
To fix it I tweaked the typescript config to make sure it generated type declaration files
I also made sure the outDir points and compiles to the dist folder"
340,679b152824d155be01f0dffb,pull-request,documentation -> internationalization support,https://github.com/luxdotdev/parsertime/pull/57,"Add korean translations from ChatGPT
None"
341,679b152824d155be01f0e01f,pull-request,software quality -> fixing -> functional bugs,https://github.com/YunoHost-Apps/friendica_ynh/pull/149,"Update systemd.service
This seems to be fixing a new issue with `warning: mail_queue_enter: create file maildrop/513692.12834: Permission denied`

Posts posted on friendica failed to be delivered. I have used chatgpt (need to be honest) but also did research on my own. I tried many things. In the end these changes to the systemd file worked. 

I ask if anyone can check it please to make sure it is fine.
"
342,679b152824d155be01f0e0df,pull-request,feature implementation/enhancement -> implementing a new feature,https://github.com/cyberbotics/webots_ros2/pull/1024,"Master
I'm create waypoints generation using ChatGPT for Dji Mavic Pro 2
"
